Content from https://docs.livekit.io/agents/v0/overview:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
What is LiveKit Agents?
What you can do with agents
How agents connect to LiveKit
How to create an agent
Agents framework features
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
LiveKit Agents 1.0
.
v1.0 for Node.js is coming soon.
What is LiveKit Agents?
LiveKit Agents is a framework for building programmable, multimodal AI agents that orchestrate LLMs and other AI models to accomplish tasks. This framework allows you to build agents using Python or Node.js.
Unlike traditional HTTP servers, agents operate as stateful, long-running processes. They connect to the LiveKit network via WebRTC, enabling low-latency, realtime media and data exchange with frontend applications.
The Agents framework overcomes several key limitations of traditional architectures:
Multimodal
: Agents can exchange voice, video, and text with users.
Simpler frontend
: Frontend applications use LiveKit’s SDKs to handle the complexities of WebRTC transport, media device management, and audio/video encoding and decoding.
Low-latency
: The
LiveKit Cloud
global mesh network connects each user to their nearest edge server, minimizing transport latency.
Centralized business logic
: Keeping business logic within the agent process allows it to support clients across platforms, including telephony integrations.
Stateful
: End-user interactions are inherently stateful. Rather than synchronizing client-side state through request/response cycles, agents provide a more intuitive way to manage these interactions.
What you can do with agents
The LiveKit Agents framework is designed to give you flexibility when building server side, programmable participants. You can create multiple frontends that all connect to the same backend agent.
Some great use cases for agents include:
AI voice agents
: An agent that has natural voice conversations with users.
Call center
: Answer incoming calls, or make outbound calls with AI agents.
Transcription
: Realtime voice-to-text transcription.
Object detection/recognition
: Identify objects over realtime video.
AI-driven avatars
: Generated avatars using prompts.
Translation
: Realtime translation.
Video manipulation
: Realtime video filters and transforms.
How agents connect to LiveKit
When you start running your agent code, it registers itself with a LiveKit server (either
self hosted
or
LiveKit Cloud
and runs as a background "worker" process. The worker waits on standby for users to connect. Once a end-user session is initiated (that is, a room is created for the user), an available worker dispatches an agent to the room.
Users connect to a LiveKit room using a frontend application. Each user is a
participant
in the room and the agent is an AI participant. How the agent interacts with end-user participants depends on the custom code you write.
How to create an agent
To create an agent using the framework, you’ll need to write a Python or Node.js application (your agent) and a frontend for your users:
Write the application code for your agent. The configuration, functions, and plugin options are all part of your agent code. You can use plugins included in the framework for LLM, STT, TTS, VAD, and utilities for working with text, or write your own custom plugins. Define the entrypoint function that executes when a connection is made. You can also define optional functions to preprocess connections and set connection thresholds or permissions for the worker process.
To learn more, see
Integrations
and
Worker options
.
Create a frontend for users to connect to your agent in a LiveKit room. For development and testing, you can use the
Agents Playground
.
Agents framework features
LiveKit audio/video transport
: Use the same
LiveKit API primitives
to transport voice and video from your frontend to your application server in realtime.
Abstractions over common tasks
: Tasks such as speech-to-text, text-to-speech, and using LLMs are simplified so you can focus on your core application logic.
Extensive and extensible plugins
: Prebuilt integrations with OpenAI, Deepgram, Google, ElevenLabs, and more. You can create a plugin to integrate any other provider.
End-to-end dev experience
: Compatible with
LiveKit server
and
LiveKit Cloud
. Develop locally and deploy to production without changing a single line of code.
Orchestration and scaling
: Built-in worker service for agent orchestration and load balancing. To scale, just add more servers.
Open Source
: Like the rest of LiveKit, the Agents framework is Apache 2.0.
Edge optimized
: When using LiveKit Cloud, your agents transmit voice and video over LiveKit's global edge network, ensuring minimal latency for users worldwide.
On this page
What is LiveKit Agents?
What you can do with agents
How agents connect to LiveKit
How to create an agent
Agents framework features
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/get-started/intro-to-livekit:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Why choose LiveKit?
What is WebRTC?
LiveKit ecosystem
Deployment options
What can you build with LiveKit?
LiveKit is an open source platform for developers building realtime media applications. It makes it easy to integrate audio, video, text, data, and AI models while offering scalable realtime infrastructure built on top of WebRTC.
Why choose LiveKit?
LiveKit provides a complete solution for realtime applications with several key advantages:
Developer-friendly
: Consistent APIs across platforms with comprehensive and well-documented SDKs.
Open source
: No vendor lock-in with complete transparency and flexibility.
AI-native
: First-class support for integrating AI models into realtime experiences.
Scalable
: Can support anywhere from a handful of users to thousands of concurrent participants, or more.
Deployment flexibility
: Choose between fully-managed cloud or self-hosted options.
Private and secure
: End-to-end encryption, HIPAA-compliance, and more.
Built on WebRTC
: The most robust realtime media protocol for peak performance in any network condition.
What is WebRTC?
WebRTC
provides significant advantages over other options for building realtime applications such as websockets.
Optimized for media
: Purpose-built for audio and video with advanced codecs and compression algorithms.
Network resilient
: Performs reliably even in challenging network conditions due to UDP, adaptive bitrate, and more.
Broad compatibility
: Natively supported in all modern browsers.
LiveKit handles all of the complexity of running production-grade WebRTC infrastructure while extending support to mobile apps, backends, and telephony.
LiveKit ecosystem
The LiveKit platform consists of these core components:
LiveKit Server
: An open-source media server that enables realtime communication between participants. Use LiveKit's fully-managed global cloud, or self-host your own.
LiveKit SDKs
: Full-featured web, native, and backend SDKs that make it easy to join rooms and publish and consume realtime media and data.
LiveKit Agents
: A framework for building realtime multimodal AI agents, with an extensive collection of plugins for nearly every AI provider.
Telephony
: A flexible SIP integration for inbound or outbound calling into any LiveKit room or agent session.
Egress
: Record and export realtime media from LiveKit rooms.
Ingress
: Ingest external streams (such as RTMP and WHIP) into LiveKit rooms.
Server APIs
: A REST API for managing rooms, and more. Includes SDKs and a CLI.
Deployment options
LiveKit offers two deployment options for LiveKit Server to fit your needs:
LiveKit Cloud
: A fully-managed, globally distributed service with automatic scaling and high reliability. Trusted by companies of all sizes, from startups to enterprises.
Self-hosted
: Run the open source LiveKit server on your own infrastructure for maximum control and customization.
Both options provide the same core platform features and use the same SDKs.
What can you build with LiveKit?
AI assistants
: Voice and video agents powered by any AI model.
Video conferencing
: Secure, private meetings for teams of any size.
Interactive livestreaming
: Broadcast to audiences with realtime engagement.
Robotics
: Integrate realtime video and powerful AI models into real-world devices.
Healthcare
: HIPAA-compliant telehealth with AI and humans in the loop.
Customer service
: Flexible and observable web, mobile, and telephone support options.
Whatever your use case, LiveKit makes it easy to build innovative, intelligent realtime applications without worrying about scaling media infrastructure.
Get started with LiveKit today
.
On this page
Why choose LiveKit?
What is WebRTC?
LiveKit ecosystem
Deployment options
What can you build with LiveKit?
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/get-started/api-primitives:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Room
Participant
Participant fields
Types of participants
Track
TrackPublication fields
Track subscription
Overview
LiveKit has only three core constructs: a room, participant, and track. A room is simply a realtime session
between one or more participants. A participant can publish one or more tracks and/or subscribe to one or more
tracks from another participant.
Room
A
Room
is a container object representing a LiveKit session.
Each participant in a room receives updates about changes to other participants in the same room. For example, when a participant adds, removes, or modifies the state (for example, mute) of a track, other participants are notified of this change. This is a powerful mechanism for synchronizing state and fundamental to building any realtime experience.
A room can be created manually via
server API
, or automatically, when the first participant joins it. Once the last participant leaves a room, it closes after a short delay.
Participant
A
Participant
is a user or process that is participating in a realtime session. They are represented by a unique developer-provided
identity
and a server-generated
sid
. A participant object also contains metadata about its state and tracks they've published.
Important
A participant's identity is unique per room. Thus, if participants with the same identity join a room, only the most recent one to join will remain; the server automatically disconnects other participants using that identity.
There are two kinds of participant objects in the SDKs:
A
LocalParticipant
represents the current user who, by default, can publish tracks in a room.
A
RemoteParticipant
represents a remote user. The local participant, by default, can subscribe to any tracks published by a remote participant.
A participant may also
exchange data
with one or many other participants.
Participant fields
Field
Type
Description
sid
string
A UID for this particular participant, generated by LiveKit server.
identity
string
Unique identity of the participant, as specified when connecting.
name
string
Optional display name.
state
ParticipantInfo.State
JOINING, JOINED, ACTIVE, or DISCONNECTED.
kind
ParticipantInfo.Kind
The type of participant; more below.
attributes
string
User-specified
attributes
for the participant.
permission
ParticipantInfo.Permission
Permissions granted to the participant.
Types of participants
In a realtime session, a participant could represent an end-user, as well as a server-side process. It's possible to distinguish between them with the
kind
field:
STANDARD
: A regular participant, typically an end-user in your application.
AGENT
: An agent spawned with the
Agents framework
.
SIP
: A telephony user connected via
SIP
.
EGRESS
: A server-side process that is recording the session using
LiveKit Egress
.
INGRESS
: A server-side process that is ingesting media into the session using
LiveKit Ingress
.
Track
A
Track
represents a stream of information, be it audio, video or custom data. By default, a participant in a room may publish tracks, such as their camera or microphone streams and subscribe to one or more tracks published by other participants. In order to model a track which may not be subscribed to by the local participant, all track objects have a corresponding
TrackPublication
object:
Track
: a wrapper around the native WebRTC
MediaStreamTrack
, representing a playable track.
TrackPublication
: a track that's been published to the server. If the track is subscribed to by the local participant and available for playback locally, it will have a
.track
attribute representing the associated
Track
object.
We can now list and manipulate tracks (via track publications) published by other participants, even if the local participant is not subscribed to them.
TrackPublication fields
A
TrackPublication
contains information about its associated track:
Field
Type
Description
sid
string
A UID for this particular track, generated by LiveKit server.
kind
Track.Kind
The type of track, whether it be audio, video or arbitrary data.
source
Track.Source
Source of media: Camera, Microphone, ScreenShare, or ScreenShareAudio.
name
string
The name given to this particular track when initially published.
subscribed
boolean
Indicates whether or not this track has been subscribed to by the local participant.
track
Track
If the local participant is subscribed, the associated
Track
object representing a WebRTC track.
muted
boolean
Whether this track is muted or not by the local participant. While muted, it won't receive new bytes from the server.
Track subscription
When a participant is subscribed to a track (which hasn't been muted by the publishing participant), they continuously receive its data. If the participant unsubscribes, they stop receiving media for that track and may resubscribe to it at any time.
When a participant creates or joins a room, the
autoSubscribe
option is set to
true
by default. This means the participant automatically subscribes to all existing tracks being published and any track published in the future. For more fine-grained control over track subscriptions, you can set
autoSubscribe
to
false
and instead use
selective subscriptions
.
Note
For most use cases, muting a track on the publisher side or unsubscribing from it on the subscriber side is typically recommended over unpublishing it. Publishing a track requires a negotiation phase and consequently has worse time-to-first-byte performance.
On this page
Overview
Room
Participant
Participant fields
Types of participants
Track
TrackPublication fields
Track subscription
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/get-started/authentication:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Creating a token
Token example
Video grant
Example: subscribe-only token
Example: camera-only
SIP grant
Creating a token with SIP grants
Room configuration
Creating a token with room configuration
Token refresh
Updating permissions
Overview
For a LiveKit SDK to successfully connect to the server, it must pass an access token with the request.
This token encodes the identity of a participant, name of the room, capabilities and permissions. Access tokens are JWT-based and signed with your API secret to prevent forgery.
Access tokens also carry an expiration time, after which the server will reject connections with that token. Note: expiration time only impacts the initial connection, and not subsequent reconnects.
Creating a token
LiveKit CLI
Node.js
Go
Ruby
Java
Python
Rust
Other
lk token create
\
--api-key
<
KEY
>
\
--api-secret
<
SECRET
>
\
--identity
<
NAME
>
\
--room
<
ROOM_NAME
>
\
--join
\
--valid-for 1h
Copy
Token example
Here's an example of the decoded body of a join token:
{
"exp"
:
1621657263
,
"iss"
:
"APIMmxiL8rquKztZEoZJV9Fb"
,
"sub"
:
"myidentity"
,
"nbf"
:
1619065263
,
"video"
:
{
"room"
:
"myroom"
,
"roomJoin"
:
true
}
,
"metadata"
:
""
}
Copy
field
description
exp
Expiration time of token
nbf
Start time that the token becomes valid
iss
API key used to issue this token
sub
Unique identity for the participant
metadata
Participant metadata
attributes
Participant attributes (key/value pairs of strings)
video
Video grant, including room permissions (see below)
sip
SIP grant
Video grant
Room permissions are specified in the
video
field of a decoded join token. It may contain one or more of the following properties:
field
type
description
roomCreate
bool
Permission to create or delete rooms
roomList
bool
Permission to list available rooms
roomJoin
bool
Permission to join a room
roomAdmin
bool
Permission to moderate a room
roomRecord
bool
Permissions to use Egress service
ingressAdmin
bool
Permissions to use Ingress service
room
string
Name of the room, required if join or admin is set
canPublish
bool
Allow participant to publish tracks
canPublishData
bool
Allow participant to publish data to the room
canPublishSources
string[]
Requires
canPublish
to be true. When set, only listed source can be published. (camera, microphone, screen_share, screen_share_audio)
canSubscribe
bool
Allow participant to subscribe to tracks
canUpdateOwnMetadata
bool
Allow participant to update its own metadata
hidden
bool
Hide participant from others in the room
kind
string
Type of participant (standard, ingress, egress, sip, or agent). this field is typically set by LiveKit internals.
Example: subscribe-only token
To create a token where the participant can only subscribe, and not publish into the room, you would use the following grant:
{
...
"video"
:
{
"room"
:
"myroom"
,
"roomJoin"
:
true
,
"canSubscribe"
:
true
,
"canPublish"
:
false
,
"canPublishData"
:
false
}
}
Copy
Example: camera-only
Allow the participant to publish camera, but disallow other sources
{
...
"video"
:
{
"room"
:
"myroom"
,
"roomJoin"
:
true
,
"canSubscribe"
:
true
,
"canPublish"
:
true
,
"canPublishSources"
:
[
"camera"
]
}
}
Copy
SIP grant
In order to interact with the SIP service, permission must be granted in the
sip
field
of the JWT. It may contain the following properties:
field
type
description
admin
bool
Permission to manage SIP trunks and dispatch rules.
call
bool
Permission to make SIP calls via
CreateSIPParticipant
.
Creating a token with SIP grants
Node.js
Go
Ruby
Java
Python
Rust
import
{
AccessToken
,
SIPGrant
,
VideoGrant
}
from
'livekit-server-sdk'
;
const
roomName
=
'name-of-room'
;
const
participantName
=
'user-name'
;
const
at
=
new
AccessToken
(
'api-key'
,
'secret-key'
,
{
identity
:
participantName
,
}
)
;
const
sipGrant
:
SIPGrant
=
{
admin
:
true
,
call
:
true
,
}
;
const
videoGrant
:
VideoGrant
=
{
room
:
roomName
,
roomJoin
:
true
,
}
;
at
.
addGrant
(
sipGrant
)
;
at
.
addGrant
(
videoGrant
)
;
const
token
=
await
at
.
toJwt
(
)
;
console
.
log
(
'access token'
,
token
)
;
Copy
Room configuration
You can create an access token for a user that includes room configuration options. When a room is created for a user, the room is created using the configuration
stored in the token. For example, you can use this to
explicitly dispatch an agent
when a user joins a room.
For the full list of
RoomConfiguration
fields, see
RoomConfiguration
.
Creating a token with room configuration
Node.js
Go
Ruby
Python
Rust
For a full example of explicit agent dispatch, see the
example
in GitHub.
import
{
AccessToken
,
SIPGrant
,
VideoGrant
}
from
'livekit-server-sdk'
;
import
{
RoomAgentDispatch
,
RoomConfiguration
}
from
'@livekit/protocol'
;
const
roomName
=
'name-of-room'
;
const
participantName
=
'user-name'
;
const
agentName
=
'my-agent'
;
const
at
=
new
AccessToken
(
'api-key'
,
'secret-key'
,
{
identity
:
participantName
,
}
)
;
const
videoGrant
:
VideoGrant
=
{
room
:
roomName
,
roomJoin
:
true
,
}
;
at
.
addGrant
(
videoGrant
)
;
at
.
roomConfig
=
new
RoomConfiguration
(
agents
:
[
new
RoomAgentDispatch
(
{
agentName
:
"test-agent"
,
metadata
:
"test-metadata"
}
)
]
)
;
const
token
=
await
at
.
toJwt
(
)
;
console
.
log
(
'access token'
,
token
)
;
Copy
Token refresh
LiveKit server proactively issues refreshed tokens to connected clients, ensuring they can reconnect if disconnected. These refreshed access tokens have a 10-minute expiration.
Additionally, tokens are refreshed when there are changes to a participant's name, permissions or metadata.
Updating permissions
A participant's permissions can be updated at any time, even after they've already connected. This is useful in applications where the participant's role could change during the session, such as in a participatory livestream.
It's possible to issue a token with
canPublish: false
initially, and then
updating it to
canPublish: true
during the session. Permissions can be changed
with the
UpdateParticipant
server API.
On this page
Overview
Creating a token
Token example
Video grant
Example: subscribe-only token
Example: camera-only
SIP grant
Creating a token with SIP grants
Room configuration
Creating a token with room configuration
Token refresh
Updating permissions
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/egress/overview:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Introduction
Egress Types
Room Composite Egress
Web Egress
Participant Egress
Track Composite Egress
Track Egress
Service Architecture
Introduction
LiveKit Egress gives you a powerful and consistent set of APIs to export any room or individual tracks from a LiveKit session.
It supports recording to a MP4 file or HLS segments, as well as exporting to livestreaming services like YouTube Live, Twitch, and Facebook via RTMP(s).
For LiveKit Cloud customers, Egress is ready to use with your project without additional configuration.
When self-hosting LiveKit, Egress is a separate component that needs to be
deployed
.
Egress Types
Room Composite Egress
Export an entire room's video and/or audio using a web layout rendered by Chrome. Room composites are tied to a room's lifecycle, and will stop automatically when the room ends. Composition templates are customizable web pages that can be hosted anywhere.
Example use case: recording a meeting for team members to watch later.
Web Egress
Similar to Room Composite, but allows you to record and export any web page. Web Egress are not tied to LiveKit rooms, and can be used to record non-LiveKit content.
Example use case: restreaming content from a third-party source to YouTube and Twitch.
Participant Egress
Export a participant's video and audio together. This is a newer API and is designed to be easier to use than Track Composite Egress.
Example use case: record the teacher's video in an online class.
Track Composite Egress
Sync and export up to one audio and one video track. Will transcode and mux.
Example use case: exporting audio+video from many cameras at once during a production, for use in additional post-production.
Track Egress
Export individual tracks directly. Video tracks are not transcoded.
Example use case: streaming an audio track to a captioning service via websocket.
Service Architecture
Depending on your request type, the egress service will either launch a web template in Chrome and connect to the room
(room composite requests), or it will use the sdk directly (track and track composite requests). It uses GStreamer to
encode, and can output to a file or to one or more streams.
On this page
Introduction
Egress Types
Room Composite Egress
Web Egress
Participant Egress
Track Composite Egress
Track Egress
Service Architecture
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/ingress/overview:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Introduction
Supported Sources
Workflow
WHIP / RTMP
URL Input
API
CreateIngress
ListIngress
UpdateIngress
DeleteIngress
Using video presets
Custom settings
Enabling transcoding for WHIP sessions
Service architecture
Introduction
LiveKit Ingress lets you import video from another source into a LiveKit room. While WebRTC is a
versatile and scalable transport protocol for both media ingestion and delivery, some applications
require integrating with existing workflows or equipment that do not support WebRTC. Perhaps your
users want to publish video from OBS Studio or a dedicated hardware device, or maybe they want to stream
the content of media file hosted on a HTTP server to a room. LiveKit Ingress makes
these integrations easy.
LiveKit Ingress can automatically transcode the source media to ensure compatibility with LiveKit clients.
It can publish multiple layers with
Simulcast
.
The parameters of the different video layers can be defined at ingress creation time. Presets are provided to make encoding
settings configuration easy. The optional ability to provide custom encoding parameters enables more specialized use cases.
For LiveKit Cloud customers, Ingress is ready to use with your project without additional configuration.
When self-hosting LiveKit, Ingress is deployed as a separate service.
Supported Sources
RTMP/RTMPS
WHIP
Media files fetched from any HTTP server. The following media formats are supported:
HTTP Live Streaming (HLS)
ISO MPEG-4 (MP4)
Apple Quicktime (MOV)
Matroska (MKV/WEBM)
OGG audio
MP3 audio
M4A audio
Media served by a SRT server
Workflow
WHIP / RTMP
A typical push Ingress goes like this:
Your app creates an Ingress with
CreateIngress
API, which returns a URL and stream key of the Ingress
Your user copies and pastes the URL and key into your streaming workflow
Your user starts their stream
The Ingress Service starts transcoding their stream, or forwards media unchanged if transcoding is disabled.
The Ingress Service joins the LiveKit room and publishes the media for other Participants
When the stream source disconnects from the Ingress service, the Ingress Service participant leaves the room.
The Ingress remains valid, in a disconnected state, allowing it to be reused with the same stream key
URL Input
When pulling media from a HTTP or SRT server, Ingress has a slightly different lifecycle: it will start immediately after calling CreateIngress.
Your app creates an Ingress with
CreateIngress
API
The Ingress Service starts fetching the file or media and transcoding it
The Ingress Service joins the LiveKit room and publishes the transcoded media for other Participants
When the media is completely consumed, or if
DeleteIngress
is called, the Ingress Service participant leaves the room.
API
CreateIngress
WHIP / RTMP example
To provision an Ingress with the Ingress Service, use the CreateIngress API. It returns an
IngressInfo
object that describes the created Ingress, along with connection settings. These parameters can also be
queried at any time using the
ListIngress
API
LiveKit CLI
JavaScript
Go
Ruby
Create a file at
ingress.json
with the following content:
{
"input_type"
:
0
for RTMP
,
1
for WHIP
"name"
:
"Name of the Ingress goes here"
,
"room_name"
:
"Name of the room to connect to"
,
"participant_identity"
:
"Unique identity for the room participant the Ingress service will connect as"
,
"participant_name"
:
"Name displayed in the room for the participant"
,
"enable_transcoding"
:
true
// Transcode the input stream. Can only be false for WHIP,
}
Copy
Then create the Ingress using
lk
:
export
LIVEKIT_URL
=
https://my-livekit-host
export
LIVEKIT_API_KEY
=
livekit-api-key
export
LIVEKIT_API_SECRET
=
livekit-api-secret
lk ingress create ingress.json
Copy
URL Input example
With URL Input, Ingress will begin immediately after
CreateIngress
is called. URL_INPUT Ingress cannot be re-used.
LiveKit CLI
JavaScript
Go
Ruby
Create a file at
ingress.json
with the following content:
{
"input_type"
:
"URL_INPUT"
,
// or 2
"name"
:
"Name of the Ingress goes here"
,
"room_name"
:
"Name of the room to connect to"
,
"participant_identity"
:
"Unique identity for the room participant the Ingress service will connect as"
,
"participant_name"
:
"Name displayed in the room for the participant"
,
"url"
:
"HTTP(S) or SRT url to the file or stream"
}
Copy
Then create the Ingress using
lk
:
export
LIVEKIT_URL
=
https://my-livekit-host
export
LIVEKIT_API_KEY
=
livekit-api-key
export
LIVEKIT_API_SECRET
=
livekit-api-secret
lk ingress create ingress.json
Copy
ListIngress
LiveKit CLI
JavaScript
Go
Ruby
lk ingress list
Copy
The optional
--room
option allows to restrict the output to the Ingress associated to a given room. The
--id
option can check if a specific ingress is active.
UpdateIngress
The Ingress configuration can be updated using the
UpdateIngress
API. This enables
the ability to re-use the same Ingress URL to publish to different rooms. Only reusable Ingresses,
such as RTMP or WHIP, can be updated.
LiveKit CLI
JavaScript
Go
Ruby
Create a file at
ingress.json
with the fields to be updated.
{
"ingress_id"
:
"Ingress ID of the Ingress to update"
,
"name"
:
"Name of the Ingress goes here"
,
"room_name"
:
"Name of the room to connect to"
,
"participant_identity"
:
"Unique identity for the room participant the Ingress service will connect as"
,
"participant_name"
:
"Name displayed in the room for the participant"
}
Copy
The only required field is
ingress_id
. Non provided fields are left unchanged.
lk ingress update ingress.json
Copy
DeleteIngress
An Ingress can be reused multiple times. When not needed anymore, it can be deleted using the
DeleteIngress
API:
LiveKit CLI
JavaScript
Go
Ruby
lk ingress delete
<
INGRESS_ID
>
Copy
Using video presets
The Ingress service can transcode the media being received. This is the only supported behavior for RTMP and URL inputs. WHIP ingresses are not transcoded by default, but transcoding can be enabled by setting the
enable_transcoding
parameter. When transcoding is enabled, The default settings enable
video simulcast
to ensure media can be consumed by all viewers, and should be suitable for most use cases. In some situations however, adjusting these settings may be desirable to match source content or the viewer conditions better. For this purpose, LiveKit Ingress defines several presets, both for audio and video. Presets define both the characteristics of the media (codec, dimesions, framerate, channel count, sample rate) and the bitrate. For video, a single preset defines the full set of simulcast layers.
A preset can be chosen at Ingress creation time from the
constants in the Ingress protocol definition
:
LiveKit CLI
JavaScript
Go
Ruby
Create a file at
ingress.json
with the following content:
{
"name"
:
"Name of the egress goes here"
,
"room_name"
:
"Name of the room to connect to"
,
"participant_identity"
:
"Unique identity for the room participant the Ingress service will connect as"
,
"participant_name"
:
"Name displayed in the room for the participant"
"video"
:
{
"name"
:
"track name"
,
"source"
:
"SCREEN_SHARE"
,
"preset"
:
"Video preset enum value"
}
,
"audio"
:
{
"name"
:
"track name"
,
"source"
:
"SCREEN_SHARE_AUDIO"
,
"preset"
:
"Audio preset enum value"
}
}
Copy
Then create the Ingress using
lk
:
lk ingress create ingress.json
Copy
Custom settings
For specialized use cases, it is also possible to specify fully custom encoding parameters. In this case, all video layers need to be defined if simulcast is desired.
LiveKit CLI
JavaScript
Go
Ruby
Create a file at
ingress.json
with the following content:
{
"name"
:
"Name of the egress goes here"
,
"room_name"
:
"Name of the room to connect to"
,
"participant_identity"
:
"Unique identity for the room participant the Ingress service will connect as"
,
"participant_name"
:
"Name displayed in the room for the participant"
,
"video"
:
{
"options"
:
{
"video_codec"
:
"video codec ID from the [VideoCodec enum](https://github.com/livekit/protocol/blob/main/protobufs/livekit_models.proto)"
,
"frame_rate"
:
"desired framerate in frame per second"
,
"layers"
:
[
{
"quality"
:
"ID for one of the LOW, MEDIUM or HIGH VideoQualitu definitions"
,
"witdh"
:
"width of the layer in pixels"
,
"height"
:
"height of the layer in pixels"
,
"bitrate"
:
"video bitrate for the layer in bit per second"
}
]
}
}
,
"audio"
:
{
"options"
:
{
"audio_codec"
:
"audio codec ID from the [AudioCodec enum](https://github.com/livekit/protocol/blob/main/protobufs/livekit_models.proto)"
,
"bitrate"
:
"audio bitrate for the layer in bit per second"
,
"channels"
:
"audio channel count, 1 for mono, 2 for stereo"
,
"disable_dtx"
:
"wether to disable the [DTX feature](https://www.rfc-editor.org/rfc/rfc6716#section-2.1.9) for the OPUS codec"
}
}
}
Copy
Then create the Ingress using
lk
:
lk ingress create ingress.json
Copy
Enabling transcoding for WHIP sessions
By default, WHIP ingress sessions forward incoming audio and video media unmodified from the source to LiveKit clients. This behavior allows the lowest possible end to end latency between the media source and the viewers. This however requires the source encoder to be configured with settings that are compatible with all the subscribers, and ensure the right trade offs between quality and reach for clients with variable connection quality. This is best achieved when the source encoder is configured with simulcast enabled.
If the source encoder cannot be setup easily to achieve such tradeoffs, or if the available uplink bandwidth is insufficient to send all required simulcast layers, WHIP ingresses can be configured to transcode the source media similarly to other source types. This is done by setting the
enable_transcoding
option on the ingress. The encoder settings can then be configured in the
audio
and
video
settings in the same manner as for other inputs types.
LiveKit CLI
JavaScript
Go
Ruby
Create a file at
ingress.json
with the following content:
{
"input_type"
:
1
(WHIP only)
"name"
:
"Name of the egress goes here"
,
"room_name"
:
"Name of the room to connect to"
,
"participant_identity"
:
"Unique identity for the room participant the Ingress service will connect as"
,
"participant_name"
:
"Name displayed in the room for the participant"
,
"enable_transcoding"
:
true
"video"
:
{
"name"
:
"track name"
,
"source"
:
"SCREEN_SHARE"
,
"preset"
:
"Video preset enum value"
}
,
"audio"
:
{
"name"
:
"track name"
,
"source"
:
"SCREEN_SHARE_AUDIO"
,
"preset"
:
"Audio preset enum value"
}
}
Copy
Then create the Ingress using
lk
:
lk ingress create ingress.json
Copy
Service architecture
LiveKit Ingress exposes public RTMP and WHIP endpoints streamers can connect to. On initial handshake, the Ingress service validates the incoming request and retrieves the corresponding Ingress metadata, including what LiveKit room the stream belongs to. The Ingress server then sets up a GStreamer based media processing pipeline to transcode the incoming media to a format compatible with LiveKit WebRTC clients, publishes the resulting media to the LiveKit room.
On this page
Introduction
Supported Sources
Workflow
WHIP / RTMP
URL Input
API
CreateIngress
ListIngress
UpdateIngress
DeleteIngress
Using video presets
Custom settings
Enabling transcoding for WHIP sessions
Service architecture
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/overview:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Realtime API
LLM integrations
STT integrations
TTS integrations
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Integration guides
.
v1.0 for Node.js is coming soon.
The Agents framework supports integrations for providers using plugins. This topic describes LLM, STT, and TTS provider plugins. Additional plugins include support for Retrieval-Augmented Generation (RAG), Natural Language Toolkit (NLTK), LlamaIndex, Silero VAD, turn detection, and more. To see the list of additional plugins and learn more about how LiveKit plugins work, see
Working with plugins
.
If you want to use a provider not listed in the following sections, contributions for plugins are always welcome. To learn more, see the the guidelines for contributions to the
Python repository
or the
Node.js repository
.
Realtime API integrations
Realtime APIs are designed for ulta-low-latency AI responses and use multimodal models. They can be better at understanding a user and their emotions resulting in more natural interactions.
OpenAI Realtime API
Support for the OpenAI Realtime API.
Gemini Live API
Support for Google's Gemini Live API.
Azure OpenAI Realtime API
OpenAI Realtime API hosted on Azure.
LLM integrations
You can create an instance of an LLM to use in a
VoicePipelineAgent
for the following providers using a plugin:
Anthropic
Azure
Amazon Web Services (AWS)
Cerebras
Deepseek
Fireworks
Gemini
Groq
LlamaIndex
Octo
Ollama
Openai
Perplexity
Telnyx
Together
xAI
STT integrations
LiveKit plugins support the following STT providers. Create an instance of an STT to use in a `VoicePipelineAgent` or as a standalone transcription service.
Provider
Supported frameworks
AssemblyAI
Python
Azure
Python, Node.js
AWS
Python
Clova
Python
Deepgram
Python, Node.js
fal
Python
Google
Python
Groq
Python, Node.js
OpenAI
Python, Node.js
Speechmatics
Python
TTS integrations
LiveKit plugins support the following TTS providers. Create an instance of a TTS to use in a `VoicePipelineAgent` or for speech generation in your apps.
Provider
Supported frameworks
Azure
Python
AWS
Python
Cartesia
Python, Node.js
Deepgram
Python, Node.js
Elevenlabs
Python, Node.js
Neuphonic
Python
OpenAI
Python, Node.js
PlayHT
Python
Rime
Python
On this page
Realtime API
LLM integrations
STT integrations
TTS integrations
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/openai/overview:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
LLM
STT
TTS
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
OpenAI integration
.
v1.0 for Node.js is coming soon.
Overview
LiveKit's OpenAI integration provides support for LLM, STT, TTS, and the OpenAI Realtime API. The
Quick reference
section in this topic describes using OpenAI for LLM,
STT, and TTS. For a guide on using the Realtime API, see
the
OpenAI Realtime API integration guide
.
Use OpenAI STT, TTS, and LLM to create agents using the
VoicePipelineAgent
class. To use the Realtime API, you can create an agent using the
MultimodalAgent
class.
Note
The following quickstart guides are available to get you started creating an AI voice assistant with OpenAI:
Voice agent quickstart
using the
VoicePipelineAgent
class.
Speech-to-speech quickstart
using the
MultimodalAgent
class.
Quick reference
The following sections provide a quick reference for integrating OpenAI with LiveKit. For the complete
reference, see the links provided in each section.
LLM
LiveKit's OpenAI plugin provides support for creating an instance of an LLM class to be used in a
VoicePipelineAgent
.
LLM class usage
Create an instance of OpenAI LLM:
agent.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
llm
openai_llm
=
llm
.
LLM
(
model
=
"gpt-4o"
,
temperature
=
0.8
,
)
Copy
LLM parameters
This section describes some of the available parameters. For a complete reference of all available parameters,
see the
plugin reference
.
model
string | ChatModels
Optional
Default:
gpt-4o
#
ID of the model to use for inference. For a list of supported models, see
ChatModels
in the respective GitHub repository:
Python
,
Node.js
.
To learn more about available models, see
Models
.
api_key
string
Optional
Env:
OPENAI_API_KEY
#
OpenAI API key. Required if the environment variable is not set.
temperature
float
Optional
Default:
1.0
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more, see
chat completions
.
STT
LiveKit's OpenAI plugin allows you to create an instance of OpenAI STT that can be used as the first stage in a
VoicePipelineAgent
or as a standalone transcription service.
STT usage
Create an OpenAI STT:
agent.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
stt
openai_stt
=
stt
.
STT
(
language
=
"en"
,
model
=
"whisper-1"
,
)
Copy
STT parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see
the
plugin reference
.
model
WhisperModels | string
Optional
Default:
whisper-1
#
ID of the model to use for speech recognition. To learn more, see
Whisper
.
language
string
Optional
Default:
en
#
Language of input audio in
ISO-639-1
format. See OpenAI's documentation for a list of
supported languages
.
TTS
LiveKit's OpenAI plugin allows you to create an instance of OpenAI TTS that can be used in a
VoicePipelineAgent
or as a standalone speech generator.
TTS usage
Create an OpenAI TTS:
agent.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
tts
openai_tts
=
tts
.
TTS
(
model
=
"tts-1"
,
voice
=
"nova"
,
)
Copy
TTS parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
model
TTSModels | string
Optional
Default:
tts-1
#
ID of the model to use for speech generation. To learn more, see
TTS models
.
voice
TTSVoice | string
Optional
Default:
alloy
#
ID of the voice used for speech generation. To learn more, see
TTS voice options
.
On this page
Overview
Quick reference
LLM
STT
TTS
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Introduction
Use cases
Framework overview
How agents connect to LiveKit
Getting started
Introduction
The Agents framework allows you to add a Python or Node.js program to any LiveKit room as a full realtime participant. The SDK includes a complete set of tools and abstractions that make it easy to feed realtime media and data through an AI pipeline that works with any provider, and to publish realtime results back to the room.
If you want to get your hands on the code right away, follow this quickstart guide. It takes just a few minutes to build your first voice agent.
Voice AI quickstart
Build a simple voice assistant with Python in less than 10 minutes.
GitHub repository
Python source code and examples for the LiveKit Agents SDK.
SDK reference
Python reference docs for the LiveKit Agents SDK.
Use cases
Some applications for agents include:
Multimodal assistant
: Talk, text, or screen share with an AI assistant.
Telehealth
: Bring AI into realtime telemedicine consultations, with or without humans in the loop.
Call center
: Deploy AI to the front lines of customer service with inbound and outbound call support.
Realtime translation
: Translate conversations in realtime.
NPCs
: Add lifelike NPCs backed by language models instead of static scripts.
Robotics
: Put your robot's brain in the cloud, giving it access to the most powerful models.
The following
recipes
demonstrate some of these use cases:
Medical Office Triage
Agent that triages patients based on symptoms and medical history.
Restaurant Agent
A restaurant front-of-house agent that can take orders, add items to a shared cart, and checkout.
Company Directory
Build a AI company directory agent. The agent can respond to DTMF tones and voice prompts, then redirect callers.
Pipeline Translator
Implement translation in the processing pipeline.
Framework overview
Your agent code operates as a stateful, realtime bridge between powerful AI models and your users. While AI models typically run in data centers with reliable connectivity, users often connect from mobile networks with varying quality.
WebRTC ensures smooth communication between agents and users, even over unstable connections. LiveKit WebRTC is used between the frontend and the agent, while the agent communicates with your backend using HTTP and WebSockets. This setup provides the benefits of WebRTC without its typical complexity.
The agents SDK includes components for handling the core challenges of realtime voice AI, such as streaming audio through an STT-LLM-TTS pipeline, reliable turn detection, handling interruptions, and LLM orchestration. It supports plugins for most major AI providers, with more continually added. The framework is fully open source and supported by an active community.
Other framework features include:
Voice, video, and text
: Build agents that can process realtime input and produce output in any modality.
Tool use
: Define tools that are compatible with any LLM, and even forward tool calls to your frontend.
Multi-agent handoff
: Break down complex workflows into simpler tasks.
Extensive integrations
: Integrate with nearly every AI provider there is for LLMs, STT, TTS, and more.
State-of-the-art turn detection
: Use the custom turn detection model for lifelike conversation flow.
Made for developers
: Build your agents in code, not configuration.
Production ready
: Includes built-in worker orchestration, load balancing, and Kubernetes compatibility.
Open source
: The framework and entire LiveKit ecosystem are open source under the Apache 2.0 license.
How agents connect to LiveKit
When your agent code starts, it first registers with a LiveKit server (either
self hosted
or
LiveKit Cloud
) to run as a "worker" process. The worker waits until it receives a dispatch request. To fulfill this request, the worker boots a "job" subprocess which joins the room. By default, your workers are dispatched to each new room created in your LiveKit project. To learn more about workers, see the
Worker lifecycle
guide.
After your agent and user join a room, the agent and your frontend app can communicate using LiveKit WebRTC. This enables reliable and fast realtime communication in any network conditions. LiveKit also includes full support for telephony, so the user can join the call from a phone instead of a frontend app.
To learn more about how LiveKit works overall, see the
Intro to LiveKit
guide.
Getting started
Follow these guides to learn more and get started with LiveKit Agents.
Voice AI quickstart
Build a simple voice assistant with Python in less than 10 minutes.
Recipes
A comprehensive collection of examples, guides, and recipes for LiveKit Agents.
Intro to LiveKit
An overview of the LiveKit ecosystem.
Web and mobile frontends
Put your agent in your pocket with a custom web or mobile app.
Telephony integration
Your agent can place and receive calls with LiveKit's SIP integration.
Building voice agents
Comprehensive documentation to build advanced voice AI apps with LiveKit.
Worker lifecycle
Learn how to manage your agents with workers and jobs.
Deploying to production
Guide to deploying your voice agent in a production environment.
Integration guides
Explore the full list of AI providers available for LiveKit Agents.
On this page
Introduction
Use cases
Framework overview
How agents connect to LiveKit
Getting started
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Introduction
Concepts
SIP participant
Trunks
Dispatch rules
Service architecture
Using LiveKit SIP
Noise cancellation for calls
Next steps
Introduction
LiveKit SIP bridges the gap between traditional telephony and modern digital communication.
It enables seamless interaction between traditional phone systems and LiveKit rooms. You can use
LiveKit SIP to accept calls and make calls. When you add LiveKit Agents, you can use an AI voice
agent to handle your inbound and outbound calls.
Concepts
LiveKit SIP extends the
core primitives
— participant, room,
and track — to include two additional concepts specific to SIP: trunks and dispatch rules.
These concepts are represented by objects created through the
API
and control how calls are handled.
SIP participant
Each caller, callee, and AI voice agent that participates in a call is a LiveKit participant. A SIP participant is like any other participant and can be managed using the
participant APIs
. They have the same
attributes and metadata
as any other participant, and have additional
SIP specific attributes
.
For inbound calls, a SIP participant is automatically created for each caller.
To make an outbound call, you create a SIP participant using the
CreateSIPParticipant
API to make the call.
Trunks
LiveKit SIP trunks are the bridge between your SIP provider and LiveKit. LiveKit requires a SIP trunk configured with your phone
number provider. Configuration settings vary based on your app and whether you're accepting incoming calls or making outgoing calls
or both.
An
inbound trunk
is configured to accept incoming calls and can be limited to
specific IP addresses or phone numbers. An
outbound trunk
is configured for
outgoing calls.
Note
The same SIP provider trunk can be associated with both an inbound and an outbound trunk in LiveKit.
You only need to create an inbound or outbound trunk
once
.
Dispatch rules
Dispatch Rules
are associated with a specific trunk and control how inbound calls are
dispatched to LiveKit rooms. All callers can be placed in the same room or different rooms based on the dispatch
rules. Multiple dispatch rules can be associated with the same trunk as long as each rule has a different pin.
Dispatch rules can also be used to add custom participant attributes to
SIP participants
.
Service architecture
LiveKit SIP relies on the following services:
SIP trunking provider for your phone number.
LiveKit server (part of LiveKit Cloud) for API requests, managing and verifying SIP trunks and
dispatch rules, and creating
participants and rooms for calls (part of LiveKit Cloud).
LiveKit SIP (part of LiveKit Cloud) to respond to SIP requests, mediate trunk authentication, and
match dispatch rules.
If you use LiveKit Cloud, LiveKit SIP is ready to use with your project without any
additional configuration. If you're self hosting LiveKit, the SIP service needs to be deployed
separately. To learn more about self hosting, see
SIP server
.
Using LiveKit SIP
The LiveKit SIP SDK is available in multiple languages. To learn more, see
SIP API
.
LiveKit SIP has been tested with the following SIP providers:
Note
LiveKit SIP is designed to work with all SIP providers. However, compatibility testing is
limited to the providers below.
Twilio
Telnyx
Exotel
Plivo
LiveKit SIP supports the following functionality:
Feature
Description
DTMF
You can configure DTMF when making outbound calls by adding them to the
CreateSIPParticipant
request.
To learn more, see
Making a call with extension codes (DTMF)
.
SIP REFER
You can transfer calls using the
TransferSIPParticipant
API. Calls can be transferred to any valid
telephone number or SIP URI. To learn more, see
Cold transfer
.
SIP headers
You can map custom
X-*
SIP headers to participant attributes. For example, custom headers can be used
to route calls to different workflows. To learn more, see
Custom attributes
.
Noise cancellation
You can enable noise cancellation for callers and callees using Krisp. To learn more,
see
Noise cancellation for calls
.
Noise cancellation for calls
Krisp
noise cancellation uses AI models to identify and remove background noise in realtime. This improves
the quality of calls that occur in noisy environments. For LiveKit SIP applications that use agents, noise cancellation improves the
quality and clarity of user speech for turn detection, transcriptions, and recordings.
For incoming calls, see the
inbound trunks documentation
for the
krisp_enabled
attribute.
For outgoing calls, see the
CreateSIPParticipant
documentation for the
krisp_enabled
attribute used during
outbound call creation
.
Next steps
See the following guides to get started with LiveKit SIP:
SIP trunk setup
Purchase a phone number and configure your SIP trunking provider for LiveKit SIP.
Accepting inbound calls
Learn how to accept inbound calls with LiveKit SIP.
Making outbound calls
Learn how to make outbound calls with LiveKit SIP.
Voice AI telephony guide
Create an AI agent integrated with telephony.
On this page
Introduction
Concepts
SIP participant
Trunks
Dispatch rules
Service architecture
Using LiveKit SIP
Noise cancellation for calls
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Agent sessions
Voice AI providers
Capabilities
Overview
Building a great voice AI app requires careful orchestration of multiple components. In addition, the voice AI end-user experience is particularly sensitive to latency and responsiveness. LiveKit Agents provides dedicated abstractions to simplify development while giving you full control over the underlying code.
Agent sessions
The
AgentSession
is the main orchestrator for your voice AI app. The session is responsible for collecting user input, managing the voice pipeline, invoking the LLM, and sending the output back to the user.
Each session requires at least one
Agent
to orchestrate. The agent is responsible for defining the core AI logic - instructions, tools, etc - of your app. The framework supports the design of custom
workflows
to orchestrate handoff and delegation between multiple agents.
The following example shows how to begin a simple single-agent session:
from
livekit
.
agents
import
AgentSession
,
Agent
,
RoomInputOptions
from
livekit
.
plugins
import
openai
,
cartesia
,
deepgram
,
noise_cancellation
,
silero
from
livekit
.
plugins
.
turn_detector
.
multilingual
import
MultilingualModel
session
=
AgentSession
(
stt
=
deepgram
.
STT
(
)
,
llm
=
openai
.
LLM
(
)
,
tts
=
cartesia
.
TTS
(
)
,
vad
=
silero
.
VAD
.
load
(
)
,
turn_detection
=
turn_detector
.
MultilingualModel
(
)
,
)
await
session
.
start
(
room
=
ctx
.
room
,
agent
=
Agent
(
instructions
=
"You are a helpful voice AI assistant."
)
,
room_input_options
=
RoomInputOptions
(
noise_cancellation
=
noise_cancellation
.
BVC
(
)
,
)
,
)
Copy
Voice AI providers
You can choose from a variety of providers for each part of the voice pipeline to fit your needs. The framework supports both high-performance STT-LLM-TTS pipelines and speech-to-speech models. In either case, it automatically manages interruptions, transcription forwarding, turn detection, and more.
You may add these components to the
AgentSession
, where they act as global defaults within the app, or to each individual
Agent
if needed.
TTS
Text-to-speech integrations
STT
Speech-to-text integrations
LLM
Language model integrations
Multimodal
Realtime multimodal APIs
Capabilities
The following guides, in addition to others in this section, cover the core capabilities of the
AgentSession
and how to leverage them in your app.
Workflows
Orchestrate complex tasks among multiple agents.
Tool definition & use
Use tools to call external services, inject custom logic, and more.
Pipeline nodes
Add custom behavior to any component of the voice pipeline.
On this page
Overview
Agent sessions
Voice AI providers
Capabilities
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/worker/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Worker options
Overview
When you start your app with
python main.py dev
, it registers itself as a
worker
with LiveKit server. LiveKit server manages dispatching your agents to rooms with users by sending requests to available workers.
A
LiveKit session
is one or more participants in a
room
. A LiveKit session is often referred to simply as a "room." When a user connects to a room, a worker fulfills the request to dispatch an agent to the room.
An overview of the worker lifecycle is as follows:
Worker registration
: Your agent code registers itself as a "worker" with LiveKit server, then waits on standby for requests.
Job request
: When a user connects to a room, LiveKit server sends a request to an available worker. A worker accepts and starts a new process to handle the job. This is also known as
agent dispatch
.
Job
: The job initiated by your
entrypoint
function. This is the bulk of the code and logic you write. To learn more, see
Job lifecycle
.
LiveKit session close
: By default, a room is automatically closed when the last non-agent participant leaves. Any remaining agents disconnect. You can also
end the session
manually.
The following diagram shows the worker lifecycle:
Some additional features of workers include the following:
Workers automatically exchange availability and capacity information with the LiveKit server, enabling load balancing of incoming requests.
Each worker can run multiple jobs simultaneously, running each in its own process for isolation. If one crashes, it won’t affect others running on the same worker.
When you deploy updates, workers gracefully drain active LiveKit sessions before shutting down, ensuring no sessions are interrupted mid-call.
Worker options
You can change the permissions, dispatch rules, add prewarm functions, and more through
WorkerOptions
.
On this page
Overview
Worker options
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Installing plugins
Using plugins
OpenAI API compatibility
Core plugins
Additional plugins
Build your own plugin
Overview
LiveKit Agents includes support for a wide variety of AI providers, from the largest research companies to emerging startups.
The open source plugin interface makes it easy to adopt the best AI providers for your app, without needing customized code for each.
Installing plugins
Each provider is available as a separate plugin package, included as an optional dependency on the base SDK for Python.
For example, to install the SDK with the Cartesia, Deepgram, and OpenAI plugins, run the following command:
pip
install
"livekit-agents[cartesia,deepgram,openai]~=1.0"
Copy
You may also install plugins as individual packages. For example, this is equivalent to the previous command:
pip
install
\
"livekit-agents~=1.0"
\
"livekit-plugins-cartesia~=1.0"
\
"livekit-plugins-deepgram~=1.0"
\
"livekit-plugins-openai~=1.0"
Copy
Using plugins
The AgentSession class accepts plugins as arguments using a standard interface. Each plugin loads its own associated API key from environment variables. For instance, the following code creates an AgentSession that uses the OpenAI, Cartesia, and Deepgram plugins installed in the preceding section:
main.py
.env
from
livekit
.
plugins
import
openai
,
cartesia
,
deepgram
session
=
AgentSession
(
llm
=
openai
.
LLM
(
model
=
"gpt-4o"
)
,
tts
=
cartesia
.
TTS
(
model
=
"sonic-english"
)
,
stt
=
deepgram
.
STT
(
model
=
"nova-2"
)
,
)
Copy
OpenAI API compatibility
Many providers have standardized around the OpenAI API format for chat completions and more. The LiveKit Agents OpenAI plugin provides easy compatibility with many of these providers through special methods which load the correct API key from environment variables. For instance, to use Cerebras instead of OpenAI, you can use the following code:
main.py
.env
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_cerebras
(
model
=
"llama-3.1-70b-versatile"
)
,
# ... stt, tts, etc ..
)
Copy
Core plugins
The following are the core plugin types used in LiveKit Agents, to handle the primary voice AI tasks. Many providers are available for most functions.
Realtime models
Plugins for multimodal speech-to-speech models like the OpenAI Realtime API.
Large language models (LLM)
Plugins for AI models from OpenAI, Anthropic, and more.
Speech-to-text (STT)
Plugins for speech-to-text solutions like Deepgram, Whisper, and more.
Text-to-speech (TTS)
Plugins for text-to-speech solutions like Cartesia, ElevenLabs, and more.
Additional plugins
LiveKit Agents also includes the following additional specialized plugins, which are recommended for most voice AI use cases. Each runs locally and requires no additional API keys.
Silero VAD
Voice activity detection with Silero VAD.
LiveKit turn detector
A custom LiveKit model for improved end-of-turn detection.
Enhanced noise cancellation
LiveKit Cloud enhanced noise cancellation to improve voice AI performance.
Build your own plugin
The LiveKit Agents plugin framework is extensible and community-driven. Your plugin can integrate with new providers or directly load models for local inference. LiveKit especially welcomes new TTS, STT, and LLM plugins.
To learn more, see the guidelines for contributions to
the
Python
and
Node.js
SDKs.
On this page
Overview
Installing plugins
Using plugins
OpenAI API compatibility
Core plugins
Additional plugins
Build your own plugin
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/quickstarts/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
SDK quickstart guides
LiveKit has SDKs for most major platforms and languages. Quickly integrate realtime AI, audio, or video into your app by selecting your platform below.
Web SDKs
For browser-based applications.
Next.js
React
JavaScript
Unity (WebGL)
Native SDKs
For native applications on mobile, desktop, and more.
Swift
Android (Compose)
Android
Flutter
React Native
Expo
Other SDKs
Don’t see your platform listed?
View the full
list of supported SDKs
.
Integrate with a
telephone system using SIP
.
Join the
LiveKit Slack community
to share what you’re building.
On this page
Web SDKs
Native SDKs
Other platforms
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/cloud/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Why Choose LiveKit Cloud?
Comparing Open Source and Cloud
LiveKit Cloud is a fully-managed, globally distributed mesh network of LiveKit servers that provides all the power of the open-source platform with none of the operational complexity. It allows you to focus on building your application while LiveKit handles deployment, scaling, and maintenance.
Dashboard
Sign up for LiveKit Cloud to manage your projects, view analytics, and configure your LiveKit Cloud deployment.
Pricing
View LiveKit Cloud pricing plans and choose the right option for your application's needs.
Why Choose LiveKit Cloud?
Zero operational overhead
: No need to manage servers, scaling, or infrastructure.
Global edge network
: Users connect to the closest server for minimal latency.
Unlimited scale
: Support for rooms with unlimited participants through our mesh architecture.
Enterprise-grade reliability
: 99.99% uptime guarantee with redundant infrastructure.
Comprehensive analytics
: Monitor usage, performance, and quality metrics through the Cloud dashboard.
Same APIs and SDKs
: Use the exact same code whether you're on Cloud or self-hosted.
LiveKit Cloud runs the same open-source servers that you can find on GitHub. It provides the same APIs and supports all of the same SDKs. An open source user can migrate to Cloud, and a Cloud customer can switch to self-hosted at any moment. As far as your code is concerned, the only difference is the URL that it connects to.
For more details on LiveKit Cloud's architecture, see
Cloud Architecture
.
Comparing Open Source and Cloud
When building with LiveKit, you can either self-host the open-source server or use the managed LiveKit Cloud service:
Open Source
Cloud
Realtime features
Full support
Full support
Egress (recording, streaming)
Full support
Full support
Ingress (RTMP, WHIP, SRT ingest)
Full support
Full support
SIP (telephony integration)
Full support
Full support
Agents framework
Full support
Full support
Who manages it
You
LiveKit
Architecture
Single-home SFU
Mesh SFU
Connection model
Users in the same room connect to the same server
Each user connects to the closest server
Max users per room
Up to ~3,000
No limit
Analytics & telemetry
N/A
Cloud dashboard
Uptime guarantees
N/A
99.99%
On this page
Why Choose LiveKit Cloud?
Comparing Open Source and Cloud
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/accepting-calls:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Inbound call workflow
Setup for accepting calls
SIP trunking provider setup
LiveKit SIP configuration
Next steps
Inbound call workflow
When an inbound call is received, your SIP trunking provider sends a text-based
INVITE request to LiveKit SIP. The SIP service checks authorization credentials configured for
the LiveKit trunk with the credentials configured on your provider's SIP trunk and looks for a
matching dispatch rule. If there's a matching dispatch rule, a SIP participant is created for the
caller and put into a LiveKit room.
Depending on the dispatch rule, other participants (for example, a voice agent or other users) might
join the room.
User dials the SIP trunking provider phone number.
SIP trunking provider connects caller to LiveKit SIP.
LiveKit SIP authenticates the trunk credentials and finds a matching dispatch rule.
LiveKit server creates a SIP participant for the caller and places them in a LiveKit room
(per the dispatch rule).
User hears dial tone until LiveKit SIP responds to the call:
If the dispatch rule has a pin, prompts the user with
"Please enter room pin and press hash to confirm."
Incorrect pin: "No room matched the pin you entered." Call is disconnected with a tone.
Correct pin: "Entering room now."
User continues to hear a dial tone until another participant publishes tracks to the room.
Setup for accepting calls
The following are required to accept an inbound SIP call.
SIP trunking provider setup
Purchase a phone number from a SIP provider.
For a list of tested providers, see the table in
Using LiveKit SIP
.
Configure SIP trunking with the provider to send SIP traffic to your LiveKit SIP instance.
For instructions for setting up a SIP trunk,
see
Configuring a SIP provider trunk
.
LiveKit SIP configuration
Create an
inbound trunk
associated with your SIP provider phone number.
You only need to create one inbound trunk for each SIP provider phone number.
Create a
dispatch rule
. The dispatch rules dictate how SIP participants and
LiveKit rooms are created for incoming calls. The rules can include whether a caller needs to enter
a pin code to join a room and any custom metadata or attributes to be added to SIP participants.
Next steps
See the following guide to create an AI agent to receive inbound calls.
Voice AI telephony guide
Create an AI agent to receive inbound calls.
On this page
Inbound call workflow
Setup for accepting calls
SIP trunking provider setup
LiveKit SIP configuration
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/trunk-inbound:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Inbound trunk example
Accepting calls from specific phone numbers
List inbound trunks
Note
To create and configure an outbound trunk, see
Outbound trunk
.
After you purchase a phone number and
configure your SIP trunking provider
,
you must create an inbound trunk and
dispatch rule
to accept incoming calls. The inbound trunk allows
you to limit incoming calls to those coming from your SIP trunking provider.
You can also configure additional properties for all incoming calls that match the trunk including SIP headers,
participant metadata and attributes, and session properties. For a full list of available parameters, see
CreateSIPInboundTrunk
.
To learn more about LiveKit SIP, see
SIP overview
. To learn more about SIP API endpoints and types, see
SIP API
.
Inbound trunk example
The following examples create an inbound trunk that accepts calls made to the number
+1-510-555-0100
and enables Krisp
noise cancellation
. This phone number is the number purchased from your SIP
trunking provider.
LiveKit CLI
Node.js
Python
Ruby
Go
Create a file named
inbound-trunk.json
with the following content:
Telnyx
Twilio
{
"trunk"
:
{
"name"
:
"My trunk"
,
"numbers"
:
[
"+15105550100"
]
,
"krisp_enabled"
:
true
}
}
Copy
Important
The leading
+
in the phone number assumes the
Destination Number Format
is set to
+E.164
for your Telnyx number.
Then create the inbound trunk using
lk
:
lk sip inbound create inbound-trunk.json
Copy
Accepting calls from specific phone numbers
The configuration for inbound trunk accepts inbound calls to number
+1-510-555-0100
from caller numbers
+1-310-555-1100
and
+1-714-555-0100
.
Important
Remember to replace the numbers in the example with actual phone numbers when creating your trunks.
LiveKit CLI
Node.js
Python
Ruby
Go
Telnyx
Twilio
{
"trunk"
:
{
"name"
:
"My trunk"
,
"numbers"
:
[
"+15105550100"
]
,
"allowed_numbers"
:
[
"+13105550100"
,
"+17145550100"
]
}
}
Copy
Important
The leading
+
in the phone number assumes the
Destination Number Format
is set to
+E.164
for your Telnyx number.
Tip
Allowed caller numbers can also be filtered with a
Dispatch Rule
.
List inbound trunks
Use the
ListSIPInboundTrunk
API to list all inbound trunks and trunk parameters.
LiveKit CLI
Node.js
Python
Ruby
Go
lk sip inbound list
Copy
On this page
Inbound trunk example
Accepting calls from specific phone numbers
List inbound trunks
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/dispatch-rule:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Caller dispatch rule (individual)
Direct dispatch rule
Pin-protected room
Callee dispatch rule
Setting custom attributes on inbound SIP participants
Setting custom metadata on inbound SIP participants
List dispatch rules
To create a dispatch rule with the SIP service, use the
CreateSIPDispatchRule
API.
It returns a
SIPDispatchRuleInfo
object that describes the created
SIPDispatchRule
.
By default, a dispatch rule is matched against all your trunks and makes a caller's phone number visible to others in the
room. You can change these default behaviors using dispatch rule options. See the
CreateSIPDispatchRule
API reference for full list of available options.
To learn more about SIP and dispatch rules, see
SIP overview
. To learn more about SIP API endpoints and types, see
SIP API
.
Caller dispatch rule (individual)
An
SIPDispatchRuleIndividual
rule creates a new room for each caller.
The name of the created room is the phone number of the caller plus a random suffix.
You can optionally add a specific prefix to the room name by using the
roomPrefix
option.
The following examples dispatch callers into individual rooms prefixed with
call-
,
and
dispatches an agent
named
inbound-agent
to newly created rooms:
LiveKit CLI
Node.js
Python
Ruby
Go
{
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call-"
}
}
,
"roomConfig"
:
{
"agents"
:
[
{
"agentName"
:
"inbound-agent"
,
"metadata"
:
"job dispatch metadata"
}
]
}
}
Copy
Note
When the
trunk_ids
field is omitted, the dispatch rule matches calls from all inbound trunks.
Direct dispatch rule
A direct dispatch rule places all callers into a specified room.
You can optionally protect room access by adding a pin in the
pin
field:
In the following examples, all calls are immediately connected to room
open-room
on LiveKit.
LiveKit CLI
Node.js
Python
Ruby
Go
Create a file named
dispatch-rule.json
and add the following:
{
"rule"
:
{
"dispatchRuleDirect"
:
{
"roomName"
:
"open-room"
}
}
}
Copy
Create the dispatch rule using
lk
:
lk sip dispatch create dispatch-rule.json
Copy
Pin-protected room
Add a
pin
to a room to require callers to enter a pin to connect to a room in LiveKit. The following
example requires callers to enter
12345#
on the phone to enter
safe-room
:
{
"trunk_ids"
:
[
]
,
"rule"
:
{
"dispatchRuleDirect"
:
{
"roomName"
:
"safe-room"
,
"pin"
:
"12345"
}
}
}
Copy
Callee dispatch rule
This creates a dispatch rule that puts callers into rooms based on the called number.
The name of the room is the called phone number plus an optional prefix (if
roomPrefix
is set).
You can optionally add a random suffix for each caller by setting
randomize
to true, making a separate room per caller.
LiveKit CLI
Node.js
Python
Ruby
Go
{
"rule"
:
{
"dispatchRuleCallee"
:
{
"roomPrefix"
:
"number-"
,
"randomize"
:
false
}
}
}
Copy
Setting custom attributes on inbound SIP participants
LiveKit participants have an
attributes
field that stores key-value pairs. You can add custom attributes
for SIP participants in the dispatch rule. These attributes are inherited by all SIP participants created
by the dispatch rule.
To learn more, see
SIP participant attributes
.
The following examples add two attributes to SIP participants created by this dispatch rule:
LiveKit CLI
Node.js
Python
Ruby
Go
{
"attributes"
:
{
"<key_name1>"
:
"<value1>"
,
"<key_name2>"
:
"<value2>"
}
,
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call-"
}
}
}
Copy
Setting custom metadata on inbound SIP participants
LiveKit participants have a
metadata
field that can store arbitrary data for your application (typically JSON).
It can also be set on SIP participants created by a dispatch rule.
Specifically,
metadata
set on a dispatch rule will be inherited by all SIP participants created by it.
The following examples add the metadata,
{"is_internal": true}
, to all SIP participants created from an inbound
call by this dispatch rule:
LiveKit CLI
Node.js
Python
Ruby
Go
{
"metadata"
:
"{\"is_internal\": true}"
,
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call-"
}
}
}
Copy
List dispatch rules
Use the
ListSIPDispatchRule
API to list all dispatch rules.
LiveKit CLI
Node.js
Python
Ruby
Go
lk sip dispatch list
Copy
On this page
Caller dispatch rule (individual)
Direct dispatch rule
Pin-protected room
Callee dispatch rule
Setting custom attributes on inbound SIP participants
Setting custom metadata on inbound SIP participants
List dispatch rules
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/accepting-calls-twilio-voice:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Inbound calls with Twilio programmable voice
Step 1. Purchase a phone number from Twilio
Step 2. Set up a TwiML Bin
Step 3. Direct phone number to the TwiML Bin
Step 4. Create a LiveKit inbound trunk
Step 5. Create a dispatch rule to place each caller into their own room.
Testing with an agent
Connecting to a Twilio phone conference
Step 1. Set Twilio environment variables
Step 2. Bridge a Twilio conference and LiveKit SIP
Step 3.  Execute the file
Inbound calls with Twilio programmable voice
Accept inbound calls using Twilio programmable voice. All you need is an inbound trunk and a dispatch rule
created using the LiveKit CLI (or SDK) to accept calls and route callers to LiveKit rooms. The following
steps guide you through the process.
Note
This method doesn't support
SIP REFER
. To set up Elastic SIP Trunking, see the
Configuring Twilio SIP trunks
quickstart.
Step 1. Purchase a phone number from Twilio
If you don't already have a phone number, see
How to Search for and Buy a Twilio Phone Number From Console
.
Step 2. Set up a TwiML Bin
TwiML Bins are a simple way to test TwiML responses. Use a TwiML Bin to redirect an inbound call to LiveKit.
To create a TwiML Bin, follow these steps:
Navigate to your
TwiML Bins
page.
Create a TwiML Bin and add the following contents:
<?xml version="1.0" encoding="UTF-8"?>
<
Response
>
<
Dial
>
<
Sip
username
=
"
<sip_trunk_username>
"
password
=
"
<sip_trunk_password>
"
>
sip:
<
your_phone_number
>
@
<
your
SIP
host
>
</
Sip
>
</
Dial
>
</
Response
>
Copy
Step 3. Direct phone number to the TwiML Bin
Configure incoming calls to a specific phone number to use the TwiML Bin you just created:
Navigate to the
Manage numbers
page
and select the purchased phone number.
In the
Voice Configuration
section, edit the
A call comes in
fields. After you select
TwiML Bin
.
select the TwiML Bin created in the previous step.
Step 4. Create a LiveKit inbound trunk
Use the LiveKit CLI to create an
inbound trunk
for the purchased phone number.
Create an
inbound-trunk.json
file with the following contents. Replace the phone number
and add a
username
and
password
of your choosing:
{
"trunk"
:
{
"name"
:
"My inbound trunk"
,
"auth_username"
:
"<sip_trunk_username>"
,
"auth_password"
:
"<sip_trunk_password>"
}
}
Copy
Note
Be sure to use the same username and password that's specified in the TwiML Bin.
Use the CLI to create an inbound trunk:
lk sip inbound create inbound-trunk.json
Copy
Step 5. Create a dispatch rule to place each caller into their own room.
Use the LiveKit CLI to create a
dispatch rule
that places each caller into
individual rooms named with the prefix
call
.
Create a
dispatch-rule.json
file with the following contents:
{
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call"
}
}
}
Copy
Testing with an agent
Follow the
Voice AI quickstart
to create an agent that responds to incoming calls. Then call the phone number and your agent should pick up the call.
Connecting to a Twilio phone conference
You can bridge Twilio conferencing to LiveKit via SIP, allowing you to add agents and other LiveKit
clients to an existing Twilio conference. This requires the following setup:
Twilio conferencing
.
LiveKit
inbound trunk
.
LiveKit
voice AI agent
.
The example in this section uses
Node
and the
Twilio Node SDK
.
Step 1. Set Twilio environment variables
You can find these values in your
Twilio Console
:
export
TWILIO_ACCOUNT_SID
=
<
twilio_account_sid
>
export
TWILIO_AUTH_TOKEN
=
<
twilio_auth_token
>
Copy
Step 2. Bridge a Twilio conference and LiveKit SIP
Create a
bridge.js
file and update the
twilioPhoneNumber
,
conferenceSid
,
sipHost
,
and
from
field for the API call in the following code:
Note
If you're signed in to
LiveKit Cloud
, your sip host is filled in
below.
import
twilio
from
'twilio'
;
const
accountSid
=
process
.
env
.
TWILIO_ACCOUNT_SID
;
const
authToken
=
process
.
env
.
TWILIO_AUTH_TOKEN
;
const
twilioClient
=
twilio
(
accountSid
,
authToken
)
;
/**
* Phone number bought from Twilio that is associated with a LiveKit trunk.
* For example, +14155550100.
* See https://docs.livekit.io/sip/quickstarts/configuring-twilio-trunk/
*/
const
twilioPhoneNumber
=
'<sip_trunk_phone_number>'
;
/**
* SIP host is available in your LiveKit Cloud project settings.
* This is your project domain without the leading "sip:".
*/
const
sipHost
=
'<your SIP host>'
;
/**
* The conference SID from Twilio that you want to add the agent to. You
* likely want to obtain this from your conference status callback webhook handler.
* The from field must contain the phone number, client identifier, or username
* portion of the SIP address that made this call.
* See https://www.twilio.com/docs/voice/api/conference-participant-resource#request-body-parameters
*/
const
conferenceSid
=
'<twilio_conference_sid>'
;
await
twilioClient
.
conferences
(
conferenceSid
)
.
participants
.
create
(
{
from
:
'<valid_from_value>'
,
to
:
`
sip:
${
twilioPhoneNumber
}
@
${
sipHost
}
;transport=tcp
`
,
}
)
;
Copy
Step 3.  Execute the file
When you run the file, it bridges the Twilio conference to a new LiveKit session using the previously
configured dispatch rule. This allows you to automatically
dispatch an agent
to the Twilio conference.
node
bridge.js
Copy
On this page
Inbound calls with Twilio programmable voice
Step 1. Purchase a phone number from Twilio
Step 2. Set up a TwiML Bin
Step 3. Direct phone number to the TwiML Bin
Step 4. Create a LiveKit inbound trunk
Step 5. Create a dispatch rule to place each caller into their own room.
Testing with an agent
Connecting to a Twilio phone conference
Step 1. Set Twilio environment variables
Step 2. Bridge a Twilio conference and LiveKit SIP
Step 3.  Execute the file
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/making-calls:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Outbound call workflow
Setup for making calls
SIP trunking provider setup
LiveKit SIP configuration
Make an outbound call
Next steps
Outbound call workflow
To make an outbound call, you create a
SIP participant
with the user's phone
number. When you execute the
CreateSIPParticipant
request, LiveKit SIP sends an INVITE request to
your SIP provider. If the SIP provider accepts the call, the SIP participant is added to the
LiveKit room.
Call the
CreateSIPParticipant
API to create a SIP participant.
LiveKit SIP sends an INVITE request to the SIP trunking provider.
SIP trunking provider validates trunk credentials and accepts the call.
LiveKit server places SIP participant in the LiveKit room specified in the
CreateSIPParticipant
request.
Setup for making calls
The following sections outline the steps required to make an outbound SIP call.
SIP trunking provider setup
Purchase a phone number from a SIP Provider.
For a list of tested providers, see the table in
Using LiveKit SIP
.
Configure the SIP Trunk on the provider to send SIP traffic to accept SIP traffic from the
LiveKit SIP service.
For instructions for setting up a SIP trunk,
see
Configuring a SIP provider trunk
.
LiveKit SIP configuration
Create an
outbound trunk
associated with your SIP provider phone number.
This is the number that is used to dial out to the user. Include the authentication credentials
required by your SIP trunking provider to make calls.
Make an outbound call
Create a SIP participant. When the
CreateSIPParticipant
request is executed, a SIP call is initiated:
An INVITE request is sent to the SIP trunk provider. The provider checks authentication credentials
and returns a response to LiveKit.
If the call is accepted, LiveKit dials the user and creates a SIP participant in the LiveKit room.
If the call is not accepted by the SIP trunk provider, the
CreateSIPParticipant
request fails.
After the call starts ringing, you can check the call status by listening to
participant events
:
If the
sip.callStatus
participant attribute is updated to
active
, the call has connected.
If the call fails, the participant is disconnected and leaves the room.
Next steps
See the following guide to create an AI agent that makes outbound calls.
Voice AI telephony guide
Create an AI agent to make outbound calls.
On this page
Outbound call workflow
Setup for making calls
SIP trunking provider setup
LiveKit SIP configuration
Make an outbound call
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/trunk-outbound:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
SIP outbound trunk with username and password authentication
IP address range for LiveKit Cloud SIP
To provision an outbound trunk with the SIP Service, use the
CreateSIPOutboundTrunk
API. It returns an
SIPOutboundTrunkInfo
object that describes the created SIP trunk. These parameters can also be queried at any time using the
ListSIPOutboundTrunk
API.
SIP outbound trunk with username and password authentication
The following creates a SIP outbound trunk with username and password authentication. It will make outbound calls from number
+15105550100
.
LiveKit CLI
Node.js
Python
Ruby
Go
Create a file named
outbound-trunk.json
using your phone number, trunk domain name, and
username
and
password
:
Twilio
Telnyx
{
"trunk"
:
{
"name"
:
"My outbound trunk"
,
"address"
:
"<my-trunk>.pstn.twilio.com"
,
"numbers"
:
[
"+15105550100"
]
,
"auth_username"
:
"<username>"
,
"auth_password"
:
"<password>"
}
}
Copy
Create the outbound trunk using the CLI:
lk sip outbound create outbound-trunk.json
Copy
The output of the command returns the trunk ID. Copy it for the next step:
SIPTrunkID: <your-trunk-id>
Copy
IP address range for LiveKit Cloud SIP
LiveKit Cloud nodes do not have a static IP address range, thus there's no way currently to use IP range for outbound authentication.
Thus, prefer setting user/password authentication on SIP trunk Provider.
If it's unavailable, or IP range is required in addition to user/password, set range(s) that include all IPs: e.g.
0.0.0.0/0
or
0.0.0.0/1
+
128.0.0.0/1
.
On this page
SIP outbound trunk with username and password authentication
IP address range for LiveKit Cloud SIP
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/outbound-calls:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Creating a SIP participant
Making a call with extension codes (DTMF)
Playing dial tone while the call is dialing
The following sections include examples for making an outbound call by creating a LiveKit SIP
participant and configuring call settings for dialing out. To create an AI agent to make outbound
calls on your behalf, see the
Voice AI telephony guide
.
Creating a SIP participant
To make outbound calls with SIP Service, create a SIP participant with the
CreateSIPParticipant
API.
It returns an
SIPParticipantInfo
object that describes the participant.
Outbound calling requires at least one
Outbound Trunk
.
LiveKit CLI
Node.js
Python
Ruby
Go
Create a
sip-participant.json
file with the following participant details:
{
"sip_trunk_id"
:
"<your-trunk-id>"
,
"sip_call_to"
:
"<phone-number-to-dial>"
,
"room_name"
:
"my-sip-room"
,
"participant_identity"
:
"sip-test"
,
"participant_name"
:
"Test Caller"
,
"krisp_enabled"
:
true
}
Copy
Create the SIP Participant using the CLI. After you run this command, the participant makes a
call to the
sip_call_to
number configured in your outbound trunk. You can monitor the call status
using the
SIP participant attributes
. When the call
is picked up by the callee, the
sip.callStatus
attribute is
active
.
lk sip participant create sip-participant.json
Copy
Once the user picks up, they will be connected to
my-sip-room
.
Making a call with extension codes (DTMF)
To make outbound calls with fixed extension codes (DTMF tones), set
dtmf
field in
CreateSIPParticipant
request:
LiveKit CLI
Node.js
Python
Ruby
Go
{
"sip_trunk_id"
:
"<your-trunk-id>"
,
"sip_call_to"
:
"<phone-number-to-dial>"
,
"dtmf"
:
"*123#ww456"
,
"room_name"
:
"my-sip-room"
,
"participant_identity"
:
"sip-test"
,
"participant_name"
:
"Test Caller"
}
Copy
Tip
Character
w
can be used to delay DTMF by 0.5 sec.
This example will dial a specified number and will send the following DTMF tones:
*123#
Wait 1 sec
456
Playing dial tone while the call is dialing
SIP participants emit no audio by default while the call connects.
This can be changed by setting
play_dialtone
field in
CreateSIPParticipant
request:
LiveKit CLI
Node.js
Python
Ruby
Go
{
"sip_trunk_id"
:
"<your-trunk-id>"
,
"sip_call_to"
:
"<phone-number-to-dial>"
,
"room_name"
:
"my-sip-room"
,
"participant_identity"
:
"sip-test"
,
"participant_name"
:
"Test Caller"
,
"play_dialtone"
:
true
}
Copy
If
play_dialtone
is enabled, the SIP Participant plays a dial tone to the room until the phone is picked up.
On this page
Creating a SIP participant
Making a call with extension codes (DTMF)
Playing dial tone while the call is dialing
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/dtmf:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Sending DTMF
Receiving DTMF
LiveKit's Telephony stack fully supports DTMF tones, enabling integration
with legacy IVR systems. It also enables agents to receive DTMF tones from telephone users.
Sending DTMF
To send DTMF tones, use the
publishDtmf
API on the
localParticipant
.
This API transmits DTMF tones to the room; tones can be sent by any participant in the room.
SIP participants in the room receive the tones and relay them to the telephone user.
Node.js
Python
Go
// publishes 123# in DTMF
await
localParticipant
.
publishDtmf
(
1
,
'1'
)
;
await
localParticipant
.
publishDtmf
(
2
,
'2'
)
;
await
localParticipant
.
publishDtmf
(
3
,
'3'
)
;
await
localParticipant
.
publishDtmf
(
11
,
'#'
)
;
Copy
Tip
Sending DTMF tones requires both a numeric code and a string representation to
ensure compatibility with various SIP implementations.
Special characters like
*
and
#
are mapped to their respective numeric codes.
See
RFC 4733
for details.
Receiving DTMF
When SIP receives DTMF tones, they are relayed to the room as events that participants can listen for.
Node.js
Python
Go
room
.
on
(
RoomEvent
.
DtmfReceived
,
(
code
,
digit
,
participant
)
=>
{
console
.
log
(
'DTMF received from participant'
,
participant
.
identity
,
code
,
digit
)
;
}
)
;
Copy
On this page
Sending DTMF
Receiving DTMF
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/transfer-cold:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Transferring a SIP participant using SIP REFER
Enable call transfers for your Twilio SIP trunk
TransferSIPParticipant server API parameters
Usage
A "cold transfer" refers to transferring a caller (SIP participant) to another number or SIP endpoint without a hand off. A cold
transfer shuts down the room (that is, the session) of the original call.
Transferring a SIP participant using SIP REFER
REFER is a SIP method that allows you to move an active session to another endpoint (that is, transfer a call). For LiveKit
telephony apps, you can use the
TransferSIPParticipant
server API to transfer a caller to another phone number or SIP endpoint.
In order to successfully transfer calls, you must configure your provider trunks to allow call transfers.
Enable call transfers for your Twilio SIP trunk
Enable call transfer and PSTN transfers for your Twilio SIP trunk. To learn more, see Twilio's
Call Transfer via SIP REFER
documentation.
When you transfer a call, you have the option to set the caller ID to display the phone number of the transferee (the caller)
or the transferor (the phone number associated with your LiveKit trunk).
CLI
Console
The following command enables call transfers and sets the caller ID to display the number of the transferee:
Note
To list trunks, execute
twilio api trunking v1 trunks list
.
To set the caller ID to the transferor, set
transfer-caller-id
to
from-transferor
.
twilio api trunking v1 trunks update
--sid
<
twilio-trunk-sid
>
\
--transfer-mode enable-all
\
--transfer-caller-id from-transferee
Copy
TransferSIPParticipant server API parameters
transfer_to
string
Required
#
The
transfer_to
value can either be a valid telephone number or a SIP URI.
The following examples are valid values:
tel:+15105550100
sip:+15105550100@sip.telnyx.com
sip:+15105550100@my-livekit-demo.pstn.twilio.com
participant_identity
string
Required
#
Identity of the SIP participant that should be transferred.
room_name
string
Required
#
Source room name for the transfer.
play_dialtone
bool
Required
#
Play dial tone to the user being transferred when a transfer is initiated.
Usage
Set up the following environment variables:
export
LIVEKIT_URL
=
<
your LiveKit server URL
>
export
LIVEKIT_API_KEY
=
<
your API Key
>
export
LIVEKIT_API_SECRET
=
<
your API Secret
>
Reveal API Key and Secret
Copy
Node.js
Python
Ruby
Go
This example uses the LiveKit URL, API key, and secret set as environment variables.
import
{
SipClient
}
from
'livekit-server-sdk'
;
// ...
async
function
transferParticipant
(
participant
)
{
console
.
log
(
"transfer participant initiated"
)
;
const
sipTransferOptions
=
{
playDialtone
:
false
}
;
const
sipClient
=
new
SipClient
(
process
.
env
.
LIVEKIT_URL
,
process
.
env
.
LIVEKIT_API_KEY
,
process
.
env
.
LIVEKIT_API_SECRET
)
;
const
transferTo
=
"tel:+15105550100"
;
await
sipClient
.
transferSipParticipant
(
'open-room'
,
participant
.
identity
,
transferTo
,
sipTransferOptions
)
;
console
.
log
(
'transfer participant'
)
;
}
Copy
On this page
Transferring a SIP participant using SIP REFER
Enable call transfers for your Twilio SIP trunk
TransferSIPParticipant server API parameters
Usage
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/hd-voice:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Configuring Telnyx
Other Providers
Telephone calls have traditionally been routed through the Public Switched Telephone Network (PSTN), a technology for landlines dating back over a century. PSTN calls are limited to an 8kHz sample rate using a narrowband audio codec, resulting in audio that typically sounds muffled or lacks range.
Modern cell phones can use VoIP for calls when connected via Wi-Fi or mobile data. VoIP can leverage wideband audio codecs that transmit audio at a higher sample rate, resulting in much higher quality audio, often referred to as HD Voice.
LiveKit SIP supports wideband audio codecs such as G.722 out of the box, providing higher quality audio when used with HD Voice-capable SIP trunks or endpoints.
Configuring Telnyx
Telnyx supports HD Voice for customers in the US. To enable HD Voice with Telnyx, ensure the following are configured in your Telnyx portal:
HD Voice feature
is enabled on the phone number you are trying to use (under Number -> Voice)
G.722
codec is enabled on your SIP Trunk (under SIP Connection -> Inbound)
We recommend leaving G.711U enabled for compatibility.
Other Providers
Currently, Twilio does not support HD Voice. If you find other providers that support HD Voice, please let us know so we can update this guide.
On this page
Configuring Telnyx
Other Providers
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/sip-participant:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
SIP participant attributes
SIP attributes
Twilio attributes
Custom attributes
Examples
Basic example
Modify voice AI agent based on caller attributes
Creating a SIP participant to make outbound calls
Note
To create a SIP participant to make outbound calls, see
Make outbound calls
.
Each user in a LiveKit telephony app is a
LiveKit participant
.
This includes end users who call in using your inbound trunk, the participant you use to make outbound calls,
and if you're using an agent, the AI voice agent that interacts with callers.
SIP participants are managed like any other participant using the
participant management commands
.
SIP participant attributes
SIP participants can be identified using the
kind
field for participants, which identifies the
type of participant
in a LiveKit room (i.e. session).
For SIP participants, this is
Participant.Kind == SIP
.
The participant
attributes
field contains SIP specific attributes that identify the caller and call details. You can use SIP participant attributes to create different workflows based on the caller.
For example, look up customer information in a database to identify the caller.
SIP attributes
All SIP participants have the following attributes:
Attribute
Description
sip.callID
LiveKit's SIP call ID. A unique ID used as a SIP call tag to identify a conversation
(i.e. match requests and responses).
sip.callStatus
Current call status for the SIP call associated with this participant. Valid values are:
active
: Participant is connected and the call is active.
automation
: For outbound calls using Dual-Tone Multi-Frequency (DTMF), this status indicates the call
has successfully connected, but is still dialing DTMF numbers. After all the numbers are dialed,
the status changes to
active
.
dialing
: Call is dialing and waiting to be picked up.
hangup
: Call has been ended by a participant.
ringing
: Inbound call is ringing for the caller. Status changes to
active
when the SIP participant subscribes to any remote audio tracks.
sip.phoneNumber
User's phone number. For inbound trunks, this is the phone number the call originates from.
For outbound SIP, this is the number dialed by the SIP participant.
Note
This attribute isn't available if
HidePhoneNumber
is set in the dispatch rule.
sip.ruleID
SIP
DispatchRule
ID used for the inbound call. This field is empty for outbound calls.
sip.trunkID
The inbound or outbound SIP trunk ID used for the call.
sip.trunkPhoneNumber
Phone number associated with SIP trunk. For inbound trunks, this is the number
dialed in to by an end user. For outbound trunks, this is the number a call originates from.
Twilio attributes
If you're using Twilio SIP trunks, the following additional attributes are included:
Attribute
Description
sip.twilio.accountSid
Twilio account SID.
sip.twilio.callSid
Twilio call SID.
Custom attributes
You can add custom SIP participant attributes in one of two ways:
Adding attributes to the dispatch rule. To learn more, see
Setting custom attributes on inbound SIP participants
.
Using SIP headers: For any
X-*
SIP headers, you can configure your trunk with
headers_to_attributes
and a key/value pair mapping.
For example:
Telnyx
Twilio
{
"trunk"
:
{
"name"
:
"Demo inbound trunk"
,
"numbers"
:
[
"+15105550100"
]
,
"headers_to_attributes"
:
{
"X-<custom_key_value>"
:
"<custom_attribute_name>"
,
}
}
}
Copy
Caution
Note the leading
+
assumes the
Destination Number Format
is set to
+E.164
for your Telnyx number.
Examples
The following examples use SIP participant attributes.
Basic example
Node.js
Python
This example logs the Twilio call SID if the user is a SIP participant.
if
(
participant
.
kind
==
ParticipantKind
.
SIP
)
{
console
.
log
(
participant
.
attributes
[
'sip.twilio.callSid'
]
)
;
}
;
Copy
Modify voice AI agent based on caller attributes
Follow the
Voice AI quickstart
to create an agent that responds to incoming calls. Then modify the agent to use SIP participant attributes.
Python
Node.js
Before starting your
AgentSession
, select the best Deepgram STT model for the participant. Add this code
to your
entrypoint
function:
# Add this import to the top of your file
from
livekit
import
rtc
# add this after `await ctx.connect()`:
participant
=
await
ctx
.
wait_for_participant
(
)
dg_model
=
"nova-2-general"
# Check if the participant is a SIP participant
if
participant
.
kind
==
rtc
.
ParticipantKind
.
PARTICIPANT_KIND_SIP
:
# Use a Deepgram model better suited for phone calls
dg_model
=
"nova-2-phonecall"
if
participant
.
attributes
[
'sip.phoneNumber'
]
==
'+15105550100'
:
logger
.
info
(
"Caller phone number is +1-510-555-0100"
)
# Add other logic here to modify the agent based on the caller's phone number
session
=
AgentSession
(
stt
=
deepgram
.
STT
(
model
=
dg_model
)
,
# ... llm, vad, tts, etc.
)
# ... rest of your entrypoint, including `await session.start(...)`
Copy
Creating a SIP participant to make outbound calls
To make outbound calls, create a SIP participant. To learn more, see
Make outbound calls
.
On this page
SIP participant attributes
SIP attributes
Twilio attributes
Custom attributes
Examples
Basic example
Modify voice AI agent based on caller attributes
Creating a SIP participant to make outbound calls
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/api:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Using endpoints
SIPService APIs
CreateSIPInboundTrunk
CreateSIPOutboundTrunk
CreateSIPDispatchRule
CreateSIPParticipant
DeleteSIPDispatchRule
DeleteSIPTrunk
GetSIPInboundTrunk
GetSIPOutboundTrunk
ListSIPDispatchRule
ListSIPInboundTrunk
ListSIPOutboundTrunk
TransferSIPParticipant
Types
GetSIPInboundTrunkResponse
GetSIPOutboundTrunkResponse
SIPDispatchRule
SIPHeaderOptions
SIPDispatchRuleInfo
SIPInboundTrunkInfo
SIPOutboundTrunkInfo
SIPParticipantInfo
SIPMediaEncryption
SIPTransport
SIPTrunkInfo
TrunkKind
Overview
LiveKit has built-in APIs that let you to manage SIP trunks, dispatch rules, and SIP participants. The SIP API is
available with LiveKit server SDKs and CLI:
Go SIP client
JS SIP client
Ruby SIP client
Python SIP client
Java SIP client
CLI
Important
Requests to the SIP API require the SIP
admin
permission unless otherwise noted. To create a token with the appropriate
grant, see
SIP grant
.
To learn more about additional APIs, see
Server APIs
.
Using endpoints
The SIP API is accessible via
/twirp/livekit.SIP/<MethodName>
. For example, if you're using LiveKit Cloud the
following URL is for the
CreateSIPInboundTrunk
API endpoint:
https://
<
your LiveKit URL domain
>
/twirp/livekit.SIP/CreateSIPInboundTrunk
Copy
Authorization header
All endpoints require a signed access token. This token should be set via HTTP header:
Authorization
:
Bearer
<
token
>
Copy
LiveKit server SDKs automatically include the above header.
Post body
Twirp expects an HTTP POST request. The body of the request must be
a JSON object (
application/json
) containing parameters specific to that request. Use an empty
{}
body for requests that don't require parameters.
Examples
For example, create an inbound trunk using
CreateSIPInboundTrunk
:
curl
-X
POST https://
<
your LiveKit URL domain
>
/twirp/livekit.SIP/CreateSIPInboundTrunk
\
-H
"Authorization: Bearer <token-with-sip-admin>"
\
-H
'Content-Type: application/json'
\
-d
'{ "name": "My trunk", "numbers": ["+15105550100"] }'
Copy
List inbound trunks using
ListSIPInboundTrunk
API endpoint to list inbound trunks:
curl
-X
POST https://
<
your LiveKit URL domain
>
/twirp/livekit.SIP/ListSIPInboundTrunk
\
-H
"Authorization: Bearer <token-with-sip-admin>"
\
-H
'Content-Type: application/json'
\
-d
'{}'
Copy
SIPService APIs
The SIPService APIs allow you to manage trunks, dispatch rules, and SIP participants.
Tip
All RPC definitions and options can be found
here
.
CreateSIPInboundTrunk
Create an inbound trunk with the specified settings.
Returns
SIPInboundTrunkInfo
.
Parameter
Type
Required
Description
name
string
yes
name of the trunk.
metadata
string
Initial metadata to assign to the trunk. This metadata is added to every SIP participant that uses the trunk.
numbers
array<string>
yes
Array of provider phone numbers associated with the trunk.
allowed_addresses
array<string>
List of IP addresses that are allowed to use the trunk. Each item in the list can be an individual
IP address or a Classless Inter-Domain Routing notation representing a CIDR block.
allowed_numbers
array<string>
List of phone numbers that are allowed to use the trunk.
auth_username
string
If configured, the username for authorized use of the provider's SIP trunk.
auth_password
string
If configured, the password for authorized use of the provider's SIP trunk.
headers
map<string, string>
SIP X-* headers for INVITE request. These headers are sent as-is and may help identify
this call as coming from LiveKit for the other SIP endpoint.
headers_to_attributes
map<string, string>
Key-value mapping of SIP X-* header names to participant attribute names.
attributes_to_headers
map<string, string>
Map SIP headers from INVITE request to
sip.h.*
participant attributes. If the names of the required headers is known,
use
headers_to_attributes
instead.
include_headers
SIPHeaderOptions
Specify how SIP headers should be mapped to attributes.
ringing_timeout
google.protobuf.Duration
Maximum time for the call to ring.
max_call_duration
google.protobuf.Duration
Maximum call duration.
krisp_enabled
bool
True to enable
Krisp noise cancellation
for the caller.
media_encryption
SIPMediaEncryption
Whether or not to encrypt media.
CreateSIPOutboundTrunk
Create an outbound trunk with the specified settings.
Returns
SIPOutboundTrunkInfo
.
Parameter
Type
Required
Description
name
string
yes
name of the trunk.
metadata
string
User-defined metadata for the trunk. This metadata is added to every SIP participant that uses the trunk.
address
string
yes
Hostname or IP the SIP INVITE is sent to. This is
not
a SIP URI and shouldn't contain the
sip:
protocol.
numbers
array<string>
yes
List of provider phone numbers associated with the trunk.
transport
SIPTransport
Protocol to use for SIP transport: auto, TCP, or UDP.
numbers
array<string>
List of phone numbers that are allowed to use the trunk.
auth_username
string
If configured, the username for authorized use of the provider's SIP trunk.
auth_password
string
If configured, the password for authorized use of the provider's SIP trunk.
headers
map<string, string>
SIP X-* headers for INVITE request. These headers are sent as-is and may help identify
this call as coming from LiveKit for the other SIP endpoint.
headers_to_attributes
map<string, string>
Key-value mapping of SIP X-* header names to participant attribute names.
media_encryption
SIPMediaEncryption
Whether or not to encrypt media.
CreateSIPDispatchRule
Create dispatch rule.
Returns
SIPDispatchRuleInfo
.
Parameter
Type
Required
Description
rule
SIPDispatchRule
yes
Type of dispatch rule.
trunk_ids
array<string>
List of associated trunk IDs. If empty, all trunks match this dispatch rule.
hide_phone_number
bool
If true, use a random value for participant identity and phone number ommitted from attributes.
By default, the participant identity is created using the phone number (if the participant identity isn't explicitly set).
inbound_numbers
array<string>
If set, the dispatch rule only accepts calls made to numbers in the list.
name
string
yes
Human-readable name for the dispatch rule.
metadata
string
Optional metadata for the dispatch rule. If defined, participants created by the rule inherit this metadata.
attributes
map<string, string>
Key-value mapping of user-defined attributes. Participants created by this rule inherit these attributes.
room_preset
string
Only for LiveKit Cloud: Config preset to use.
room_config
RoomConfiguration
Room configuration to use if the participant initiates the room.
CreateSIPParticipant
Note
Requires SIP
call
grant on authorization token.
Create a SIP participant to make outgoing calls.
Returns
SIPParticipantInfo
Parameter
Type
Required
Description
sip_trunk_id
string
yes
ID for SIP trunk used to dial user.
sip_call_to
string
yes
Phone number to call.
sip_number
string
SIP number to call from. If empty, use trunk number.
room_name
string
yes
Name of the room to connect the participant to.
participant_identity
string
Identity of the participant.
participant_name
string
Name of the participant.
participant_metadata
string
User-defined metadata that is attached to created participant.
participant_attributes
map<string, string>
Key-value mapping of user-defined attributes to attach to created participant.
dtmf
string
DTMF digits (extension codes) to use when making the call. Use character
w
to add a 0.5 second delay.
play_dialtone
bool
Optionally play dial tone in the room in the room as an audible indicator for existing participants.
hide_phone_number
bool
If true, use a random value for participant identity and phone number ommitted from attributes.
By default, the participant identity is created using the phone number (if the participant identity isn't explicitly set).
headers
map<string, string>
SIP X-* headers for INVITE request. These headers are sent as-is and may help identify
this call as coming from LiveKit.
include_headers
SIPHeaderOptions
Specify how SIP headers should be mapped to attributes.
ringing_timeout
google.protobuf.Duration
Maximum time for the callee to answer the call.
max_call_duration
google.protobuf.Duration
Maximum call duration.
krisp_enabled
bool
True to enable
Krisp noise cancellation
for the callee.
media_encryption
SIPMediaEncryption
Whether or not to encrypt media.
DeleteSIPDispatchRule
Delete a dispatch rule.
Returns
SIPDispatchRuleInfo
.
Parameter
Type
Required
Description
sip_dispatch_rule_id
string
ID of dispatch rule.
DeleteSIPTrunk
Delete a trunk.
Returns
SIPTrunkInfo
.
Parameter
Type
Required
Description
sip_trunk_id
string
yes
ID of trunk.
GetSIPInboundTrunk
Get inbound trunk.
Returns
GetSIPInboundTrunkResponse
.
Parameter
Type
Required
Description
sip_trunk_id
string
yes
ID of trunk.
GetSIPOutboundTrunk
Get outbound trunk.
Returns
GetSIPOutboundTrunkResponse
.
Parameter
Type
Required
Description
sip_trunk_id
string
yes
ID of trunk.
ListSIPDispatchRule
List dispatch rules.
Returns array<
SIPDispatchRuleInfo
>.
ListSIPInboundTrunk
List inbound trunks.
Returns array<
SIPInboundTrunkInfo
>.
ListSIPOutboundTrunk
List outbound trunks.
Returns array<
SIPOutboundTrunkInfo
>.
TransferSIPParticipant
Note
Requires SIP
call
grant on authorization token.
Transfer call to another number or SIP endpoint.
Returns
google.protobuf.Empty
.
Parameter
Type
Required
Description
participant_identity
string
yes
Identity of the participant to transfer.
room_name
string
yes
Name of the room the participant is currently in.
transfer_to
string
yes
Phone number or SIP endpoint to transfer participant to.
play_dialtone
bool
Optionally play dial tone during the transfer. By default, the room audio is played during the transfer.
Types
The SIP service includes the following types.
GetSIPInboundTrunkResponse
Field
Type
Description
trunk
SIPInboundTrunkInfo
Inbound trunk.
GetSIPOutboundTrunkResponse
Field
Type
Description
trunk
SIPOutboundTrunkInfo
Outbound trunk.
SIPDispatchRule
Valid values include:
Name
Type
Value
Description
dispatch_rule_direct
SIPDispatchRuleDirect
1
Dispatches callers into an existing room. You can optionally require a pin before caller enters the room.
dispatch_rule_individual
SIPDispatchRuleIndividual
2
Creates a new room for each caller.
dispatch_rule_callee
SIPDispatchRuleCallee
3
Creates a new room for each callee.
SIPHeaderOptions
Enum. Valid values are as follows:
Name
Value
Description
SIP_NO_HEADERS
0
Don't map any headers except those mapped explicitly.
SIP_X_HEADERS
1
Map all
X-*
headers to
sip.h.*
attributes.
SIP_ALL_HEADERS
2
Map all headers to
sip.h.*
attributes.
SIPDispatchRuleInfo
Field
Type
Description
sip_dispatch_rule_id
string
Dispatch rule ID.
rule
SIPDispatchRule
Type of dispatch rule.
trunk_ids
array<string>
List of associated trunk IDs.
hide_phone_number
bool
If true, hides phone number.
inbound_numbers
array<string>
If this list is included, the dispatch rule only accepts calls made to the numbers in the list.
name
string
Human-readable name for the dispatch rule.
metadata
string
User-defined metadata for the dispatch rule. Participants created by this rule inherit this metadata.
headers
map<string, string>
Custom SIP X-* headers to included in the 200 OK response.
attributes
map<string, string>
Key-value mapping of user-defined attributes. Participants created by this rule inherit these attributes.
room_preset
string
Only for LiveKit Cloud: Config preset to use.
room_config
RoomConfiguration
Room configuration object associated with the dispatch rule.
SIPInboundTrunkInfo
Field
Type
Description
sip_trunk_id
string
Trunk ID
name
string
Human-readable name for the trunk.
numbers
array<string>
Phone numbers associated with the trunk. The trunk only accepts calls made to the phone numbers in the list.
allowed_addresses
array<string>
IP addresses or CIDR blocks that are allowed to use the trunk. If this list is populated, the trunk only
accepts traffic from the IP addresses in the list.
allowed_numbers
array<string>
Phone numbers that are allowed to dial in. If this list is populated, the trunk only accepts calls from
the numbers in the list.
auth_username
string
Username used to authenticate inbound SIP invites.
auth_password
string
Password used to authenticate inbound SIP invites.
headers
map<string, string>
Custom SIP X-* headers to included in the 200 OK response.
headers_to_attributes
map<string, string>
Custom SIP X-* headers that map to SIP participant attributes.
ringing_timeout
google.protobuf.Duration
Maximum time for the caller to wait for track subscription (that is, for the call to be picked up).
max_call_duration
google.protobuf.Duration
Maximum call duration.
krisp_enabled
Boolean
True if Krisp noise cancellation is enabled for the call.
SIPOutboundTrunkInfo
Field
Type
Description
sip_trunk_id
string
Trunk ID.
name
string
Trunk name.
metadata
string
User-defined metadata for trunk.
address
string
Hostname or IP address the SIP request message (SIP INVITE) is sent to.
transport
SIPTransport
Protocol to use for SIP transport: auto, TCP, or UDP.
numbers
array<string>
Phone numbers used to make calls. A random number in the list is selected whenever a call is made.
auth_username
string
Username used to authenticate with the SIP server.
auth_password
string
Password used to authenticate with the SIP server.
headers
map<string, string>
Custom SIP X-* headers to included in the 200 OK response.
headers_to_attributes
map<string, string>
Custom SIP X-* headers that map to SIP participant attributes.
SIPParticipantInfo
Field
Type
Description
participant_id
string
Participant ID.
participant_identity
string
Participant name.
room_name
string
Name of the room.
sip_call_id
string
SIP call ID.
SIPMediaEncryption
Enum. Valid values are as follows:
Name
Value
Description
SIP_MEDIA_ENCRYPT_DISABLE
0
Don't turn on encryption.
SIP_MEDIA_ENCRYPT_ALLOW
1
Use encryption if available.
SIP_MEDIA_ENCRYPT_REQUIRE
2
Require encryption.
SIPTransport
Enum. Valid values are as follows:
Name
Value
Description
SIP_TRANSPORT_AUTO
0
Detect automatically.
SIP_TRANSPORT_UDP
1
UDP
SIP_TRANSPORT_TCP
2
TCP
SIP_TRANSPORT_TLS
3
TLS
SIPTrunkInfo
Note
This type is deprecated. See
SIPInboundTrunkInfo
and
SIPOutboundTrunkInfo
.
Field
Type
Description
sip_trunk_id
string
Trunk ID.
kind
TrunkKind
Type of trunk.
inbound_addresses
array<string>
IP addresses or CIDR blocks that are allowed to use the trunk. If this list is populated, the trunk only
accepts traffic from the IP addresses in the list.
outbound_address
string
IP address that the SIP INVITE is sent to.
outbound_number
string
Phone number used to make outbound calls.
transport
SIPTransport
Protocol to use for SIP transport: auto, TCP, or UDP.
inbound_numbers
array<string>
If this list is populated, the trunk only accepts calls to the numbers in this list.
inbound_username
string
Username used to authenticate inbound SIP invites.
inbound_password
string
Password used to authenticate inbound SIP invites.
outbound_username
string
Username used to authenticate outbound SIP invites.
outbound_password
string
Password used to authenticate outbound SIP invites.
name
string
Trunk name.
metadata
string
Initial metadata to assign to the trunk. This metadata is added to every SIP participant that uses the trunk.
TrunkKind
Enum. Valid values are as follows:
Name
Value
Description
TRUNK_LEGACY
0
Legacy trunk.
TRUNK_INBOUND
1
Inbound trunk
.
TRUNK_OUTBOUND
2
Outbound trunk
.
On this page
Overview
Using endpoints
SIPService APIs
CreateSIPInboundTrunk
CreateSIPOutboundTrunk
CreateSIPDispatchRule
CreateSIPParticipant
DeleteSIPDispatchRule
DeleteSIPTrunk
GetSIPInboundTrunk
GetSIPOutboundTrunk
ListSIPDispatchRule
ListSIPInboundTrunk
ListSIPOutboundTrunk
TransferSIPParticipant
Types
GetSIPInboundTrunkResponse
GetSIPOutboundTrunkResponse
SIPDispatchRule
SIPHeaderOptions
SIPDispatchRuleInfo
SIPInboundTrunkInfo
SIPOutboundTrunkInfo
SIPParticipantInfo
SIPMediaEncryption
SIPTransport
SIPTrunkInfo
TrunkKind
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/voice-agent/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Features
Multimodal or voice pipeline
Handling background noise
Turn detection
Voice activity detection (VAD)
Turn detection model
Agent state
Transcriptions
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Building voice agents
.
v1.0 for Node.js is coming soon.
Companies like OpenAI, Character.ai, Retell, and Speak have built their conversational AI products on the LiveKit platform. AI voice agents are one of the primary use cases for LiveKit's Agents framework.
Features
Programmable conversation flows
Integrated LLM function calls
Provide context to the conversation via RAG
Leverage connectors from an open-source plugin ecosystem
Send synchronized transcriptions to your frontend
Multimodal or voice pipeline
LiveKit offers two types of voice agents:
MultimodalAgent
and
VoicePipelineAgent
.
MultimodalAgent
uses OpenAI’s multimodal model and realtime API to directly process user audio and generate audio responses, similar to OpenAI’s advanced voice mode, producing more natural-sounding speech.
VoicePipelineAgent
uses a pipeline of STT, LLM, and TTS models, providing greater control over the conversation flow by allowing applications to modify the text returned by the LLM.
Multimodal
Voice pipeline
Python
✅
✅
Node.JS
✅
✅
Model type
single multimodal
STT, LLM, TTS
Function calling
✅
✅
RAG
via function calling
✅
Natural speech
more natural
Modify LLM response
✅
Model vendors
OpenAI
various
Turn detection
VAD
VAD and turn detection model
Handling background noise
While humans can easily ignore background noise, AI models often struggle, leading to misinterpretations or unnecessary pauses when detecting non-speech sounds. Although WebRTC includes built-in noise suppression, it often falls short in real-world environments.
To address this, LiveKit has partnered with
Krisp
to bring best-in-class noise suppression technology to AI agents.
For instructions on enabling Krisp, see
Krisp integration guide
Turn detection
Endpointing
is the process of detecting the start and end of speech in an audio stream. This is crucial for conversational AI agents to understand when a user has finished speaking and when to start responding to user input.
Determining the end of a turn is particularly challenging for AI agents. Humans rely on multiple cues, such as pauses, speech tone, and content, to recognize when someone has finished speaking.
LiveKit employs two primary strategies to approximate how humans determine turn boundaries:
Voice activity detection (VAD)
LiveKit Agents uses VAD to detect when the user has finished speaking. The agent waits for a minimum duration of silence before considering the turn complete.
Both VoicePipelineAgent and MultimodalAgent use VAD for turn detection.
For OpenAI Multimodal configuration, refer to the
MultimodalAgent turn detection
docs.
VoicePipelineAgent uses Silero VAD to detect end of speech. The
min_endpointing_delay
parameter in the agent constructor specifies the minimum silence duration to consider the end of a turn.
Turn detection model
While VAD provides a simple approximation of turn completion, it lacks contextual awareness. In natural conversations, pauses often occur as people think or formulate responses.
To address this, LiveKit has developed a custom, open-weights language model to incorporate conversational context as an additional signal to VAD.
The
turn-detector
plugin uses this model to predict whether a user is done speaking.
When the model predicts that the user is
not done
with their turn, the agent will wait for a significantly longer period of silence before responding. This helps to prevent unwanted interruptions during natural pauses in speech.
Here's
a demo
of the model in action.
Benchmarks
In our testing, the turn detector model demonstrated the following performance:
85% true positive rate
: avoids early interruptions by correctly identifying when the user is not done speaking.
97% true negative rate
: accurately determines the end of a turn when the user has finished speaking.
Using turn detector
Currently, this model is supported for
VoicePipelineAgent
in Python. To use it, install the
livekit-plugins-turn-detector
package.
Then, initialize the agent with the turn detector:
from
livekit
.
plugins
import
turn_detector
agent
=
VoicePipelineAgent
(
.
.
.
turn_detector
=
turn_detector
.
EOUModel
(
)
,
)
Copy
Before running the agent for the first time, download the model weights:
python my_agent.py download-files
Copy
Agent state
Voice agents automatically publish their current state to your frontend, making it easy to build UI that reflects the agent’s status.
The state is passed to your frontend as a
participant attribute
on the agent participant. Components like
useVoiceAssistant
expose the following states:
disconnected
: either agent or user is disconnected
connecting
: agent is being connected with the user
initializing
: agent is connected, but not yet ready
listening
: agent is listening for user input
thinking
: agent is performing inference on user input
speaking
: agent is playing out a response
Transcriptions
LiveKit provides realtime transcriptions for both the agent and the user, which are sent to your frontend via the
transcription protocol
.
User speech transcriptions are delivered as soon as they are processed by STT. Since the agent’s text response is available before speech synthesis, we manually synchronize the text transcription with audio playback.
On this page
Features
Multimodal or voice pipeline
Handling background noise
Turn detection
Voice activity detection (VAD)
Turn detection model
Agent state
Transcriptions
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/deployment:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Deployment best practices
Networking
Environment variables
Storage
Memory and CPU
Rollout
Load balancing
Worker availability
Autoscaling
Where to deploy
Recommendations
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Deploying to production
.
v1.0 for Node.js is coming soon.
Agent workers are designed to be deployable and scalable by default.
When you start your worker locally,
you are adding it to a pool (of one). LiveKit automatically load balances across
available workers in the pool.
This works the same way in production,
the only difference is the worker is deployed somewhere with more than
one running instance.
Deployment best practices
Networking
Workers communicate with LiveKit and accept incoming jobs via a WebSocket connection to a LiveKit server.
This means that workers are not web servers and do not need to be exposed to the public internet.
No inbound hosts or ports need to be exposed.
Workers can optionally expose a health check endpoint for monitoring purposes. This is not required for normal operation.
The default health check server listens on
http://0.0.0.0:8081/
.
Environment variables
Your production worker will need certain environment variables configured.
A minimal worker requires the LiveKit URL, API key and secret:
LIVEKIT_URL
LIVEKIT_API_KEY
LIVEKIT_API_SECRET
Depending on the plugins your agent uses, you might need additional environment variables:
DEEPGRAM_API_KEY
CARTESIA_API_KEY
OPENAI_API_KEY
etc.
Important
If you use a
LIVEKIT_URL
,
LIVEKIT_API_KEY
, and
LIVEKIT_API_SECRET
from the same project that you use
for local development, your local worker will join the same pool as your production workers.
This means real users could be connected to your local worker.
This is usually not what you want so make sure to use
a different project for local development.
Storage
Workers are stateless and do not require persistent storage. The minimal docker image is about 1GB in size.
For ephemeral storage, 10GB should be more than enough to account for the docker image size and
any temporary files that are created.
Memory and CPU
Different agents will have different memory and CPU requirements. To help guide your scaling decisions, we ran a load test that approximates the load of a
voice-to-voice session on a 4-Core, 8GB machine.
Tip
During the automated load test we also added one human participant interacting with a
voice assistant agent to make sure quality of service was maintained.
This test created 30 agents corresponding to 30 users (so 60 participants in total).
The users published looping speech audio. The agents were subscribed to their corresponding
user's audio and running the Silero voice activity detection plugin against that audio.
The agents also published their own audio which was a simple sine wave.
In short, this test was designed to evaluate a voice assistant use case where the agent is
listening to user speech, running VAD, and publishing audio back to the user.
The results of running the above test on a 4-Core, 8GB machine are:
CPU Usage: ~3.8 cores
Memory usage: ~2.8GB
To be safe and account for spikes, we recommend 4 cores for every 25 voice agents.
Rollout
Workers stop accepting jobs when they receive a SIGINT or SIGTERM. Agents that are still running
on the worker continue to run. It's important that you configure a large enough grace period
for your containers to allow agents to finish.
Voice agents could require a 10+ minute grace period to allow for conversations to finish.
Different deployment platforms have different ways of setting this grace period.
In Kubernetes, it's the
terminationGracePeriodSeconds
field in the pod spec.
Consult your deployment platform's documentation for more information.
Load balancing
Workers don't need an external load balancer.
They rely on a job distribution system embedded within LiveKit servers.
This system is responsible for ensuring that when a job becomes available
(e.g. a new room is created), it is dispatched to only one worker at a time.
If a worker fails to accept the job within a predetermined timeout period,
the job is routed to another available worker.
In the case of LiveKit Cloud, the system prioritizes available workers at the "edge"
or geographically closest to the end-user. Within a region, job distribution
is uniform across workers.
Worker availability
As mentioned in the Load Balancing section, LiveKit will automatically distribute load
across available workers. This means that LiveKit needs a way to know which workers are available.
This "worker availability" is defined by the
load_fnc
and
load_threshold
in the
WorkerOptions
configuration.
The
load_fnc
returns a value between 0 and 1, indicating how busy the worker is while
load_threshold
, a value between 0 and 1, is that load value at which the worker will stop
accepting new jobs.
By default, the
load_fnc
returns the CPU usage of the worker and the
load_threshold
is 0.75.
Autoscaling
Many voice agent use cases have non-uniform load patterns over a day/week/month so it's a good idea
to configure an autoscaler.
An autoscaler should be configured at a
lower
threshold than the worker's
load_threshold
. This allows for existing workers to continue to accept new jobs
while additional workers are still starting up.
Since voice agents are typically long running tasks (relative to typical web requests), rapid
increases in load are more likely to be sustained. In technical terms: spikes are less spikey. For your
autoscaling configuration, you should consider
reducing
cooldown/stabilization periods when scaling
up. When scaling down, consider
increasing
cooldown/stabilization periods because workers take time to
drain.
For example, if deploying on Kubernetes using a Horizontal Pod Autoscaler,
see
stabilizationWindowSeconds
.
Where to deploy
There are many ways to deploy software to a production environment.
We provide some platform-specific
deployment examples
.
All of the examples assume Docker containers are used for deployment.
Recommendations
Render.com
: We (and other builders in the LiveKit community) have found Render.com to
be the easiest way to deploy and autoscale workers. We provide an example
render.yaml
and
instructions in the
deployment examples
repo.
Kubernetes
: If you have a running Kubernetes cluster, it makes sense to deploy your workers there.
We provide an example manifest in the deployment example repo.
On this page
Deployment best practices
Networking
Environment variables
Storage
Memory and CPU
Rollout
Load balancing
Worker availability
Autoscaling
Where to deploy
Recommendations
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/playground:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Using the hosted playground
Self-hosted playground
1. Clone the project
2. Create environment variables
3. Customizing configuration
4. Run the playground
5. Deploying the playground
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Agents playground
.
v1.0 for Node.js is coming soon.
To ease the process of building and testing an agent, we've developed a versatile web frontend called "playground". This app is available for use or customization according to your specific requirements. It can also serve as a starting point for
your application.
Playground source code:
https://github.com/livekit/agents-playground/
Built with Next.js and
LiveKit Components
.
Using the hosted playground
A hosted version of the playground is available at
https://agents-playground.livekit.io
. This version is compatible with any LiveKit server instance (including one running locally). To use it, you'll need to input your LiveKit server's URL and a corresponding access token.
Self-hosted playground
When you run your own version of the playground, it can be configured to automatically generate access tokens for your users. This setup streamlines the user experience, removing the need for manual token entry.
1. Clone the project
git
clone https://github.com/livekit/agents-playground.git
Copy
2. Create environment variables
Create an
.env.local
file in the root of your project folder with the following variables:
LIVEKIT_API_KEY
=
YOUR_API_KEY
LIVEKIT_API_SECRET
=
YOUR_API_SECRET
# Public configuration
NEXT_PUBLIC_LIVEKIT_URL
=
wss://YOUR_LIVEKIT_URL
NEXT_PUBLIC_APP_CONFIG
=
"
title: 'LiveKit Agent Playground'
description: 'LiveKit Agent Playground allows you to test your LiveKit Agent integration by connecting to your LiveKit Cloud or self-hosted instance.'
github_link: 'https://github.com/livekit/agents-playground'
video_fit: 'cover' # 'contain' or 'cover'
settings:
editable: true # Should the user be able to edit settings in-app
theme_color: 'cyan'
chat: true  # Enable or disable chat feature
outputs:
audio: true # Enable or disable audio output
video: true # Enable or disable video output
inputs:
mic: true    # Enable or disable microphone input
camera: true # Enable or disable camera input
sip: true    # Enable or disable SIP input
"
Copy
3. Customizing configuration
NEXT_PUBLIC_APP_CONFIG
is a configurable YAML string designed to tailor the playground's capabilities. By default, the playground is set up to publish both the
user's camera and microphone when they connect to a room. However if, for example, your agent doesn't need vision, you can disable the camera by setting
inputs.camera
to
false
.
Similarly, agents have varying output requirements. If your agent does not provide voice/audio feedback to the user, you can set
outputs.audio
to
false
. This adjustment will consequently remove the related audio component from the playground UI.
4. Run the playground
That's it! You're ready to run your own version of the playground.
npm
install
npm
run dev
Copy
5. Deploying the playground
To deploy your version of the playground, you can use any hosting provider that supports Node.js.
We're hosting the public version of it on
Vercel
.
Deploy with Vercel
On this page
Using the hosted playground
Self-hosted playground
1. Clone the project
2. Create environment variables
3. Customizing configuration
4. Run the playground
5. Deploying the playground
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/start/voice-ai:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Requirements
Python
LiveKit server
AI providers
Setup
Packages
Environment variables
Agent code
Download model files
Speak to your agent
Connect to playground
Next steps
Overview
This guide walks you through the setup of your very first voice assistant using LiveKit Agents for Python. In less than 10 minutes, you'll have a voice assistant that you can speak to in your terminal, browser, telephone, or native app.
Requirements
The following sections describe the minimum requirements to get started with LiveKit Agents.
Python
LiveKit Agents requires Python 3.9 or later.
Looking for Node.js?
The Node.js beta is still in development and has not yet reached v1.0. See the
v0.x documentation
for Node.js reference and join the
LiveKit Community Slack
to be the first to know when the next release is available.
LiveKit server
You need a LiveKit server instance to transport realtime media between user and agent. The easiest way to get started is with a free
LiveKit Cloud
account. Create a project and use the API keys in the following steps. You may also
self-host LiveKit
if you prefer.
AI providers
LiveKit Agents
integrates with most AI model providers
and supports both high-performance STT-LLM-TTS voice pipelines, as well as lifelike multimodal models.
The rest of this guide assumes you use one of the following two starter packs, which provide the best combination of value, features, and ease of setup.
STT-LLM-TTS pipeline
Realtime model
Your agent strings together three specialized providers into a high-performance voice pipeline. You need accounts and API keys for each.
Component
Provider
Required Key
Alternatives
STT
Deepgram
DEEPGRAM_API_KEY
STT integrations
LLM
OpenAI
OPENAI_API_KEY
LLM integrations
TTS
Cartesia
CARTESIA_API_KEY
TTS integrations
Setup
Use the instructions in the following sections to set up your new project.
Packages
Noise cancellation
This example integrates LiveKit Cloud
enhanced background voice/noise cancellation
, powered by Krisp. If you're not using LiveKit Cloud, omit the plugin and the
noise_cancellation
parameter from the following code.
STT-LLM-TTS pipeline
Realtime model
Install the following packages to build a complete voice AI agent with your STT-LLM-TTS pipeline, noise cancellation, and
turn detection
:
pip
install
\
"livekit-agents[deepgram,openai,cartesia,silero,turn-detector]~=1.0"
\
"livekit-plugins-noise-cancellation~=0.2"
\
"python-dotenv"
Copy
Environment variables
Create a file named
.env
and add your LiveKit credentials along with the necessary API keys for your AI providers.
STT-LLM-TTS pipeline
Realtime model
.env
DEEPGRAM_API_KEY
=
<
Your Deepgram API Key
>
OPENAI_API_KEY
=
<
Your OpenAI API Key
>
CARTESIA_API_KEY
=
<
Your Cartesia API Key
>
LIVEKIT_API_KEY
=
<
your API Key
>
LIVEKIT_API_SECRET
=
<
your API Secret
>
LIVEKIT_URL
=
<
your LiveKit server URL
>
Reveal API Key and Secret
Copy
Agent code
Create a file named
main.py
containing the following code for your first voice agent.
STT-LLM-TTS pipeline
Realtime model
main.py
from
dotenv
import
load_dotenv
from
livekit
import
agents
from
livekit
.
agents
import
AgentSession
,
Agent
,
RoomInputOptions
from
livekit
.
plugins
import
(
openai
,
cartesia
,
deepgram
,
noise_cancellation
,
silero
,
)
from
livekit
.
plugins
.
turn_detector
.
multilingual
import
MultilingualModel
load_dotenv
(
)
class
Assistant
(
Agent
)
:
def
__init__
(
self
)
-
>
None
:
super
(
)
.
__init__
(
instructions
=
"You are a helpful voice AI assistant."
)
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
await
ctx
.
connect
(
)
session
=
AgentSession
(
stt
=
deepgram
.
STT
(
model
=
"nova-3"
,
language
=
"multi"
)
,
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
,
tts
=
cartesia
.
TTS
(
)
,
vad
=
silero
.
VAD
.
load
(
)
,
turn_detection
=
MultilingualModel
(
)
,
)
await
session
.
start
(
room
=
ctx
.
room
,
agent
=
Assistant
(
)
,
room_input_options
=
RoomInputOptions
(
noise_cancellation
=
noise_cancellation
.
BVC
(
)
,
)
,
)
await
session
.
generate_reply
(
instructions
=
"Greet the user and offer your assistance."
)
if
__name__
==
"__main__"
:
agents
.
cli
.
run_app
(
agents
.
WorkerOptions
(
entrypoint_fnc
=
entrypoint
)
)
Copy
Download model files
To use the
turn-detector
,
silero
, or
noise-cancellation
plugins, you first need to download the model files:
python main.py download-files
Copy
Speak to your agent
Start your agent in
console
mode to run inside your terminal:
python main.py console
Copy
Your agent speaks to you in the terminal, and you can speak to it as well.
Connect to playground
Start your agent in
dev
mode to connect it to LiveKit and make it available from anywhere on the internet:
python main.py dev
Copy
Use the
Agents playground
to speak with your agent and explore its full range of multimodal capabilities.
Congratulations, your agent is up and running. Continue to use the playground or the
console
mode as you build and test your agent.
Agent CLI modes
In the
console
mode, the agent runs locally and is only available within your terminal.
Run your agent in
dev
(development / debug) or
start
(production) mode to connect to LiveKit and join rooms.
Next steps
Follow these guides bring your voice AI app to life in the real world.
Web and mobile frontends
Put your agent in your pocket with a custom web or mobile app.
Telephony integration
Your agent can place and receive calls with LiveKit's SIP integration.
Building voice agents
Comprehensive documentation to build advanced voice AI apps with LiveKit.
Worker lifecycle
Learn how to manage your agents with workers and jobs.
Deploying to production
Guide to deploying your voice agent in a production environment.
Integration guides
Explore the full list of AI providers available for LiveKit Agents.
Recipes
A comprehensive collection of examples, guides, and recipes for LiveKit Agents.
On this page
Overview
Requirements
Python
LiveKit server
AI providers
Setup
Packages
Environment variables
Agent code
Download model files
Speak to your agent
Connect to playground
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/start/telephony:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Getting started
Agent dispatch
Inbound calls
Dispatch rules
Answering the phone
Call your agent
Outbound calls
Dialing a number
Make a call with your agent
Voicemail detection
Hangup
Transferring call to another number
Recipes
Further reading
Overview
It's easy to integrate LiveKit Agents with telephony systems using Session Initiation Protocol (SIP). You can choose to support inbound calls, outbound calls, or both. LiveKit also provides features including DTMF, SIP REFER, and more.
Telephony integration requires no significant changes to your existing agent code, as phone calls are simply bridged into LiveKit rooms using a special participant type.
Getting started
Follow the
Voice AI quickstart
to get a simple agent up and running.
Set up a SIP trunk for your project.
Return to this guide to enable inbound and outbound calls.
Voice AI quickstart
Follow the Voice AI quickstart to get your agent up and running.
SIP trunk setup
Configure your SIP trunk provider to route calls in LiveKit.
Agent dispatch
LiveKit recommends using explicit agent dispatch for telephony integrations to ensure no unexpected automatic dispatch occurs given the complexity of inbound and outbound calling.
To enable explicit dispatch, give your agent a name. This disables automatic dispatch.
main.py
# ... your existing agent code ...
if
__name__
==
"__main__"
:
agents
.
cli
.
run_app
(
agents
.
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
# agent_name is required for explicit dispatch
agent_name
=
"my-telephony-agent"
)
)
Copy
Full examples
See the docs on
agent dispatch
for more complete examples.
Inbound calls
After you configure your
inbound trunk
follow these steps to enable inbound calling for your agent.
Dispatch rules
The following rule routes all inbound calls to a new room and dispatches your agent to that room:
dispatch-rule.json
{
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call-"
}
}
,
"roomConfig"
:
{
"agents"
:
[
{
"agentName"
:
"my-telephony-agent"
}
]
}
}
Copy
Create this rule with the following command:
lk sip dispatch create dispatch-rule.json
Copy
Answering the phone
Call the
generate_reply
method of your
AgentSession
to greet the caller after picking up. This code goes after
session.start
:
await
session
.
generate_reply
(
instructions
=
"Greet the user and offer your assistance."
)
Copy
Call your agent
After you start your agent with the following command, dial the number you set up earlier to hear your agent answer the phone.
python main.py dev
Copy
Outbound calls
After setting up your
outbound trunk
, you may place outbound calls by dispatching an agent and then creating a SIP participant.
The following guide describes how to modify the
voice AI quickstart
for outbound calling. Alternatively, see the following complete example on GitHub:
Outbound caller example
Complete example of an outbound calling agent.
Dialing a number
Add the following code so your agent reads the phone number and places an outbound call by creating a SIP participant after connection.
You should also remove the initial greeting or place it behind an
if
statement to ensure the agent waits for the user to speak first when placing an outbound call.
SIP trunk ID
You must fill in the
sip_trunk_id
for this example to work. You can get this from LiveKit CLI with
lk sip outbound list
.
main.py
# add these imports at the top of your file
from
livekit
import
api
import
json
# ... any existing code / imports ...
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
await
ctx
.
connect
(
)
# If a phone number was provided, then place an outbound call
# By having a condition like this, you can use the same agent for inbound/outbound telephony as well as web/mobile/etc.
dial_info
=
json
.
loads
(
ctx
.
job
.
metadata
)
phone_number
=
dial_info
[
"phone_number"
]
# The participant's identity can be anything you want, but this example uses the phone number itself
sip_participant_identity
=
phone_number
if
phone_number
is
not
None
:
# The outbound call will be placed after this method is executed
try
:
await
ctx
.
api
.
sip
.
create_sip_participant
(
api
.
CreateSIPParticipantRequest
(
# This ensures the participant joins the correct room
room_name
=
ctx
.
room
.
name
,
# This is the outbound trunk ID to use (i.e. which phone number the call will come from)
# You can get this from LiveKit CLI with `lk sip outbound list`
sip_trunk_id
=
'ST_xxxx'
,
# The outbound phone number to dial and identity to use
sip_call_to
=
phone_number
,
participant_identity
=
sip_participant_identity
,
# This will wait until the call is answered before returning
wait_until_answered
=
True
,
)
)
print
(
"call picked up successfully"
)
except
api
.
TwirpError
as
e
:
print
(
f"error creating SIP participant:
{
e
.
message
}
, "
f"SIP status:
{
e
.
metadata
.
get
(
'sip_status_code'
)
}
"
f"
{
e
.
metadata
.
get
(
'sip_status'
)
}
"
)
ctx
.
shutdown
(
)
# .. create and start your AgentSession as normal ...
# Add this guard to ensure the agent only speaks first in an inbound scenario.
# When placing an outbound call, its more customary for the recipient to speak first
# The agent will automatically respond after the user's turn has ended.
if
phone_number
is
None
:
await
session
.
generate_reply
(
instructions
=
"Greet the user and offer your assistance."
)
Copy
Make a call with your agent
Use either the LiveKit CLI or the Python API to instruct your agent to place an outbound phone call.
In this example, the job's metadata includes the phone number to call. You can extend this to include more information if needed for your use case.
LiveKit CLI
Python
The following command creates a new room and dispatches your agent to it with the phone number to call. Ensure the agent name matches the name you set earlier in the
agent dispatch
section.
lk dispatch create
\
--new-room
\
--agent-name my-telephony-agent
\
--metadata
'{"phone_number": "+15105550123"}'
# insert your own phone number here
Copy
Voicemail detection
Your agent may still encounter an automated system such as an answering machine or voicemail. You can give your LLM the ability to detect a likely voicemail system via tool call, and then perform special actions such as leaving a message and
hanging up
.
main.py
import
asyncio
# add this import at the top of your file
class
Assistant
(
Agent
)
:
## ... existing init code ...
@function_tool
async
def
detected_answering_machine
(
self
)
:
"""Call this tool if you have detected a voicemail system, AFTER hearing the voicemail greeting"""
await
self
.
session
.
generate_reply
(
instructions
=
"Leave a voicemail message letting the user know you'll call back later."
)
await
asyncio
.
sleep
(
0.5
)
# Add a natural gap to the end of the voicemail message
await
hangup_call
(
)
Copy
Hangup
To end a call for all participants, use the
delete_room
API. If only the agent session ends, the user will continue to hear silence until they hang up. The example below shows a basic
hangup_call
function you can use as a starting point.
main.py
# Add these imports at the top of your file
from
livekit
import
api
,
rtc
from
livekit
.
agents
import
get_job_context
# Add this function definition anywhere
async
def
hangup_call
(
)
:
ctx
=
get_job_context
(
)
if
ctx
is
None
:
# Not running in a job context
return
ctx
.
api
.
room
.
delete_room
(
api
.
DeleteRoomRequest
(
room
=
ctx
.
room
.
name
,
)
)
Copy
Transferring call to another number
In case the agent needs to transfer the call to another number or SIP destination, you can use the
transfer_sip_participant
API.
This is a "cold" transfer, where the agent hands the call off to another party without staying on the line. The current session ends after the transfer is complete.
main.py
class
Assistant
(
Agent
)
:
## ... existing init code ...
@function_tool
(
)
async
def
transfer_call
(
self
,
ctx
:
RunContext
)
:
"""Transfer the call to a human agent, called after confirming with the user"""
transfer_to
=
"+15105550123"
participant_identity
=
"+15105550123"
# let the message play fully before transferring
await
ctx
.
session
.
generate_reply
(
instructions
=
"Inform the user that you're transferring them to a different agent."
)
job_ctx
=
get_job_context
(
)
try
:
await
job_ctx
.
api
.
sip
.
transfer_sip_participant
(
api
.
TransferSIPParticipantRequest
(
room_name
=
job_ctx
.
room
.
name
,
participant_identity
=
participant_identity
,
# to use a sip destination, use `sip:user@host` format
transfer_to
=
f"tel:
{
transfer_to
}
"
,
)
)
except
Exception
as
e
:
print
(
f"error transferring call:
{
e
}
"
)
# give the LLM that context
return
"could not transfer call"
Copy
SIP REFER
You must enable SIP REFER on your SIP trunk provider to use
transfer_sip_participant
.
For Twilio, you must also enable
Enable PSTN Transfer
.
Recipes
The following recipes are particular helpful to learn more about telephony integration.
Company Directory
Build a AI company directory agent. The agent can respond to DTMF tones and voice prompts, then redirect callers.
SIP Warm Handoff
Transfer calls from an AI agent to a human operator seamlessly.
SIP Lifecycle
Complete lifecycle management for SIP calls.
Survey Caller
Automated survey calling system.
Further reading
The following guides provide more information on building voice agents for telephony.
Workflows
Orchestrate detailed workflows such as collecting credit card information over the phone.
Tool definition & use
Extend your agent's capabilities with tools.
Telephony documentation
Full documentation on the LiveKit SIP integration and features.
Agent speech
Customize and perfect your agent's verbal interactions.
On this page
Overview
Getting started
Agent dispatch
Inbound calls
Dispatch rules
Answering the phone
Call your agent
Outbound calls
Dialing a number
Make a call with your agent
Voicemail detection
Hangup
Transferring call to another number
Recipes
Further reading
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/start/frontend:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Starter apps
Media and text
Data sharing
State and control
Audio visualizer
Authentication
Responsiveness tips
Minimize connection time
Connection indicators
Effects
Overview
LiveKit Agents is ready to integrate with your preferred frontend platform using the
LiveKit SDKs
for JavaScript, Swift, Android, Flutter, React Native, and more. Your agent can communicate with your frontend through LiveKit WebRTC, which provides fast and reliable realtime connectivity.
For example, a simple voice agent subscribes to the user's microphone track and publishes its own.
Text transcriptions
are also available as text streams. A more complex agent with vision capabilities can subscribe to a video track published from the user's camera or shared screen. An agent can also publish its own video to implement a virtual avatar or other features.
In all of these cases, the LiveKit SDKs are production grade and easy to use so you can build useful and advanced agents without worrying about the complexities of realtime media delivery. This topic contains resources and tips for building a high-quality frontend for your agent.
Starter apps
LiveKit recommends using one of the following starter apps to get up and running quickly on your preferred platform. Each app is open source under the MIT License so you can freely modify it to your own needs. The mobile apps require a hosted token server, but include a
LiveKit Cloud Sandbox
for development purposes.
Swift
A native iOS, macOS, and visionOS voice assistant built in SwiftUI.
Web
A web voice assistant app built with React and Next.js.
Flutter
An iOS and Android voice assistant app built with Flutter.
Android
A native Android voice assistant app built with Kotlin and Jetpack Compose.
Media and text
To learn more about realtime media and text streams, see the following documentation.
Media tracks
Use the microphone, speaker, cameras, and screenshare with your agent.
Text streams
Send and receive realtime text and transcriptions.
Data sharing
To share images, files, or any other kind of data between your frontend and your agent, you can use the following features.
Byte streams
Send and receive images, files, or any other data.
Data packets
Low-level API for sending and receiving any kind of data.
State and control
In some cases, your agent and your frontend code might need a custom integration of state and configuration to meet your application's requirements. In these cases, the LiveKit realtime state and data features can be used to create a tightly-coupled and responsive experience.
AgentSession automatically manages the
lk.agent.state
participant attribute to contain the appropriate string value from among
initializing
,
listening
,
thinking
, or
speaking
.
State synchronization
Share custom state between your frontend and agent.
RPC
Define and call methods on your agent or your frontend from the other side.
Audio visualizer
The LiveKit component SDKs for React, SwiftUI, Android Compose, and Flutter include an audio visualizer component that can be used to give your voice agent a visual presence in your application.
For complete examples, see the sample apps listed above. The following documentation is a quick guide to using these components:
React
Swift
Android
Flutter
Install the
React components
and
styles
packages to use the
useVoiceAssistant
hook and the
BarVisualizer
. These components work automatically within a
LiveKitRoom
or
RoomContext.Provider
).
Also see
VoiceAssistantControlBar
, which provides a simple set of common UI controls for voice agent applications.
"use client"
;
import
"@livekit/components-styles"
;
import
{
useVoiceAssistant
,
BarVisualizer
,
}
from
"@livekit/components-react"
;
export
default
function
SimpleVoiceAssistant
(
)
{
// Get the agent's audio track and current state
const
{
state
,
audioTrack
}
=
useVoiceAssistant
(
)
;
return
(
<
div className
=
"h-80"
>
<
BarVisualizer state
=
{
state
}
barCount
=
{
5
}
trackRef
=
{
audioTrack
}
style
=
{
{
}
}
/
>
<
p className
=
"text-center"
>
{
state
}
<
/
p
>
<
/
div
>
)
;
}
Copy
Authentication
The LiveKit SDKs require a
token
to connect to a room. In web apps, you can typically include a simple token endpoint as part of the application.  For mobile apps, you'll need a separate token server.
Responsiveness tips
This section contains some suggestions to make your app feel more responsive to the user.
Minimize connection time
To connect your user to your agent, these steps must all occur:
Fetch an access token.
The user connects to the room.
Dispatch an agent process.
The agent connects to the room.
User and agent publish and subscribe to each other's media tracks.
If done in sequence, this takes up to a few seconds to complete. You can reduce this time by eliminating or parallelizing these steps.
Option 1: "Warm" token
In this case, your application will generate a token for the user at login with a long expiration time. When you need to connect to the room, the token is already available in your frontend.
Option 2: Dispatch agent during token generation
In this case, your application will optimistically create a room and dispatch the agent at the same time the token is generated, using
explicit agent dispatch
. This allows the user and the agent to connect to the room at the same time.
Connection indicators
Make your app feel more responsive, even when slow to connect, by linking various events into only one or two status indicators for the user rather than a number of discrete steps and UI changes.  Refer to the
event handling
documentation for more information on how to monitor the connection state and other events.
In the case that your agent fails to connect, you should notify the user and allow them to try again rather than leaving them to speak into an empty room.
Room connection
: The
room.connect
method can be awaited in most SDKs, and most also provide a
room.connectionState
property. Also monitor the
Disconnected
event to know when the connection is lost.
Agent presence
: Monitor
ParticipantConnected
events with
participant.kind === ParticipantKind.AGENT
Agent state
: Access the agent's state (
initializing
,
listening
,
thinking
, or
speaking
)
Track subscription
: Listen for
TrackSubscribed
events to know when your media has been subscribed to.
Effects
You should use sound effects, haptic feedback, and visual effects to make your agent feel more responsive. This is especially important during long thinking states (for instance, when performing external lookups or tool use). The
visualizer
includes basic "thinking" state indication and also allows the user to notice when their audio is not working. For more advanced effects, use the
state and control
features to trigger effects in your frontend.
On this page
Overview
Starter apps
Media and text
Data sharing
State and control
Audio visualizer
Authentication
Responsiveness tips
Minimize connection time
Connection indicators
Effects
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/start/playground:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Links
Overview
The LiveKit Agents playground is a versatile web frontend that makes it easy to test your multimodal AI agent without having to worry about UI until you're happy with your AI.
To use the playground, you first need to have an agent running in
dev
or
start
mode. If you haven't done that yet, first follow the
Voice AI quickstart
.
Feature
Notes
Audio
Mic input and speaker output with visualizer
Text
Live transcription and chat input
Video
Live webcam input, live output
Links
Follow these links to get started with the playground.
Hosted playground
A hosted playground that seamlessly integrates with LiveKit Cloud.
Source code
Run the playground yourself or use it as a starting point for your own application.
On this page
Overview
Links
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/start/v0-migration:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Unified agent interface
Customizing pipeline behavior
before_llm_cb -> llm_node
before_tts_cb -> tts_node
Tool definition and use
Chat context
Updating chat context
Transcriptions
Accepting text input
State change events
User state
Agent state
Removed features
Unified agent interface
Agents 1.0 introduces
AgentSession
, a single, unified
agent orchestrator
that serves as the foundation for all types of agents built using the framework.  With this change, the
VoicePipelineAgent
and
MultimodalAgent
classes have been deprecated and 0.x agents will need to be updated to use
AgentSession
in order to be compatible with 1.0 and later.
AgentSession
contains a superset of the functionality of
VoicePipelineAgent
and
MultimodalAgent
, allowing you to switch between pipelined and speech-to-speech models without changing your core application logic.
Version 0.x
Version 1.0
from
livekit
.
agents
import
JobContext
,
llm
from
livekit
.
agents
.
pipeline
import
VoicePipelineAgent
from
livekit
.
plugins
import
(
cartesia
,
deepgram
,
google
,
silero
,
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
initial_ctx
=
llm
.
ChatContext
(
)
.
append
(
role
=
"system"
,
text
=
"You are a helpful voice AI assistant."
,
)
agent
=
VoicePipelineAgent
(
vad
=
silero
.
VAD
.
load
(
)
,
stt
=
deepgram
.
STT
(
)
,
llm
=
google
.
LLM
(
)
,
tts
=
cartesia
.
TTS
(
)
,
)
await
agent
.
start
(
room
,
participant
)
await
agent
.
say
(
"Hey, how can I help you today?"
,
allow_interruptions
=
True
)
Copy
Customizing pipeline behavior
We’ve introduced more flexibility for developers to customize the behavior of agents built on 1.0 through the new concept of
pipeline nodes
, which enable custom processing within the pipeline steps while also delegating to the default implementation of each node as needed.
Pipeline nodes replaces the
before_llm_cb
and
before_tts_cb
callbacks.
before_llm_cb -> llm_node
before_llm_cb
has been replaced by
llm_node
. This node can be used to modify the chat context before sending it to LLM, or integrate with custom LLM providers without having to create a plugin. As long as it returns AsyncIterable[llm.ChatChunk], the LLM node will forward the chunks to the next node in the pipeline.
Version 0.x
Version 1.0
async
def
add_rag_context
(
assistant
:
VoicePipelineAgent
,
chat_ctx
:
llm
.
ChatContext
)
:
rag_context
:
str
=
retrieve
(
chat_ctx
)
chat_ctx
.
append
(
text
=
rag_context
,
role
=
"system"
)
agent
=
VoicePipelineAgent
(
.
.
.
before_llm_cb
=
add_rag_context
,
)
Copy
before_tts_cb -> tts_node
before_tts_cb
has been replaced by
tts_node
. This node gives greater flexibility in customizing the TTS pipeline. It's possible to modify the text before synthesis, as well as the audio buffers after synthesis.
Version 0.x
Version 1.0
def
_before_tts_cb
(
agent
:
VoicePipelineAgent
,
text
:
str
|
AsyncIterable
[
str
]
)
:
# The TTS is incorrectly pronouncing "LiveKit", so we'll replace it with MFA-style IPA
# spelling for Cartesia
return
tokenize
.
utils
.
replace_words
(
text
=
text
,
replacements
=
{
"livekit"
:
r"<<l|aj|v|cʰ|ɪ|t|>>"
}
)
agent
=
VoicePipelineAgent
(
.
.
.
before_tts_cb
=
_before_tts_cb
,
)
Copy
Tool definition and use
Agents 1.0 streamlines the way in which
tools
are defined for use within your agents, making it easier to add and maintain agent tools.  When migrating from 0.x to 1.0, developers will need to make the following changes to existing use of functional calling within their agents in order to be compatible with versions 1.0 and later.
The
@llm.ai_callable
decorator for function definition has been replaced with the new
@function_tool
decorator.
If you define your functions within an
Agent
and use the
@function_tool
decorator, these tools are automatically accessible to the LLM. In this scenario, you no longer required to define your functions in a
llm.FunctionContext
class and pass them into the agent constructor.
Argument types are now inferred from the function signature and docstring. Annotated types are no longer supported.
Functions take in a
RunContext
object, which provides access to the current agent state.
Version 0.x
Version 1.0
from
livekit
.
agents
import
llm
from
livekit
.
agents
.
pipeline
import
VoicePipelineAgent
from
livekit
.
agents
.
multimodal
import
MultimodalAgent
class
AssistantFnc
(
llm
.
FunctionContext
)
:
@llm
.
ai_callable
(
)
async
def
get_weather
(
self
,
.
.
.
)
.
.
.
fnc_ctx
=
AssistantFnc
(
)
pipeline_agent
=
VoicePipelineAgent
(
.
.
.
fnc_ctx
=
fnc_ctx
,
)
multimodal_agent
=
MultimodalAgent
(
.
.
.
fnc_ctx
=
fnc_ctx
,
)
Copy
Chat context
ChatContext has been overhauled in 1.0 to provide a more powerful and flexible API for managing chat history. It now accounts for differences between LLM providers—such as stateless and stateful APIs—while exposing a unified interface.
Chat history can now include three types of items:
ChatMessage
: a message associated with a role (e.g., user, assistant). Each message includes a list of
content
items, which can contain text, images, or audio.
FunctionCall
: a function call initiated by the LLM.
FunctionCallOutput
: the result returned from a function call.
Updating chat context
In 0.x, updating the chat context required modifying chat_ctx.messages directly. This approach was error-prone and difficult to time correctly, especially with realtime APIs.
In v1.0, there are two supported ways to update the chat context:
Agent handoff
–
transferring control
to a new agent, which will have its own chat context.
Explicit update
- calling
agent.update_chat_ctx()
to modify the context directly.
Transcriptions
Agents 1.0 brings some new changes to how
transcriptions
are handled:
Transcriptions now use
text streams
with topic
lk.transcription
.
The
old transcription protocol
is deprecated and will be removed in v1.1.
for now both protocols are used for backwards compatibility.
Upcoming versions SDKs/components standardize on text streams for transcriptions.
Accepting text input
Agents 1.0 introduces
improved support for text input
. Previously, text had to be manually intercepted and injected into the agent via
ChatManager
.
In this version, agents automatically receive text input from a text stream on the
lk.chat
topic.
The
ChatManager
has been removed in Python SDK v1.0.
State change events
User state
user_started_speaking
and
user_stopped_speaking
events are no longer emitted. They've been combined into a single
user_state_changed
event.
Version 0.x
Version 1.0
@agent
.
on
(
"user_started_speaking"
)
def
on_user_started_speaking
(
)
:
print
(
"User started speaking"
)
Copy
Agent state
Version 0.x
Version 1.0
@agent
.
on
(
"agent_started_speaking"
)
def
on_agent_started_speaking
(
)
:
# Log transcribed message from user
print
(
"Agent started speaking"
)
Copy
Removed features
OpenAI Assistants API support has been removed in 1.0.
The beta integration with the Assistants API in the OpenAI LLM plugin has been deprecated. Its stateful model made it difficult to manage state consistently between the API and agent.
On this page
Unified agent interface
Customizing pipeline behavior
before_llm_cb -> llm_node
before_tts_cb -> tts_node
Tool definition and use
Chat context
Updating chat context
Transcriptions
Accepting text input
State change events
User state
Agent state
Removed features
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/workflows:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Defining an agent
Handing off control to another agent
Context preservation
Passing state
Overriding plugins
Examples
Further reading
Overview
Agents are composable within a single session to model complex tasks with a high degree of reliability. Some specific scenarios where this is useful include:
Acquiring recording consent at the beginning of a call.
Collecting specific structured information, such as an address or a credit card number.
Moving through a series of questions, one at a time.
Leaving a voicemail message when the user is unavailable.
Including multiple personas with unique traits within a single session.
Defining an agent
Extend the
Agent
class to define a custom agent.
from
livekit
.
agents
import
Agent
class
HelpfulAssistant
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"You are a helpful voice AI assistant."
)
async
def
on_enter
(
self
)
-
>
None
:
await
self
.
session
.
say
(
"Hello, how can I help you today?"
)
Copy
You can also create an instance of
Agent
class directly:
agent
=
Agent
(
instructions
=
"You are a helpful voice AI assistant."
)
Copy
Handing off control to another agent
Return a different agent from within a tool call to hand off control. This allows the LLM to make decisions about when handoff should occur.
from
livekit
.
agents
import
Agent
,
function_tool
,
get_job_context
class
ConsentCollector
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"""Your are a voice AI agent with the singular task to collect positive
recording consent from the user. If consent is not given, you must end the call."""
)
async
def
on_enter
(
self
)
-
>
None
:
await
self
.
session
.
say
(
"May I record this call for quality assurance purposes?"
)
@function_tool
(
)
async
def
on_consent_given
(
self
)
:
"""Use this tool to indicate that consent has been given and the call may proceed."""
# Perform a handoff, immediately transfering control to the new agent
return
HelpfulAssistant
(
)
@function_tool
(
)
async
def
end_call
(
self
)
-
>
None
:
"""Use this tool to indicate that consent has not been given and the call should end."""
await
self
.
session
.
say
(
"Thank you for your time, have a wonderful day."
)
job_ctx
=
get_job_context
(
)
await
job_ctx
.
api
.
room
.
delete_room
(
api
.
DeleteRoomRequest
(
room
=
job_ctx
.
room
.
name
)
)
Copy
Context preservation
By default, each new agent starts with a fresh conversation history for their LLM prompt. To include the prior conversation, set the
chat_ctx
parameter in the
Agent
constructor. You can either copy the prior agent's
chat_ctx
, or construct a new one based on custom business logic to provide the appropriate context.
from
livekit
.
agents
import
ChatContext
,
function_tool
,
Agent
class
HelpfulAssistant
(
Agent
)
:
def
__init__
(
self
,
chat_ctx
:
ChatContext
)
:
super
(
)
.
__init__
(
instructions
=
"You are a helpful voice AI assistant."
,
chat_ctx
=
chat_ctx
)
class
ConsentCollector
(
Agent
)
:
# ...
@function_tool
(
)
async
def
on_consent_given
(
self
)
:
"""Use this tool to indicate that consent has been given and the call may proceed."""
# Pass the chat context during handoff
return
HelpfulAssistant
(
chat_ctx
=
self
.
session
.
chat_ctx
)
Copy
The complete conversation history for the session is always available in
session.history
.
Passing state
To store custom state within your session, use the
userdata
attribute. The type of userdata is up to you, but the recommended approach is to use a
dataclass
.
from
livekit
.
agents
import
AgentSession
from
dataclasses
import
dataclass
@dataclass
class
MySessionInfo
:
user_name
:
str
|
None
=
None
age
:
int
|
None
=
None
Copy
To add userdata to your session, pass it in the constructor. You must also specify the type of userdata on the
AgentSession
itself.
session
=
AgentSession
[
MySessionInfo
]
(
userdata
=
MySessionInfo
(
)
,
# ... tts, stt, llm, etc.
)
Copy
Userdata is available as
session.userdata
, and is also available within function tools on the
RunContext
. The following example shows how to use userdata in an agent workflow that starts with the
IntakeAgent
.
class
IntakeAgent
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"""Your are an intake agent. Learn the user's name and age."""
)
@function_tool
(
)
async
def
record_name
(
self
,
context
:
RunContext
[
MySessionInfo
]
,
name
:
str
)
:
"""Use this tool to record the user's name."""
context
.
userdata
.
user_name
=
name
return
self
.
_handoff_if_done
(
)
@function_tool
(
)
async
def
record_age
(
self
,
context
:
RunContext
[
MySessionInfo
]
,
age
:
int
)
:
"""Use this tool to record the user's age."""
context
.
userdata
.
age
=
age
return
self
.
_handoff_if_done
(
)
def
_handoff_if_done
(
self
)
:
if
self
.
session
.
userdata
.
user_name
and
self
.
session
.
userdata
.
age
:
return
HelpfulAssistant
(
)
else
:
return
None
class
HelpfulAssistant
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"You are a helpful voice AI assistant."
)
async
def
on_enter
(
self
)
-
>
None
:
userdata
:
MySessionInfo
=
self
.
session
.
userdata
await
self
.
session
.
generate_reply
(
instructions
=
f"Greet
{
userdata
.
user_name
}
and tell them a joke about being
{
userdata
.
age
}
years old."
)
Copy
Overriding plugins
You can override any of the plugins used in the session by setting the corresponding attributes in your
Agent
constructor. For instance, you can change the voice for a specific agent by overriding the
tts
attribute:
from
livekit
.
agents
import
Agent
from
livekit
.
plugins
import
cartesia
class
AssistantManager
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
instructions
=
"You are manager behind a team of helpful voice assistants."
,
tts
=
cartesia
.
TTS
(
voice
=
"6f84f4b8-58a2-430c-8c79-688dad597532"
)
)
Copy
Examples
These examples show how to build more complex workflows with multiple agents:
Medical Office Triage
Agent that triages patients based on symptoms and medical history.
Restaurant Agent
A restaurant front-of-house agent that can take orders, add items to a shared cart, and checkout.
Further reading
For more information on concepts touched on in this article, see the following related articles:
Tool definition and use
Complete guide to defining and using tools in your agents.
Nodes
Add custom behavior to any component of the voice pipeline.
Agent speech
Customize the speech output of your agents.
On this page
Overview
Defining an agent
Handing off control to another agent
Context preservation
Passing state
Overriding plugins
Examples
Further reading
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/speech:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Text to speech (TTS)
Speech-to-speech
Usage examples
Creating a TTS
RealtimeModel usage
Initiating speech
session.say
generate_reply
Controlling agent speech
SpeechHandle
Interruptions
Customizing pronunciation
Overview
Speech capabilities are a core feature of LiveKit agents, enabling them to interact with users through voice.
This guide covers the various speech features and functionalities available for agents.
LiveKit Agents provide a unified interface for controlling agents using both the STT-LLM-TTS pipeline and real-time models.
Text to speech (TTS)
TTS is a synthesis process that converts text into audio, giving AI agents a "voice." Some providers offer voice cloning so you can give your own voice to your agent.
Speech-to-speech
Multimodal, realtime APIs such as the
OpenAI Realtime
and
Gemini Live
can understand speech input and generate speech output directly. In this case, your application does not have access to the text stream prior to speech generation.
Usage examples
Creating a TTS
Create a TTS instance using the specific provider plugin. This example uses
ElevenLabs
for TTS:
Install the provider plugin:
pip
install
"livekit-agents[elevenlabs]~=1.0"
Copy
Create a TTS instance:
from
livekit
.
plugins
import
elevenlabs
from
livekit
.
agents
import
AgentSession
eleven_tts
=
elevenlabs
.
TTS
(
model
=
"eleven_turbo_v2_5"
,
voice
=
elevenlabs
.
Voice
(
id
=
"EXAVITQu4vr4xnSDxMaL"
,
name
=
"Bella"
,
)
,
language
=
"en"
,
enable_ssml_parsing
=
False
,
)
session
=
AgentSession
(
.
.
.
tts
=
eleven_tts
,
)
Copy
For a more complete example, see the
ElevenLabs TTS guide
.
RealtimeModel usage
This example creates an agent session using the OpenAI Realtime API:
from
livekit
.
plugins
import
openai
from
livekit
.
agents
import
AgentSession
session
=
AgentSession
(
llm
=
openai
.
realtime
.
RealtimeModel
(
voice
=
"alloy"
,
)
,
)
Copy
Initiating speech
By default, the agent waits for user input before responding—the Agents framework automatically handles response generation.
In some cases, though, the agent may need to initiate the conversation. For example, it might greet the user at the start of a session or check in after a period of silence.
session.say
To have the agent speak a predefined message, use
session.say()
. This triggers the configured TTS to synthesize speech and play it back to the user.
You can also optionally provide pre-synthesized audio for playback. This skips the TTS step and reduces response time.
Realtime models and TTS
The
say
method requires a TTS plugin. If you're using a realtime model, you need to add a TTS plugin to your session
or use the
generate_reply()
method instead.
await
session
.
say
(
"Hello. How can I help you today?"
,
allow_interruptions
=
False
,
)
Copy
Parameters
text
str | AsyncIterable[str]
Required
#
The text to speak.
audio
AsyncIterable[rtc.AudioFrame]
Optional
#
Pre-synthesized audio to play.
allow_interruptions
boolean
Optional
#
If
True
, allow the user to interrupt the agent while speaking. (default
True
)
add_to_chat_ctx
boolean
Optional
#
If
True
, add the text to the agent's chat context after playback. (default
True
)
Returns
Returns a
SpeechHandle
object.
generate_reply
To make conversations more dynamic, use
session.generate_reply()
to prompt the LLM to generate a response.
There are two ways to use
generate_reply
:
give the agent instructions to generate a response
session
.
generate_reply
(
instructions
=
"greet the user and ask where they are from"
,
)
Copy
provide the user's input via text
session
.
generate_reply
(
user_input
=
"how is the weather today?"
,
)
Copy
Impact to chat history
When using
generate_reply
with
instructions
, the agent uses the instructions to generate a response, which is added to the chat history. The instructions themselves are not recorded in the history.
In contrast,
user_input
is directly added to the chat history.
Parameters
user_input
string
Optional
#
The user input to respond to.
instructions
string
Optional
#
Instructions for the agent to use for the reply.
allow_interruptions
boolean
Optional
#
If
True
, allow the user to interrupt the agent while speaking. (default
True
)
Returns
Returns a
SpeechHandle
object.
Controlling agent speech
SpeechHandle
The
say()
and
generate_reply()
methods return a
SpeechHandle
object, which lets you track the state of the agent's speech. This can be useful for coordinating follow-up actions—for example, notifying the user before ending the call.
await
session
.
say
(
"Goodbye for now."
,
allow_interruptions
=
False
)
# the above is a shortcut for
# handle = session.say("Goodbye for now.", allow_interruptions=False)
# await handle.wait_for_playout()
Copy
You can wait for the agent to finish speaking before continuing:
handle
=
session
.
generate_reply
(
instructions
=
"Tell the user we're about to run some slow operations."
)
# perform an operation that takes time
.
.
.
await
handle
# finally wait for the speech
Copy
The following example makes a web request for the user, and cancels the request when the user interrupts:
async
with
aiohttp
.
ClientSession
(
)
as
client_session
:
web_request
=
client_session
.
get
(
'https://api.example.com/data'
)
handle
=
await
session
.
generate_reply
(
instructions
=
"Tell the user we're processing their request."
)
if
handle
.
interrupted
:
# if the user interrupts, cancel the web_request too
web_request
.
cancel
(
)
Copy
SpeechHandle
has an API similar to
ayncio.Future
, allowing you to add a callback:
handle
=
session
.
say
(
"Hello world"
)
handle
.
add_done_callback
(
lambda
_
:
print
(
"speech done"
)
)
Copy
Getting the current speech
You can access the current speech handle via
AgentSession.current_speech
. This is useful for checking the status of the agent's speech from anywhere in your program.
Interruptions
By default, the agent stops speaking when it detects that the user has started speaking. This behavior can be disabled by setting
allow_interruptions=False
when scheduling speech.
To explicitly interrupt the agent, call the
interrupt()
method on the handle or session at any time. This can be performed even when
allow_interruptions
is set to
False
.
handle
=
session
.
say
(
"Hello world"
)
handle
.
interrupt
(
)
# or from the session
session
.
interrupt
(
)
Copy
Customizing pronunciation
Most TTS providers allow you to customize pronunciation of words using Speech Synthesis Markup Language (SSML) using
some or all of the SSML tags in the following table.
SSML Tag
Description
phoneme
Used for phonetic pronunciation using a standard phonetic alphabet. These tags provide a phonetic pronunciation for the enclosed text.
say as
Specifies how to interpret the enclosed text. For example, use
character
to speak each character individually,
or
date
to specify a calendar date.
lexicon
A custom dictionary that defines the pronunciation of certain words using phonetic notation or text-to-pronunciation mappings.
emphasis
Speak text with an emphasis.
break
Add a manual pause.
prosody
Controls pitch, speaking rate, and volume of speech output.
On this page
Overview
Text to speech (TTS)
Speech-to-speech
Usage examples
Creating a TTS
RealtimeModel usage
Initiating speech
session.say
generate_reply
Controlling agent speech
SpeechHandle
Interruptions
Customizing pronunciation
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/tools:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Tool definition
Name and description
Arguments and return value
RunContext
Error handling
Dynamic and shared tools
Updating tools
Programmatic tool creation
Forwarding to the frontend
Agent implementation
Frontend implementation
Examples
Further reading
Overview
LiveKit Agents has full support for LLM tool use. This feature allows you to create a custom library of tools to extend your agent's context, create interactive experiences, and overcome LLM limitations. Within a tool, you can:
Generate
agent speech
with
session.say()
or
session.generate_reply()
.
Call methods on the frontend using
RPC
.
Handoff control to another agent as part of a
workflow
.
Store and retrieve session data from the
context
.
Anything else that a Python function can do.
Tool definition
Add tools to your agent class with the
@function_tool
decorator. The LLM has access to them automatically.
from
livekit
.
agents
import
function_tool
,
Agent
,
RunContext
class
MyAgent
(
Agent
)
:
@function_tool
(
)
async
def
lookup_weather
(
self
,
context
:
RunContext
,
location
:
str
,
)
-
>
dict
[
str
,
Any
]
:
"""Look up weather information for a given location.
Args:
location: The location to look up weather information for.
"""
return
{
"weather"
:
"sunny"
,
"temperature_f"
:
70
}
Copy
Best practices
A good tool definition is key to reliable tool use from your LLM. Be specific about what the tool does, when it should or should not be used, what the arguments are for, and what type of return value to expect.
Name and description
By default, the tool name is the name of the function, and the description is its docstring. Override this behavior with the
name
and
description
arguments to the
@function_tool
decorator.
Arguments and return value
The tool arguments are copied automatically by name from the function arguments. Type hints for arguments and return value are included, if present.
Place additional information about the tool arguments and return value, if needed, in the tool description.
RunContext
Tools include support for a special
context
argument. This contains access to the current
session
,
function_call
,
speech_handle
, and
userdata
. Consult the documentation on
speech
and
state within workflows
for more information about how to use these features.
Error handling
Raise the
ToolError
exception to return an error to the LLM in place of a response. You may include a custom message to describe the error and/or recovery options.
@function_tool
(
)
async
def
lookup_weather
(
self
,
context
:
RunContext
,
location
:
str
,
)
-
>
dict
[
str
,
Any
]
:
if
location
==
"mars"
:
raise
ToolError
(
"This location is coming soon. Please join our mailing list to stay updated."
)
else
:
return
{
"weather"
:
"sunny"
,
"temperature_f"
:
70
}
Copy
Dynamic and shared tools
You can exercise more control over the tools available by setting the
tools
argument directly.
To share a tool between multiple agents, define it outside of their class and then provide it to each. The
RunContext
is especially useful for this purpose to access the current session, agent, and state.
from
livekit
.
agents
import
function_tool
,
Agent
,
RunContext
@function_tool
(
)
async
def
lookup_user
(
context
:
RunContext
,
user_id
:
str
,
)
-
>
dict
:
"""Look up a user's information by ID."""
return
{
"name"
:
"John Doe"
,
"email"
:
"john.doe@example.com"
}
class
AgentA
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
tools
=
[
lookup_user
]
,
# ...
)
class
AgentB
(
Agent
)
:
def
__init__
(
self
)
:
super
(
)
.
__init__
(
tools
=
[
lookup_user
]
,
# ...
)
Copy
Updating tools
Use
agent.update_tools()
to update available tools after creating an agent. Note that this replaces
all
tools, including those registered automatically within the agent class.
# add a tool
agent
.
update_tools
(
agent
.
tools
+
[
tool_a
]
)
# remove a tool
agent
.
update_tools
(
agent
.
tools
-
[
tool_a
]
)
# replace all tools
agent
.
update_tools
(
[
tool_a
,
tool_b
]
)
Copy
Programmatic tool creation
Tools can also be created programmatically, which is useful when loading definitions from dynamic sources like a database.
from
typing
import
Callable
def
set_input_for
(
field
:
str
)
:
async
def
set_input
(
input
:
str
)
:
# custom logic to set input
return
f"field
{
field
}
was set to
{
input
}
"
return
set_input
phone_number_description
=
"""
Call this function when user has provided their phone number.
Args:
phone: The phone number to set.
"""
await
agent
.
update_tools
(
agent
.
tools
+
[
function_tool
(
set_input_for
(
"phone"
)
,
name
=
"set_phone_number"
,
description
=
phone_number_description
)
]
)
Copy
Forwarding to the frontend
Forward tool calls to a frontend app using
RPC
. This is useful when the data needed to fulfill the function call is only available at the frontend. You may also use RPC to trigger actions or UI updates in a structured way.
For instance, here's a function that accesses the user's live location from their web browser:
Agent implementation
from
livekit
.
agents
import
function_tool
,
get_job_context
,
RunContext
@function_tool
(
)
async
def
get_user_location
(
context
:
RunContext
,
high_accuracy
:
bool
)
:
"""Retrieve the user's current geolocation as lat/lng.
Args:
high_accuracy: Whether to use high accuracy mode, which is slower but more precise
Returns:
A dictionary containing latitude and longitude coordinates
"""
try
:
participant_identity
=
next
(
iter
(
get_job_context
(
)
.
room
.
remote_participants
)
)
response
=
await
context
.
session
.
room
.
local_participant
.
perform_rpc
(
destination_identity
=
participant_identity
,
method
=
"getUserLocation"
,
payload
=
json
.
dumps
(
{
"highAccuracy"
:
high_accuracy
}
)
,
response_timeout
=
10.0
if
high_accuracy
else
5.0
,
)
return
response
except
Exception
:
raise
ToolError
(
"Unable to retrieve user location"
)
Copy
Frontend implementation
The following example uses the JavaScript SDK. The same pattern works for other SDKs. For more examples, see the
RPC documentation
.
import
{
RpcError
,
RpcInvocationData
}
from
'livekit-client'
;
localParticipant
.
registerRpcMethod
(
'getUserLocation'
,
async
(
data
:
RpcInvocationData
)
=>
{
try
{
let
params
=
JSON
.
parse
(
data
.
payload
)
;
const
position
:
GeolocationPosition
=
await
new
Promise
(
(
resolve
,
reject
)
=>
{
navigator
.
geolocation
.
getCurrentPosition
(
resolve
,
reject
,
{
enableHighAccuracy
:
params
.
highAccuracy
??
false
,
timeout
:
data
.
responseTimeout
,
}
)
;
}
)
;
return
JSON
.
stringify
(
{
latitude
:
position
.
coords
.
latitude
,
longitude
:
position
.
coords
.
longitude
,
}
)
;
}
catch
(
error
)
{
throw
new
RpcError
(
1
,
"Could not retrieve user location"
)
;
}
}
)
;
Copy
Examples
Use of enum
Example showing how to annotate arguments with enum.
Dynamic tool creation
Complete example with dynamic tool lists.
RAG
An agent that can answer questions using RAG.
Further reading
RPC
Complete documentation on function calling between LiveKit participants.
Agent speech
More information about precise control over agent speech output.
Workflows
Read more about handing off control to other agents.
On this page
Overview
Tool definition
Name and description
Arguments and return value
RunContext
Error handling
Dynamic and shared tools
Updating tools
Programmatic tool creation
Forwarding to the frontend
Agent implementation
Frontend implementation
Examples
Further reading
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/nodes:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Pipeline and realtime agent differences
Agent with a voice pipeline
Agent with a realtime model
Use cases for customization
Customizing node behavior
On enter and exit nodes
On turn completed node
STT node
LLM node
TTS node
Transcription node
Realtime audio output node
Examples
Overview
The Agents framework allows you to fully customize your agent's behavior at multiple
nodes
in the processing path.
A node is a point in the path where one process transitions to another. In the case of STT, LLM, and TTS nodes, in addition to customizing the pre- and post-processing at the transition point from one node to the next, you can also
entirely replace the default process with custom code.
These nodes are exposed on the
Agent
class and occur at the following points in the pipeline:
on_enter()
: Agent enters session.
on_exit()
: Agent exits session.
on_user_turn_completed()
: User's turn is completed.
transcription_node()
: Processing agent's LLM output to transcriptions.
stt_node()
: Agent's STT processing step (pipeline only).
llm_node()
: Agent's LLM processing step (pipeline only).
tts_node()
: Agent's TTS processing step (pipeline only).
realtime_audio_output_node()
: Agent's audio output step (realtime only).
Note
If you're using a realtime model,
on_user_turn_completed
is available only if turn detection is handled by the agent instead of the realtime API.
Pipeline and realtime agent differences
Realtime agents aren't componentized like pipeline agents and don't have nodes for STT, LLM, and TTS. Instead, realtime agents use a single model for the entire agent, and the agent processes user input in realtime. You can still customize the behavior of a realtime agent by overriding the transcription node, updating the agent's instructions, or adding to its chat context.
Agent with a voice pipeline
Processing path for a voice pipeline agent:
Agent with a realtime model
Processing path for a realtime agent:
Use cases for customization
The following use cases are some examples of how you can customize your agent's behavior:
Use a custom STT, LLM, or TTS provider without a plugin.
Generate a custom greeting when an agent enters a session.
Modify STT output to remove filler words before sending it to the LLM.
Modify LLM output before sending it to TTS to customize pronunciation.
Update the user interface when an agent or user finishes speaking.
Customizing node behavior
Each node is a step in the agent pipeline where processing takes place. By default, some nodes are stub methods, and other nodes (the STT, LLM, and TTS nodes) execute
the code in the provider plugin. For these nodes, you can customize behavior by overriding the node and adding additional processing before, after, or instead of
the default behavior.
Stub methods are provided to allow you to add functionality at specific points in the processing path.
On enter and exit nodes
The
on_enter
and
on_exit
nodes are called when the agent enters or leaves an agent session. When an agent enters a session, it becomes that agent in control and
handles processing for the session until the agent exits. To learn more, see
Workflows
.
For example, initiate a conversation when an agent enters the session:
async
def
on_enter
(
self
)
:
# Instruct the agent to greet the user when it's added to a session
self
.
session
.
generate_reply
(
instructions
=
"Greet the user with a warm welcome"
,
)
Copy
For a more comprehensive example of a handoff between agents, and saving chat history in the
on_enter
node,
see the
restaurant ordering and reservations example
.
You can override the
on_exit
method to say goodbye before the agent exits the session:
async
def
on_exit
(
self
)
:
# Say goodbye
await
self
.
session
.
generate_reply
(
instructions
=
"Tell the user a friendly goodbye before you exit."
,
)
Copy
On turn completed node
The
on_user_turn_completed
node is called when the user finishes speaking. You can customize this node by overriding the
on_user_turn_completed
method in your
Agent
.
At this point,
new_message
contains the user's input but hasn't yet been added to the chat context. The message is added after
on_user_turn_completed
returns.
This hook can be used to perform RAG (retrieval-augmented generation) by injecting additional context into the chat history before generation.
async
def
on_user_turn_completed
(
self
,
turn_ctx
:
ChatContext
,
new_message
:
ChatMessage
,
)
-
>
None
:
# look up context via RAG
rag_content
=
await
my_rag_lookup
(
new_message
.
text_content
(
)
)
turn_ctx
.
add_message
(
role
=
"assistant"
,
content
=
rag_content
)
# changes to chat_ctx is used for only the next generation, and not persisted
# to persist changes to the chat context do the following:
# chat_ctx = turn_ctx.copy()
# chat_ctx.add_message(...)
# await self.update_chat_ctx(chat_ctx)
Copy
To abort generation entirely—for example, in a push-to-talk interface—you can do the following:
async
def
on_user_turn_completed
(
self
,
turn_ctx
:
ChatContext
,
new_message
:
ChatMessage
,
)
-
>
None
:
if
not
new_message
.
text_content
:
# for example, raise StopResponse to stop the agent from generating a reply
raise
StopResponse
(
)
Copy
For a complete example, see the
multi-user agent with push to talk example
.
STT node
From the STT node, you can customize how audio frames are handled before being sent to the default STT provider, and post-process the STT output before it's passed to the LLM.
To use the default implementation, call
Agent.default.stt_node()
.
For example, you can add noise filtering to the STT node by overriding the
stt_node
method in your
Agent
:
# add these imports
from
livekit
import
rtc
from
livekit
.
agents
.
voice
import
ModelSettings
from
livekit
.
agents
import
stt
from
typing
import
AsyncIterable
,
Optional
async
def
stt_node
(
self
,
audio
:
AsyncIterable
[
rtc
.
AudioFrame
]
,
model_settings
:
ModelSettings
)
-
>
Optional
[
AsyncIterable
[
stt
.
SpeechEvent
]
]
:
async
def
filtered_audio
(
)
:
async
for
frame
in
audio
:
# Apply some noise filtering logic here
yield
frame
async
for
event
in
Agent
.
default
.
stt_node
(
self
,
filtered_audio
(
)
,
model_settings
)
:
yield
event
Copy
LLM node
The LLM node is responsible for generating the agent's response. You can customize the LLM node by overriding the
llm_node
method in your
Agent
.
llm_node
can be used to integrate with custom LLM providers without having to create a plugin. As long as it returns
AsyncIterable[llm.ChatChunk]
, the LLM node will forward the chunks to the next node in the pipeline.
You can also update the LLM output before sending it to the TTS node as in the following example:
# add these imports
from
livekit
.
agents
.
voice
import
ModelSettings
from
livekit
.
agents
import
llm
,
FunctionTool
from
typing
import
AsyncIterable
async
def
llm_node
(
self
,
chat_ctx
:
llm
.
ChatContext
,
tools
:
list
[
FunctionTool
]
,
model_settings
:
ModelSettings
)
-
>
AsyncIterable
[
llm
.
ChatChunk
]
:
# Process with base LLM implementation
async
for
chunk
in
Agent
.
default
.
llm_node
(
self
,
chat_ctx
,
tools
,
model_settings
)
:
# Do something with the LLM output before sending it to the next node
yield
chunk
Copy
llm_node
can also be used to handle structured output. See full example
here
.
TTS node
The TTS node is responsible for converting the LLM output into audio. You can customize the TTS node by overriding the
tts_node
method in your
Agent
. For example, you can update the TTS output before sending it to the user interface as in the following example:
# add these imports
from
livekit
import
rtc
from
livekit
.
agents
.
voice
import
ModelSettings
from
livekit
.
agents
import
tts
from
typing
import
AsyncIterable
async
def
tts_node
(
self
,
text
:
AsyncIterable
[
str
]
,
model_settings
:
ModelSettings
)
-
>
AsyncIterable
[
rtc
.
AudioFrame
]
:
"""
Process text-to-speech with custom pronunciation rules before synthesis.
Adjusts common technical terms and abbreviations for better pronunciation.
"""
# Dictionary of pronunciation replacements.
# Support for custom pronunciations depends on the TTS provider.
# To learn more, see the Speech documentation:
# https://docs.livekit.io/agents/build/speech/#pronunciation.
pronunciations
=
{
"API"
:
"A P I"
,
"REST"
:
"rest"
,
"SQL"
:
"sequel"
,
"kubectl"
:
"kube control"
,
"AWS"
:
"A W S"
,
"UI"
:
"U I"
,
"URL"
:
"U R L"
,
"npm"
:
"N P M"
,
"LiveKit"
:
"Live Kit"
,
"async"
:
"a sink"
,
"nginx"
:
"engine x"
,
}
async
def
adjust_pronunciation
(
input_text
:
AsyncIterable
[
str
]
)
-
>
AsyncIterable
[
str
]
:
async
for
chunk
in
input_text
:
modified_chunk
=
chunk
# Apply pronunciation rules
for
term
,
pronunciation
in
pronunciations
.
items
(
)
:
# Use word boundaries to avoid partial replacements
modified_chunk
=
re
.
sub
(
rf'\b
{
term
}
\b'
,
pronunciation
,
modified_chunk
,
flags
=
re
.
IGNORECASE
)
yield
modified_chunk
# Process with modified text through base TTS implementation
async
for
frame
in
Agent
.
default
.
tts_node
(
self
,
adjust_pronunciation
(
text
)
,
model_settings
)
:
yield
frame
Copy
Transcription node
The transcription node is part of the forwarding path for agent transcriptions. By default, the node
simply passes the transcription to the task that forwards it to the designated output. You can customize
this behavior by overriding the
transcription_node
method in your
Agent
. For example, you can strip any unwanted formatting before it's sent to the client as transcripts.
# add these imports
from
livekit
.
agents
.
voice
import
ModelSettings
from
typing
import
AsyncIterable
async
def
transcription_node
(
self
,
text
:
AsyncIterable
[
str
]
,
model_settings
:
ModelSettings
)
-
>
AsyncIterable
[
str
]
:
def
cleanup_text
(
text_chunk
:
str
)
-
>
str
:
# Strip unwanted characters
return
text_chunk
.
replace
(
"😘"
,
""
)
async
for
delta
in
text
:
yield
cleanup_text
(
delta
)
Copy
Realtime audio output node
The
realtime_audio_output_node
is called when a realtime model outputs speech. This allows you to modify the audio output before it's sent to the user. For example, you can speed up or slow down the audio in the following example:
# add these imports
from
livekit
import
rtc
from
livekit
.
agents
.
voice
import
ModelSettings
from
livekit
.
agents
import
utils
from
typing
import
AsyncIterable
def
_process_audio
(
self
,
frame
:
rtc
.
AudioFrame
)
-
>
rtc
.
AudioFrame
:
pass
async
def
_process_audio_stream
(
audio
:
AsyncIterable
[
rtc
.
AudioFrame
]
)
-
>
AsyncIterable
[
rtc
.
AudioFrame
]
:
stream
:
utils
.
audio
.
AudioByteStream
|
None
=
None
async
for
frame
in
audio
:
if
stream
is
None
:
stream
=
utils
.
audio
.
AudioByteStream
(
sample_rate
=
frame
.
sample_rate
,
num_channels
=
frame
.
num_channels
,
samples_per_channel
=
frame
.
sample_rate
//
10
,
# 100ms
)
for
f
in
stream
.
push
(
frame
.
data
)
:
yield
_process_audio
(
f
)
for
f
in
stream
.
flush
(
)
:
yield
_process_audio
(
f
)
async
def
realtime_audio_output_node
(
self
,
audio
:
AsyncIterable
[
rtc
.
AudioFrame
]
,
model_settings
:
ModelSettings
)
-
>
AsyncIterable
[
rtc
.
AudioFrame
]
:
return
_process_audio_stream
(
Agent
.
default
.
realtime_audio_output_node
(
self
,
audio
,
model_settings
)
)
Copy
See full example
here
.
Examples
The following examples demonstrate various node customizations:
Chain of thought agent
Build an agent for chain-of-thought reasoning using the `llm_node` to clean the text before TTS.
Keyword Detection
Detect specific keywords in speech in real-time.
LLM Content Filter
Implement content filtering in the `llm_node`.
On this page
Overview
Pipeline and realtime agent differences
Agent with a voice pipeline
Agent with a realtime model
Use cases for customization
Customizing node behavior
On enter and exit nodes
On turn completed node
STT node
LLM node
TTS node
Transcription node
Realtime audio output node
Examples
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/media:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Receiving tracks
Manually handling subscriptions
Working with video
Publishing
Publishing audio
Publishing video
Recipes
Publishing background audio
Reducing background noise
Synchronizing audio and video
Overview
Tracks
are one of the core constructs of LiveKit and represent a realtime media stream that participants can publish and consume. Tracks include audio and video input from participants, including agent participants. User input is available through enabled input devices, for example, a microphone or camera, but can include other media types. To learn more about tracks, see
Media tracks
.
Receiving tracks
To see and hear published tracks, you must
subscribe
to them. By default, the
job
context automatically subscribes to all published tracks from all participants. You can
manage subscriptions manually
by turning this off.
LiveKit reads WebRTC tracks as streams, exposed in Python as
AsyncIterators
. The LiveKit SDKs provide utilities for working with both audio and video tracks.
When you subscribe to a track, it triggers a
TrackSubscribed
event
. You can add the following example to your entrypoint function to handle tracks when this event is triggered:
Python
Node.js
from
livekit
import
rtc
async
def
do_something
(
track
:
rtc
.
Track
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_AUDIO
:
audio_stream
=
rtc
.
AudioStream
(
track
)
async
for
event
in
audio_stream
:
# Do something here to process event.frame
pass
await
audio_stream
.
aclose
(
)
elif
track
.
kind
==
rtc
.
TrackKind
.
KIND_VIDEO
:
video_stream
=
rtc
.
VideoStream
(
track
)
async
for
event
in
video_stream
:
# Do something here to process event.frame
pass
await
video_stream
.
aclose
(
)
@ctx
.
room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
publication
:
rtc
.
TrackPublication
,
participant
:
rtc
.
RemoteParticipant
,
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_AUDIO
:
asyncio
.
create_task
(
do_something
(
track
)
)
elif
track
.
kind
==
rtc
.
TrackKind
.
KIND_VIDEO
:
asyncio
.
create_task
(
do_something
(
track
)
)
Copy
Manually handling subscriptions
By default, the agent automatically subscribes to tracks when a worker accepts the job. To manage subscriptions manually, set
auto_subscribe
to
AutoSubscribe.SUBSCRIBE_NONE
:
Python
async
def
entrypoint_fnc
(
ctx
:
JobContext
)
:
await
ctx
.
connect
(
# valid values are SUBSCRIBE_ALL, SUBSCRIBE_NONE, VIDEO_ONLY, AUDIO_ONLY
# when omitted, it defaults to SUBSCRIBE_ALL
auto_subscribe
=
AutoSubscribe
.
SUBSCRIBE_NONE
,
)
Copy
Note
LiveKit server streams data for all subscribed tracks. In a room with many published tracks you should subscribe only to the ones you need to ensure efficient use of CPU and memory in your agent.
Working with video
LiveKit supports many video buffer encodings and translates between them automatically.
VideoFrame
provides the current video buffer type and a method to convert it to any of the other encodings:
Python
async
def
handle_video
(
track
:
rtc
.
Track
)
:
video_stream
=
rtc
.
VideoStream
(
track
)
async
for
event
in
video_stream
:
video_frame
=
event
.
frame
current_type
=
video_frame
.
type
frame_as_bgra
=
video_frame
.
convert
(
rtc
.
VideoBufferType
.
BGRA
)
# [...]
await
video_stream
.
aclose
(
)
@ctx
.
room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
publication
:
rtc
.
TrackPublication
,
participant
:
rtc
.
RemoteParticipant
,
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_VIDEO
:
asyncio
.
create_task
(
handle_video
(
track
)
)
Copy
Publishing
Agents publish data to their tracks as a continuous live feed. Audio streams carry raw PCM data at a specified sample rate and channel count, while video streams can transmit data in any of
11 buffer encodings
.
Publishing audio
Publishing audio involves splitting the stream into audio frames of a configurable length. An internal buffer holds 50 ms of queued audio to be sent to the realtime stack. The
capture_frame
method, used to send new frames, is blocking and doesn't return control until the buffer has taken in the entire frame. This allows for easier interruption handling.
In order to publish an audio track, you need to determine the sample rate and number of channels beforehand, as well as the length (number of samples) of each frame. In the following example, the agent transmits a constant 16-bit sine wave at 48kHz in 10 ms long frames:
Python
SAMPLE_RATE
=
48000
NUM_CHANNELS
=
1
# mono audio
AMPLITUDE
=
2
**
8
-
1
SAMPLES_PER_CHANNEL
=
480
# 10 ms at 48kHz
async
def
entrypoint
(
ctx
:
JobContext
)
:
await
ctx
.
connect
(
)
source
=
rtc
.
AudioSource
(
SAMPLE_RATE
,
NUM_CHANNELS
)
track
=
rtc
.
LocalAudioTrack
.
create_audio_track
(
"example-track"
,
source
)
# since the agent is a participant, our audio I/O is its "microphone"
options
=
rtc
.
TrackPublishOptions
(
source
=
rtc
.
TrackSource
.
SOURCE_MICROPHONE
)
# ctx.agent is an alias for ctx.room.local_participant
publication
=
await
ctx
.
agent
.
publish_track
(
track
,
options
)
frequency
=
440
async
def
_sinewave
(
)
:
audio_frame
=
rtc
.
AudioFrame
.
create
(
SAMPLE_RATE
,
NUM_CHANNELS
,
SAMPLES_PER_CHANNEL
)
audio_data
=
np
.
frombuffer
(
audio_frame
.
data
,
dtype
=
np
.
int16
)
time
=
np
.
arange
(
SAMPLES_PER_CHANNEL
)
/
SAMPLE_RATE
total_samples
=
0
while
True
:
time
=
(
total_samples
+
np
.
arange
(
SAMPLES_PER_CHANNEL
)
)
/
SAMPLE_RATE
sinewave
=
(
AMPLITUDE
*
np
.
sin
(
2
*
np
.
pi
*
frequency
*
time
)
)
.
astype
(
np
.
int16
)
np
.
copyto
(
audio_data
,
sinewave
)
# send this frame to the track
await
source
.
capture_frame
(
frame
)
total_samples
+=
samples_per_channel
Copy
Warning
When streaming finite audio (for example, from a file), make sure the frame length isn't longer than the number of samples left to stream, otherwise the end of the buffer consists of noise.
Publishing video
When publishing video tracks, you need to establish the frame rate and buffer encoding of the video beforehand. In this example, the agent connects to the room and starts publishing a solid color frame at 10 frames per second:
Python
WIDTH
=
640
HEIGHT
=
480
async
def
entrypoint
(
ctx
:
JobContext
)
:
await
ctx
.
connect
(
)
source
=
rtc
.
VideoSource
(
WIDTH
,
HEIGHT
)
track
=
rtc
.
LocalVideoTrack
.
create_video_track
(
"example-track"
,
source
)
options
=
rtc
.
TrackPublishOptions
(
# since the agent is a participant, our video I/O is its "camera"
source
=
rtc
.
TrackSource
.
SOURCE_CAMERA
,
simulcast
=
True
,
# when modifying encoding options, max_framerate and max_bitrate must both be set
video_encoding
=
rtc
.
VideoEncoding
(
max_framerate
=
30
,
max_bitrate
=
3_000_000
,
)
,
audio_encoding
=
rtc
.
AudioEncoding
(
max_bitrate
=
48000
)
,
video_codec
=
rtc
.
VideoCodec
.
H264
,
)
publication
=
await
ctx
.
agent
.
publish_track
(
track
,
options
)
# this color is encoded as ARGB. when passed to VideoFrame it gets re-encoded.
COLOR
=
[
255
,
255
,
0
,
0
]
;
# FFFF0000 RED
async
def
_draw_color
(
)
:
argb_frame
=
bytearray
(
WIDTH
*
HEIGHT
*
4
)
while
True
:
await
asyncio
.
sleep
(
0.1
)
# 10 fps
argb_frame
[
:
]
=
COLOR
*
WIDTH
*
HEIGHT
frame
=
rtc
.
VideoFrame
(
WIDTH
,
HEIGHT
,
rtc
.
VideoBufferType
.
RGBA
,
argb_frame
)
# send this frame to the track
source
.
capture_frame
(
frame
)
asyncio
.
create_task
(
_draw_color
(
)
)
Copy
Note
Although the published frame is static, it's still necessary to stream it continuously for the benefit of participants joining the room after the initial frame is sent.
Note
Unlike audio, video
capture_frame
doesn't keep an internal buffer.
Recipes
The following recipes demonstrate audio and video features:
Playing Audio
Play audio files during agent interactions.
Sound Repeater
Simple sound repeating demo for testing audio pipelines.
Vision AI agent
A voice AI agent with live video input powered by Gemini.
Publishing background audio
Agents can publish background noise or sound effects. For example, adding the ambient noise of an office or contact center places the agent in a known setting and can feel more realistic.
The
BackgroundAudioPlayer
class manages audio playback to a room. You can add the following example to your entrypoint function:
Python
background_audio
=
BackgroundAudioPlayer
(
# play office ambience sound looping in the background
ambient_sound
=
AudioConfig
(
BuiltinAudioClip
.
OFFICE_AMBIENCE
,
volume
=
0.8
)
,
# play keyboard typing sound when the agent is thinking
thinking_sound
=
[
AudioConfig
(
BuiltinAudioClip
.
KEYBOARD_TYPING
,
volume
=
0.8
)
,
AudioConfig
(
BuiltinAudioClip
.
KEYBOARD_TYPING2
,
volume
=
0.7
)
,
]
,
)
await
background_audio
.
start
(
room
=
ctx
.
room
,
agent_session
=
session
)
# Play another audio file at any time using the play method:
# background_audio.play("filepath.ogg")
Copy
For a full example, see the
background audio example
.
Reducing background noise
Enhanced noise cancellation
is available in LiveKit Cloud and improves the quality of turn detection and speech-to-text (STT) for voice AI apps. You can add background noise and voice cancellation to your agent by adding it to the
room_input_options
when you start your agent session.
Install the noise cancellation plugin:
pip
install
livekit-plugins-noise-cancellation
Copy
Add background voice cancellation (BVC), which includes background noise cancellation, to your agent app:
Python
await
session
.
start
(
agent
=
MyAgent
(
)
,
room
=
ctx
.
room
,
room_input_options
=
RoomInputOptions
(
# Enable Krisp BVC noise cancellation
noise_cancellation
=
noise_cancellation
.
BVC
(
)
,
)
,
room_output_options
=
RoomOutputOptions
(
transcription_enabled
=
True
)
,
)
Copy
For a full example, uncomment the
noise_cancellation
line in the
basic agent example
.
Synchronizing audio and video
You can synchronize audio and video by using the
AudioSynchronizer
class. This class allows you to align the initial audio and video frames and keeps them synchronized. To learn more, see
Audio and video synchronization
.
On this page
Overview
Receiving tracks
Manually handling subscriptions
Working with video
Publishing
Publishing audio
Publishing video
Recipes
Publishing background audio
Reducing background noise
Synchronizing audio and video
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/text:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Transcriptions
Synchronized transcription forwarding
Text input
Text-only output
Usage examples
Frontend integration
Configuring input/output options
Manual text input
Custom topics
Transcription events
Overview
LiveKit Agents supports text inputs and outputs in addition to audio, based on the
text streams
feature of the LiveKit SDKs. This guide explains what's possible and how to use it in your app.
Transcriptions
When an agent performs STT as part of its processing pipeline, the transcriptions are also published to the frontend in realtime. Additionally, a text representation of the agent speech is also published in sync with audio playback when the agent speaks. These features are both enabled by default when using
AgentSession
.
Transcriptions use the
lk.transcription
text stream topic. They include a
lk.transcribed_track_id
attribute and the sender identity is the transcribed participant.
To disable transcription output, set
transcription_enabled=False
in
RoomOutputOptions
.
Synchronized transcription forwarding
When both voice and transcription are enabled, the agent's speech is synchronized with its transcriptions, displaying text word by word as it speaks. If the agent is interrupted, the transcription stops and is truncated to match the spoken output.
Text input
Your agent also monitors the
lk.chat
text stream topic for incoming text messages from its linked participant. The agent interrupts its current speech, if any, to process the message and generate a new response.
To disable text input, set
text_enabled=False
in
RoomInputOptions
.
Text-only output
To disable audio output entirely and send text only, set
audio_enabled=False
in
RoomOutputOptions
. The agent will publish text responses to the
lk.transcription
text stream topic, without a
lk.transcribed_track_id
attribute and without speech synchronization.
Usage examples
This section contains small code samples demonstrating how to use the text features.
For more information, see the
text streams
documentation. For more complete examples, see the
recipes
collection.
Frontend integration
Use the
registerTextStreamHandler
method to receive incoming transcriptions or text:
JavaScript
Swift
room
.
registerTextStreamHandler
(
'lk.transcription'
,
async
(
reader
,
participantInfo
)
=>
{
const
message
=
await
reader
.
readAll
(
)
;
if
(
reader
.
info
.
attributes
[
'lk.transcribed_track_id'
]
)
{
console
.
log
(
`
New transcription from
${
participantInfo
.
identity
}
:
${
message
}
`
)
;
}
else
{
console
.
log
(
`
New message from
${
participantInfo
.
identity
}
:
${
message
}
`
)
;
}
}
)
;
Copy
Use the
sendText
method to send text messages:
JavaScript
Swift
const
text
=
'Hello how are you today?'
;
const
info
=
await
room
.
localParticipant
.
sendText
(
text
,
{
topic
:
'lk.chat'
,
}
)
;
Copy
Configuring input/output options
The AgentSession constructor accepts configuration for input and output options:
session
=
AgentSession
(
.
.
.
,
# STT, LLM, etc.
room_input_options
=
RoomInputOptions
(
text_enabled
=
False
# disable text input
)
,
room_output_options
=
RoomOutputOptions
(
audio_enabled
=
False
# disable audio output
)
)
Copy
Manual text input
To insert text input and generate a response, use the
generate_reply
method of AgentSession:
session.generate_reply(user_input="...")
.
Custom topics
You may override the
text_input_topic
of
RoomInputOptions
and
transcription_output_topic
of
RoomOutputOptions
to set a custom text stream topic for text input or output, if desired. The default values are
lk.chat
and
lk.transcription
respectively.
Transcription events
Frontend SDKs can also receive transcription events via
RoomEvent.TranscriptionReceived
.
Deprecated feature
Transcription events will be removed in a future version. Use
text streams
on the
lk.chat
topic instead.
Android
Flutter
JavaScript
Swift
room
.
events
.
collect
{
event
->
if
(
event
is
RoomEvent
.
TranscriptionReceived
)
{
event
.
transcriptionSegments
.
forEach
{
segment
->
println
(
"New transcription from
${
segment
.
senderIdentity
}
:
${
segment
.
text
}
"
)
}
}
}
Copy
On this page
Overview
Transcriptions
Synchronized transcription forwarding
Text input
Text-only output
Usage examples
Frontend integration
Configuring input/output options
Manual text input
Custom topics
Transcription events
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/turns/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Turn detection
Turn detector model
Realtime models
VAD only
STT endpointing
Manual turn control
Interruptions
Session configuration
Turn-taking events
Further reading
Overview
Turn detection is the process of determining when a user begins or ends their "turn" in a conversation. This lets the agent know when to listening and when to respond.
Most turn detection techniques rely on voice activity detection (VAD) to detect periods of silence in user input. The agent applies heuristics to the VAD data to perform phrase endpointing, which determines the end of a sentence or thought. The agent can use endpoints alone or apply more contextual analysis to determine when a turn is complete.
Effective turn detection and interruption management is essential to great voice AI experiences.
Turn detection
The
AgentSession
supports the following turn detection modes, in addition to manual turn control that's always available.
Turn detector model
: A custom, open-weights model for context-aware turn detection on top of VAD or STT endpoint data.
Realtime models
: Support for the built-in turn detection or VAD in realtime models like the OpenAI Realtime API.
VAD only
: Detect end of turn from speech and silence data alone.
STT endpointing
: Use phrase endpoints returned in realtime STT data from your chosen STT provider in place of VAD.
Manual turn control
: Disable automatic turn detection entirely.
Turn detector model
To achieve the recommended behavior of an agent that listens while the user speaks and replies after they finish their thought, use the following plugins in an STT-LLM-TTS pipeline:
Turn detection model
Open-weights model for contextually-aware turn detection.
Silero VAD
Silero VAD model for voice activity detection.
from
livekit
.
plugins
.
turn_detector
.
multilingual
import
MultilingualModel
from
livekit
.
plugins
import
silero
session
=
AgentSession
(
turn_detection
=
MultilingualModel
(
)
,
# or EnglishModel()
vad
=
silero
.
VAD
.
load
(
)
,
# ... stt, tts, llm, etc.
)
Copy
See the
Voice AI quickstart
for a complete example.
Realtime model turn detection
For a realtime model, LiveKit recommends using the built-in turn detection capabilities of the
chosen model provider
. This is the most cost-effective option, since the custom turn detection model requires realtime speech-to-text (STT) that would need to run separately.
Realtime models
Realtime models include built-in turn detection options based on VAD and other techniques. Leave the
turn_detection
parameter unset and configure the realtime model's turn detection options directly.
To use the LiveKit turn detector model with a realtime model, you must also provide an STT plugin. The turn detector model operates on STT output.
OpenAI Realtime API turn detection
Turn detection options for the OpenAI Realtime API.
Gemini Live API turn detection
Turn detection options for the Gemini Live API.
VAD only
In some cases, VAD is the best option for turn detection. For example, VAD works with any spoken language. To use VAD alone, use the Silero VAD plugin and set
turn_detection="vad"
.
session
=
AgentSession
(
turn_detection
=
"vad"
,
vad
=
silero
.
VAD
.
load
(
)
,
# ... stt, tts, llm, etc.
)
Copy
STT endpointing
You can also use your STT model for turn detection as they process audio and perform phrase endpointing to construct speech fragments. In this mode, the
AgentSession
treats the final STT transcript as a turn boundary.
Note that STT endpointing is less responsive to interruptions than VAD.
session
=
AgentSession
(
turn_detection
=
"stt"
,
stt
=
deepgram
.
STT
(
)
,
# ... tts, llm, etc.
)
Copy
Manual turn control
Disable automatic turn detection entirely by setting
turn_detection="manual"
in the
AgentSession
constructor.
You can now control the user's turn with
session.interrupt()
,
session.clear_user_turn()
, and
session.commit_user_turn()
methods.
For instance, you can use this to implement a push-to-talk interface. Here is a simple example using
RPC
methods that the frontend can call:
session
=
AgentSession
(
turn_detection
=
"manual"
,
# ... stt, tts, llm, etc.
)
# Disable audio input at the start
session
.
input
.
set_audio_enabled
(
False
)
# When user starts speaking
@ctx
.
room
.
local_participant
.
register_rpc_method
(
"start_turn"
)
async
def
start_turn
(
data
:
rtc
.
RpcInvocationData
)
:
session
.
interrupt
(
)
# Stop any current agent speech
session
.
clear_user_turn
(
)
# Clear any previous input
session
.
input
.
set_audio_enabled
(
True
)
# Start listening
# When user finishes speaking
@ctx
.
room
.
local_participant
.
register_rpc_method
(
"end_turn"
)
async
def
end_turn
(
data
:
rtc
.
RpcInvocationData
)
:
session
.
input
.
set_audio_enabled
(
False
)
# Stop listening
session
.
commit_user_turn
(
)
# Process the input and generate response
# When user cancels their turn
@ctx
.
room
.
local_participant
.
register_rpc_method
(
"cancel_turn"
)
async
def
cancel_turn
(
data
:
rtc
.
RpcInvocationData
)
:
session
.
input
.
set_audio_enabled
(
False
)
# Stop listening
session
.
clear_user_turn
(
)
# Discard the input
Copy
A more complete example is available here:
Push-to-Talk Agent
A voice AI agent that uses push-to-talk for controlled multi-participant conversations, only enabling audio input when explicitly triggered.
Interruptions
The user can interrupt the agent at any time, either by speaking with automatic turn detection or via the
session.interrupt()
method. When an interruption occurs, the agent stops speaking and automatically truncates its conversation history to reflect only the speech that the user actually heard before interruption.
Session configuration
The following parameters related to turn detection and interruptions are available on the
AgentSession
constructor:
allow_interruptions
bool
Optional
Default:
True
#
Whether to allow the user to interrupt the agent mid-turn. Ignored when using a realtime model with built-in turn detection.
min_interruption_duration
float
Optional
Default:
0.5
#
Minimum detected speech duration before triggering an interruption.
min_endpointing_delay
float
Optional
Default:
0.5
#
The number of seconds to wait before considering the turn complete. The session uses this delay when no turn detector model is present, or when the model indicates a likely turn boundary.
max_endpointing_delay
float
Optional
Default:
6.0
#
The maximum time to wait for the user to speak after the turn detector model indicates the user is likely to continue speaking. This parameter has no effect without the turn detector model.
Turn-taking events
The
AgentSession
exposes user and agent state events to monitor the flow of a conversation:
from
livekit
.
agents
import
UserStateChangedEvent
,
AgentStateChangedEvent
@session
.
on
(
"user_state_changed"
)
def
on_user_state_changed
(
ev
:
UserStateChangedEvent
)
:
if
ev
.
new_state
==
"speaking"
:
print
(
"User started speaking"
)
elif
ev
.
new_state
==
"listening"
:
print
(
"User stopped speaking"
)
elif
ev
.
new_state
==
"away"
:
print
(
"User is not present (e.g. disconnected)"
)
@session
.
on
(
"agent_state_changed"
)
def
on_agent_state_changed
(
ev
:
AgentStateChangedEvent
)
:
if
ev
.
new_state
==
"initializing"
:
print
(
"Agent is starting up"
)
elif
ev
.
new_state
==
"idle"
:
print
(
"Agent is ready but not processing"
)
elif
ev
.
new_state
==
"listening"
:
print
(
"Agent is listening for user input"
)
elif
ev
.
new_state
==
"thinking"
:
print
(
"Agent is processing user input and generating a response"
)
elif
ev
.
new_state
==
"speaking"
:
print
(
"Agent started speaking"
)
Copy
Further reading
Agent speech
Guide to agent speech and related methods.
Pipeline nodes
Monitor input and output as it flows through the voice pipeline.
On this page
Overview
Turn detection
Turn detector model
Realtime models
VAD only
STT endpointing
Manual turn control
Interruptions
Session configuration
Turn-taking events
Further reading
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/metrics:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Logging events
Aggregating metrics
Metrics reference
Speech-to-text (STT)
LLM
Text-to-speech (TTS)
End-of-utterance (EOU)
Measuring conversation latency
Overview
To improve observability into agent performance and model usage, you can log detailed metrics provided by LiveKit Agents. These metrics offer insights into duration, latency, and usage across different stages of a session.
Logging events
Agent metrics events are fired by the
AgentSession
whenever there is a new metrics object available during an active session.
A
log_metrics
helper function is also provided to format logging output for each metric type.
from
livekit
.
agents
import
metrics
,
MetricsCollectedEvent
.
.
.
@session
.
on
(
"metrics_collected"
)
def
_on_metrics_collected
(
ev
:
MetricsCollectedEvent
)
:
metrics
.
log_metrics
(
ev
.
metrics
)
Copy
Aggregating metrics
The
metrics
module also includes a
UsageCollector
helper class for aggregating usage metrics across a session. It tracks metrics such as LLM, TTS, and STT API usage, which can help estimate session cost.
from
livekit
.
agents
import
metrics
,
MetricsCollectedEvent
.
.
.
usage_collector
=
metrics
.
UsageCollector
(
)
@session
.
on
(
"metrics_collected"
)
def
_on_metrics_collected
(
ev
:
MetricsCollectedEvent
)
:
usage_collector
.
collect
(
ev
.
metrics
)
async
def
log_usage
(
)
:
summary
=
usage_collector
.
get_summary
(
)
logger
.
info
(
f"Usage:
{
summary
}
"
)
# At shutdown, generate and log the summary from the usage collector
ctx
.
add_shutdown_callback
(
log_usage
)
Copy
Metrics reference
Speech-to-text (STT)
STTMetrics
is emitted after the STT model has processed the audio input. This metrics is only available when a STT component is used, which does not apply to Realtime APIs.
Metric
Description
audio_duration
The duration (seconds) of the audio input received by the STT model.
duration
For non-streaming STT, the amount of time (seconds) it took to create the transcript. Always
0
for streaming STT.
streamed
True
if the STT is in streaming mode.
LLM
LLMMetrics
is emitted after each LLM inference completes. If the response includes tool calls, the event does not include the time taken to execute those calls. Each tool call response triggers a separate
LLMMetrics
event.
Metric
Description
duration
The amount of time (seconds) it took for the LLM to generate the entire completion.
completion_tokens
The number of tokens generated by the LLM in the completion.
prompt_tokens
The number of tokens provided in the prompt sent to the LLM.
speech_id
An unique identifier representing a turn in the user input.
total_tokens
Total token usage for the completion.
tokens_per_second
The rate of token generation (tokens/second) by the LLM to generate the completion.
ttft
The amount of time (seconds) that it took for the LLM to generate the first token of the completion.
Text-to-speech (TTS)
TTSMetrics
is emitted after a TTS has generated speech from text input.
Metric
Description
audio_duration
The duration (seconds) of the audio output generated by the TTS model.
characters_count
The number of characters in the text input to the TTS model.
duration
The amount of time (seconds) it took for the TTS model to generate the entire audio output.
ttfb
The amount of time (seconds) that it took for the TTS model to generate the first byte of its audio output.
speech_id
An identifier linking to a user's turn.
streamed
True
if the TTS is in streaming mode.
End-of-utterance (EOU)
EOUMetrics
is emitted when the user is determined to have finished speaking. It includes metrics related to end-of-turn detection and transcription latency.
This event is only available in Realtime APIs when
turn_detection
is set to either VAD or LiveKit's turn detector plugin. When using server-side turn detection, EOUMetrics is not emitted, as this information is not available.
Metric
Description
end_of_utterance_delay
Time (in seconds) from the end of speech (as detected by VAD) to the point when the user's turn is considered complete. This includes any
transcription_delay
.
transcription_delay
Time (seconds) between the end of speech and when final transcript is available
on_user_turn_completed_delay
Time (in seconds) taken to execute the
on_user_turn_completed
callback.
speech_id
A unique identifier indicating the user's turn.
Measuring conversation latency
Total conversation latency is defined as the time it takes for the agent to respond to a user's utterance. Given the metrics above, it can be computed as follows:
total_latency
=
eou
.
end_of_utterance_delay
+
llm
.
ttft
+
tts
.
ttfb
Copy
On this page
Overview
Logging events
Aggregating metrics
Metrics reference
Speech-to-text (STT)
LLM
Text-to-speech (TTS)
End-of-utterance (EOU)
Measuring conversation latency
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/integrating-existing-backends:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
Article coming soon
While docs are still under development, the best place to get the most current reference is on
GitHub
.
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/rag:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Recipes
Article coming soon
While docs are still under development, the best place to get the most current reference is on
GitHub
.
Recipes
The following recipes demonstrate RAG implementations:
LlamaIndex RAG
A voice AI agent that uses LlamaIndex for RAG to answer questions from a knowledge base.
RAG
An agent that can answer questions using RAG.
On this page
Recipes
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/latency:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
Article coming soon
While docs are still under development, the best place to get the most current reference is on
GitHub
.
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/worker/agent-dispatch/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Dispatching agents
Automatic agent dispatch
Explicit agent dispatch
Dispatch via API
Dispatch from inbound SIP calls
Dispatch on participant connection
Dispatching agents
Dispatch is the process of assigning an agent to a room. LiveKit server manages this process as part of the
worker lifecycle
. LiveKit optimizes dispatch for high concurrency and low latency, typically supporting hundred of thousands of new connections per second with a max dispatch time under 150 ms.
Automatic agent dispatch
By default, an agent is automatically dispatched to each new room. Automatic dispatch is the best option if you want to assign the same agent to all new participants.
Explicit agent dispatch
Explicit dispatch is available for greater control over when and how agents join rooms. This approach leverages the same worker systems, allowing you to run agent workers in the same way.
To use explicit dispatch, set the
agent_name
field in the
WorkerOptions
:
Python
Node.js
opts
=
WorkerOptions
(
.
.
.
agent_name
=
"test-agent"
,
)
Copy
Important
Automatic dispatch is disabled if the
agent_name
property is set.
Dispatch via API
Agent workers with the
agent_name
set can be explicitly dispatched to a room via
AgentDispatchService
.
Python
Node.js
LiveKit CLI
Go
import
asyncio
from
livekit
import
api
room_name
=
"my-room"
agent_name
=
"test-agent"
async
def
create_explicit_dispatch
(
)
:
lkapi
=
api
.
LiveKitAPI
(
)
dispatch
=
await
lkapi
.
agent_dispatch
.
create_dispatch
(
api
.
CreateAgentDispatchRequest
(
agent_name
=
agent_name
,
room
=
room_name
,
metadata
=
"my_job_metadata"
)
)
print
(
"created dispatch"
,
dispatch
)
dispatches
=
await
lkapi
.
agent_dispatch
.
list_dispatch
(
room_name
=
room_name
)
print
(
f"there are
{
len
(
dispatches
)
}
dispatches in
{
room_name
}
"
)
await
lkapi
.
aclose
(
)
asyncio
.
run
(
create_explicit_dispatch
(
)
)
Copy
The room,
my-room
, is automatically created during dispatch if it doesn't already exist, and the worker assigns
test-agent
to it.
Handling job metadata
The
job.metadata
passed to the agent includes the
metadata
set in the dispatch request. The following example logs the metadata:
Python
Node.js
async
def
entrypoint
(
ctx
:
JobContext
)
:
logger
.
info
(
f"job metadata:
{
ctx
.
job
.
metadata
}
"
)
.
.
.
Copy
Dispatch from inbound SIP calls
Agents can be explicitly dispatched for inbound SIP calls.
SIP dispatch rules
can define one or more agents using the
room_config.agents
field.
LiveKit recommends explicit agent dispatch for SIP inbound calls rather than automatic agent dispatch as it allows multiple agents within a single project.
Dispatch on participant connection
You can configure a participant's token to dispatch one or more agents immediately upon connection.
To dispatch multiple agents, include multiple
RoomAgentDispatch
entries in
RoomConfiguration
.
The following example creates a token that dispatches the
test-agent
agent to the
my-room
room when the participant connects:
Python
Node.js
Go
from
livekit
.
api
import
(
AccessToken
,
RoomAgentDispatch
,
RoomConfiguration
,
VideoGrants
,
)
room_name
=
"my-room"
agent_name
=
"test-agent"
def
create_token_with_agent_dispatch
(
)
-
>
str
:
token
=
(
AccessToken
(
)
.
with_identity
(
"my_participant"
)
.
with_grants
(
VideoGrants
(
room_join
=
True
,
room
=
room_name
)
)
.
with_room_config
(
RoomConfiguration
(
agents
=
[
RoomAgentDispatch
(
agent_name
=
"test-agent"
,
metadata
=
"my_metadata"
)
]
,
)
,
)
.
to_jwt
(
)
)
return
token
Copy
On this page
Dispatching agents
Automatic agent dispatch
Explicit agent dispatch
Dispatch via API
Dispatch from inbound SIP calls
Dispatch on participant connection
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/worker/job/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Lifecycle
Entrypoint
Customizing for participant
Ending the session
Disconnecting the agent
Disconnecting everyone
Post-processing and cleanup
Lifecycle
After a
worker
accepts the job request from LiveKit server, it starts a new process to run your entrypoint function with the context of that specific job. Each job runs in a separate process to isolate agents from each other. If a session instance crashes, it doesn't affect other agents running on the same worker.
Entrypoint
The entrypoint function is executed as the main function of the process for each new job run by the worker.
You have full control over the job in the entrypoint function. The job runs until all participants leave the room, or  the job is explicitly shut down.
Python
async
def
do_something
(
track
:
rtc
.
RemoteAudioTrack
)
:
audio_stream
=
rtc
.
AudioStream
(
track
)
async
for
event
in
audio_stream
:
# Do something here to process event.frame
pass
await
audio_stream
.
aclose
(
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
# an rtc.Room instance from the LiveKit Python SDK
room
=
ctx
.
room
# set up listeners on the room before connecting
@room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
*
_
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_AUDIO
:
asyncio
.
create_task
(
do_something
(
track
)
)
# connect to room
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
# when connected, room.local_participant represents the agent
await
room
.
local_participant
.
publish_data
(
"hello world"
)
# iterate through currently connected remote participants
for
rp
in
room
.
remote_participants
.
values
(
)
:
print
(
rp
.
identity
)
Copy
For more LiveKit Agents examples, see the
GitHub repository
. To learn more about publishing and receiving tracks, see the following topics:
Media tracks
Use the microphone, speaker, cameras, and screenshare with your agent.
Realtime text and data
Use text and data channels to communicate with your agent.
Customizing for participant
You can customize agent behavior based on the connected participant, enabling a personalized experience.
LiveKit provides several ways to identify participants:
ctx.room.name
: the name of the room that the participant is connected to.
participant.identity
: the identity of the participant.
participant.attributes
and
participant.metadata
:
custom attributes
set on the participant.
Here's an example:
Python
async
def
entrypoint
(
ctx
:
JobContext
)
:
# connect to the room
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
# wait for the first participant to arrive
participant
=
await
ctx
.
wait_for_participant
(
)
# customize behavior based on the participant
print
(
f"connected to room
{
ctx
.
room
.
name
}
with participant
{
participant
.
identity
}
"
)
# inspect the current value of the attribute
language
=
participant
.
attributes
.
get
(
"user.language"
)
# listen to when the attribute is changed
@ctx
.
room
.
on
(
"participant_attributes_changed"
)
def
on_participant_attributes_changed
(
changed_attrs
:
dict
[
str
,
str
]
,
p
:
rtc
.
Participant
)
:
if
p
==
participant
:
language
=
p
.
attributes
.
get
(
"user.language"
)
print
(
f"participant
{
p
.
identity
}
changed language to
{
language
}
"
)
Copy
Ending the session
Disconnecting the agent
You can disconnect an agent after it completes its task and is no longer needed in the room. This allows the other participants in the LiveKit session to continue. Your
shutdown hooks
run after the
shutdown
function.
Python
async
def
entrypoint
(
ctx
:
JobContext
)
:
# do some work
.
.
.
# disconnect from the room
ctx
.
shutdown
(
reason
=
"Session ended"
)
Copy
Disconnecting everyone
If the session should end for everyone, use the server API
deleteRoom
to end the session.
The
Disconnected
room event
will be sent, and the room will be removed from the server.
Python
from
livekit
import
api
async
def
entrypoint
(
ctx
:
JobContext
)
:
# do some work
.
.
.
api_client
=
api
.
LiveKitAPI
(
os
.
getenv
(
"LIVEKIT_URL"
)
,
os
.
getenv
(
"LIVEKIT_API_KEY"
)
,
os
.
getenv
(
"LIVEKIT_API_SECRET"
)
,
)
await
api_client
.
room
.
delete_room
(
api
.
DeleteRoomRequest
(
room
=
ctx
.
job
.
room
.
name
,
)
)
Copy
Post-processing and cleanup
After a session ends, you can perform post-processing or cleanup tasks using shutdown hooks. For example, you might want to save user state in a database.
Python
async
def
entrypoint
(
ctx
:
JobContext
)
:
async
def
my_shutdown_hook
(
)
:
# save user state
.
.
.
ctx
.
add_shutdown_callback
(
my_shutdown_hook
)
Copy
Note
Shutdown hooks should complete within a short amount of time. By default, the framework waits 60 seconds before forcefully terminating the process. You can adjust this timeout using the
shutdown_process_timeout
parameter in
WorkerOptions
.
On this page
Lifecycle
Entrypoint
Customizing for participant
Ending the session
Disconnecting the agent
Disconnecting everyone
Post-processing and cleanup
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/worker/options/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
WorkerOptions parameters
Entrypoint
Request handler
Prewarm function
Permissions
Worker type
Starting the worker
WorkerOptions parameters
The interface for creating a worker is through the
WorkerOptions
class. The following only
includes some of the available parameters. For the complete list, see the
WorkerOptions reference
.
Python
opts
=
WorkerOptions
(
# entrypoint function is called when a job is assigned to this worker
# this is the only required parameter to WorkerOptions
entrypoint_fnc
,
# inspect the request and decide if the current worker should handle it.
request_fnc
,
# a function to perform any necessary initialization in a new process.
prewarm_fnc
,
# a function that reports the current system load, whether CPU or RAM, etc.
load_fnc
,
# the maximum value of load_fnc, above which new processes will not spawn
load_threshold
,
# whether the agent can subscribe to tracks, publish data, update metadata, etc.
permissions
,
# the type of worker to create, either JT_ROOM or JT_PUBLISHER
worker_type
=
WorkerType
.
ROOM
,
)
# start the worker
cli
.
run_app
(
opts
)
Copy
Caution
For security purposes, set the LiveKit API key and secret as environment variables rather than as
WorkerOptions
parameters.
Entrypoint
This is the main function called when LiveKit server assigns the worker a new job. It's the entrypoint for your agent.
logic. The entrypoint function runs
before
the agent joins the room, and is where you can set up any necessary state or configuration. To learn more about the entrypoint function, see the
Job lifecycle
topic.
Python
async
def
entrypoint
(
ctx
:
JobContext
)
:
# connect to the room
await
ctx
.
connect
(
)
# handle the session
.
.
.
Copy
Request handler
The
request_fnc
function is executed each time that the server has a job for the agent. The framework expects workers to explicitly accept or reject each job request. If you accept the request, your entrypoint function is called. If the request is rejected, it's sent to the next available worker.
By default, if left blank, the behavior is to auto-accept all requests dispatched to the worker.
Python
async
def
request_fnc
(
req
:
JobRequest
)
:
# accept the job request
await
req
.
accept
(
# the agent's name (Participant.name), defaults to ""
name
=
"agent"
,
# the agent's identity (Participant.identity), defaults to "agent-<jobid>"
identity
=
"identity"
,
# attributes to set on the agent participant upon join
attributes
=
{
"myagent"
:
"rocks"
}
,
)
# or reject it
# await req.reject()
opts
=
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
request_fnc
=
request_fnc
)
Copy
Prewarm function
For isolation and performance reasons, the framework runs each agent job in its own process. Agents often need access to model files that take time to load. To address this, you can use a
prewarm
function to warm up the process before assigning any jobs to it. You can control the number of processes to keep warm using the
num_idle_processes
parameter.
Python
def
prewarm_fnc
(
proc
:
JobProcess
)
:
# load silero weights and store to process userdata
proc
.
userdata
[
"vad"
]
=
silero
.
VAD
.
load
(
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
# access the loaded silero instance
vad
:
silero
.
VAD
=
ctx
.
proc
.
userdata
[
"vad"
]
opts
=
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
prewarm_fnc
=
prewarm_fnc
)
Copy
Permissions
By default, agents can both publish to and subscribe from the other participants in the same room. However, you can customize these permissions by setting the
permissions
parameter in
WorkerOptions
. To see the full list of parameters, see the
WorkerPermissions reference
.
Python
opts
=
WorkerOptions
(
.
.
.
permissions
=
WorkerPermissions
(
can_publish
=
True
,
can_subscribe
=
True
,
can_publish_data
=
True
,
# when set to true, the agent won't be visible to others in the room.
# when hidden, it will also not be able to publish tracks to the room as it won't be visible.
hidden
=
False
,
)
,
)
Copy
Worker type
You can choose to start a new instance of the agent for each room or for each publisher in the room. This can be set when you register your worker:
Python
Node.js
opts
=
WorkerOptions
(
.
.
.
# when omitted, the default is WorkerType.ROOM
worker_type
=
WorkerType
.
ROOM
,
)
Copy
The
WorkerType
enum has two options:
ROOM
: Create a new instance of the agent for each room.
PUBLISHER
: Create a new instance of the agent for each publisher in the room.
If the agent is performing resource-intensive operations in a room that could potentially include multiple publishers (for example, processing incoming video from a set of security cameras), you can set
worker_type
to
JT_PUBLISHER
to ensure that each publisher has its own instance of the agent.
For
PUBLISHER
jobs, call the
entrypoint
function once for each publisher in the room. The
JobContext.publisher
object contains a
RemoteParticipant
representing that publisher.
Starting the worker
To spin up a worker with the configuration defined using
WorkerOptions
, call the CLI:
Python
Node.js
if
__name__
==
"__main__"
:
cli
.
run_app
(
opts
)
Copy
The Agents worker CLI provides two subcommands:
start
and
dev
. The former outputs raw JSON data to stdout, and is recommended for production.
dev
is recommended to use for development, as it outputs human-friendly colored logs, and supports hot reloading on Python.
On this page
WorkerOptions parameters
Entrypoint
Request handler
Prewarm function
Permissions
Worker type
Starting the worker
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/ops/deployment:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Where to deploy
Networking
Environment variables
Storage
Memory and CPU
Rollout
Load balancing
Worker availability
Autoscaling
Overview
LiveKit Agents use a worker pool model suited to a container orchestration system like Kubernetes. Each worker — an instance of
python main.py start
— registers with LiveKit server. LiveKit server balances job dispatch across available workers. The workers themselves spawn a new sub-process for each job, and that job is where your code and agent participant run.
Deploying to production generally requires a simple
Dockerfile
that ends in
CMD ["python", "main.py", "start"]
and a deployment platform that scales your worker pool based on load.
Where to deploy
LiveKit Agents can be deployed anywhere. The recommended approach is to use
Docker
and deploy to an orchestration service. The LiveKit team and community have found the following deployment platforms to be the easiest to deploy and autoscale workers.
New
LiveKit Cloud Agents Beta
Run your agent on the same network and infrastructure that serves LiveKit Cloud, with builds, deployment, and scaling handled for you. Sign up for the public beta to get started.
Kubernetes
Sample configuration for deploying and autoscaling LiveKit Agents on Kubernetes.
Render.com
Sample configuration for deploying and autoscaling LiveKit Agents on Render.com.
More deployment examples
Example
Dockerfile
and configuration files for a variety of deployment platforms.
Networking
Workers use a WebSocket connection to register with LiveKit server and accept incoming jobs. This means that workers do not need to expose any inbound hosts or ports to the public internet.
You may optionally expose a private health check endpoint for monitoring, but this is not required for normal operation.
The default health check server listens on
http://0.0.0.0:8081/
.
Environment variables
It is best to configure your worker with environment variables for secrets like API keys. In addition to the LiveKit variables, you are likely to need additional keys for external services your agent depends on.
For instance, an agent built with the
Voice AI quickstart
needs the following keys at a minimum:
.env
DEEPGRAM_API_KEY
=
<
Your Deepgram API Key
>
OPENAI_API_KEY
=
<
Your OpenAI API Key
>
CARTESIA_API_KEY
=
<
Your Cartesia API Key
>
LIVEKIT_API_KEY
=
<
your API Key
>
LIVEKIT_API_SECRET
=
<
your API Secret
>
LIVEKIT_URL
=
<
your LiveKit server URL
>
Reveal API Key and Secret
Copy
Project environments
It's recommended to use a separate LiveKit instance for staging, production, and development environments. This ensures you can continue working on your agent locally without accidentally processing real user traffic.
In LiveKit Cloud, make a separate project for each environment. Each has a unique URL, API key, and secret.
For self-hosted LiveKit server, use a separate deployment for staging and production and a local server for development.
Storage
Worker and job processes have no particular storage requirements beyond the size of the Docker image itself (typically <1GB). 10GB of ephemeral storage should be more than enough to account for this and any temporary storage needs your app has.
Memory and CPU
Memory and CPU requirements vary significantly based on the specific details of your app. For instance, agents that apply
enhanced noise cancellation
require more CPU and memory than those that don't.
LiveKit recommends 4 cores and 8GB of memory for every 25 concurrent sessions as a starting rule for most voice-to-voice apps.
Real world load test results
LiveKit ran a load test to evaluate the memory and CPU requirements of a typical voice-to-voice app.
30 agents each placed in their own LiveKit Cloud room.
30 simulated user participants, one in each room.
Each simulated participant published looping speech audio to the agents.
Each agent subscribed to the incoming audio of the user and ran the Silero VAD plugin.
Each agent published their own audio (simple looping sine wave).
One additional user participant with a corresponding voice AI agent to ensure subjective quality of service.
This test ran all agents on a single 4-Core, 8GB machine. This machine reached peak usage of:
CPU: ~3.8 cores utilized
Memory: ~2.8GB used
Rollout
Workers stop accepting jobs upon
SIGINT
or
SIGTERM
. Any job still running on the worker continues to run to completion. It's important that you configure a large enough grace period such that your jobs can finish without interrupting the user experience.
Voice AI apps might require a 10+ minute grace period to allow for conversations to finish.
Different deployment platforms have different ways of setting this grace period.
In Kubernetes, it's the
terminationGracePeriodSeconds
field in the pod spec.
Consult your deployment platform's documentation for more information.
Load balancing
LiveKit server includes a built-in balanced job distribution system. This system peforms round-robin distribution with a single-assignment principle that ensures each job is assigned to only one worker. If a worker fails to accept the job within a predetermined timeout period, the job is sent to another available worker instead.
LiveKit Cloud additionally exercises geographic affinity to prioritize matching users and workers that are geographically closest to each other. This ensures the lowest possible latency between users and agents.
Worker availability
Worker availability is defined by the
load_fnc
and
load_threshold
parameters in the
WorkerOptions
configuration.
The
load_fnc
must return a value between 0 and 1, indicating how busy the worker is.
load_threshold
is the load value above which the worker stops accepting new jobs.
The default
load_fnc
is overall CPU utilization, and the default
load_threshold
is
0.75
.
Autoscaling
To handle variable traffic patterns, add an autoscaling strategy to your deployment platform. Your autoscaler should use the same underlying metrics as your
load_fnc
(the default is CPU utilization) but should scale up at a
lower
threshold than your worker's
load_threshold
. This ensures continuity of service by adding new workers before existing ones go out of service. For example, if your
load_threshold
is
0.75
, you should scale up at
0.50
.
Since voice agents are typically long running tasks (relative to typical web requests), rapid increases in load are more likely to be sustained. In technical terms: spikes are less spikey. For your autoscaling configuration, you should consider
reducing
cooldown/stabilization periods when scaling up. When scaling down, consider
increasing
cooldown/stabilization periods because workers take time to drain.
For example, if deploying on Kubernetes using a Horizontal Pod Autoscaler,
see
stabilizationWindowSeconds
.
On this page
Overview
Where to deploy
Networking
Environment variables
Storage
Memory and CPU
Rollout
Load balancing
Worker availability
Autoscaling
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/ops/logging:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Logging events
Aggregating metrics
Metrics reference
Speech-to-text (STT)
LLM
Text-to-speech (TTS)
End-of-utterance (EOU)
Measuring conversation latency
Overview
To improve observability into agent performance and model usage, you can log detailed metrics provided by LiveKit Agents. These metrics offer insights into duration, latency, and usage across different stages of a session.
Logging events
Agent metrics events are fired by the
AgentSession
whenever there is a new metrics object available during an active session.
A
log_metrics
helper function is also provided to format logging output for each metric type.
from
livekit
.
agents
import
metrics
,
MetricsCollectedEvent
.
.
.
@session
.
on
(
"metrics_collected"
)
def
_on_metrics_collected
(
ev
:
MetricsCollectedEvent
)
:
metrics
.
log_metrics
(
ev
.
metrics
)
Copy
Aggregating metrics
The
metrics
module also includes a
UsageCollector
helper class for aggregating usage metrics across a session. It tracks metrics such as LLM, TTS, and STT API usage, which can help estimate session cost.
from
livekit
.
agents
import
metrics
,
MetricsCollectedEvent
.
.
.
usage_collector
=
metrics
.
UsageCollector
(
)
@session
.
on
(
"metrics_collected"
)
def
_on_metrics_collected
(
ev
:
MetricsCollectedEvent
)
:
usage_collector
.
collect
(
ev
.
metrics
)
async
def
log_usage
(
)
:
summary
=
usage_collector
.
get_summary
(
)
logger
.
info
(
f"Usage:
{
summary
}
"
)
# At shutdown, generate and log the summary from the usage collector
ctx
.
add_shutdown_callback
(
log_usage
)
Copy
Metrics reference
Speech-to-text (STT)
STTMetrics
is emitted after the STT model has processed the audio input. This metrics is only available when a STT component is used, which does not apply to Realtime APIs.
Metric
Description
audio_duration
The duration (seconds) of the audio input received by the STT model.
duration
For non-streaming STT, the amount of time (seconds) it took to create the transcript. Always
0
for streaming STT.
streamed
True
if the STT is in streaming mode.
LLM
LLMMetrics
is emitted after each LLM inference completes. If the response includes tool calls, the event does not include the time taken to execute those calls. Each tool call response triggers a separate
LLMMetrics
event.
Metric
Description
duration
The amount of time (seconds) it took for the LLM to generate the entire completion.
completion_tokens
The number of tokens generated by the LLM in the completion.
prompt_tokens
The number of tokens provided in the prompt sent to the LLM.
speech_id
An unique identifier representing a turn in the user input.
total_tokens
Total token usage for the completion.
tokens_per_second
The rate of token generation (tokens/second) by the LLM to generate the completion.
ttft
The amount of time (seconds) that it took for the LLM to generate the first token of the completion.
Text-to-speech (TTS)
TTSMetrics
is emitted after a TTS has generated speech from text input.
Metric
Description
audio_duration
The duration (seconds) of the audio output generated by the TTS model.
characters_count
The number of characters in the text input to the TTS model.
duration
The amount of time (seconds) it took for the TTS model to generate the entire audio output.
ttfb
The amount of time (seconds) that it took for the TTS model to generate the first byte of its audio output.
speech_id
An identifier linking to a user's turn.
streamed
True
if the TTS is in streaming mode.
End-of-utterance (EOU)
EOUMetrics
is emitted when the user is determined to have finished speaking. It includes metrics related to end-of-turn detection and transcription latency.
This event is only available in Realtime APIs when
turn_detection
is set to either VAD or LiveKit's turn detector plugin. When using server-side turn detection, EOUMetrics is not emitted, as this information is not available.
Metric
Description
end_of_utterance_delay
Time (in seconds) from the end of speech (as detected by VAD) to the point when the user's turn is considered complete. This includes any
transcription_delay
.
transcription_delay
Time (seconds) between the end of speech and when final transcript is available
on_user_turn_completed_delay
Time (in seconds) taken to execute the
on_user_turn_completed
callback.
speech_id
A unique identifier indicating the user's turn.
Measuring conversation latency
Total conversation latency is defined as the time it takes for the agent to respond to a user's utterance. Given the metrics above, it can be computed as follows:
total_latency
=
eou
.
end_of_utterance_delay
+
llm
.
ttft
+
tts
.
ttfb
Copy
On this page
Overview
Logging events
Aggregating metrics
Metrics reference
Speech-to-text (STT)
LLM
Text-to-speech (TTS)
End-of-utterance (EOU)
Measuring conversation latency
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/ops/recording:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Video or audio recording
Example
Text transcripts
Example
Overview
There are many reasons to record or persist the sessions that occur in your app, from quality monitoring to regulatory compliance. LiveKit allows you to record the video and audio from agent sessions or save the text transcripts.
Video or audio recording
Use the
Egress feature
to record audio and/or video. The simplest way to do this is to start a
room composite recorder
in your agent's entrypoint. This starts recording when the agent enters the room and automatically captures all audio and video shared in the room. Recording ends when all participants leave. Recordings are stored in the cloud storage provider of your choice.
Example
This example shows how to modify the
Voice AI quickstart
to record sessions. It uses Google Cloud Storage, but you can also save files to any Amazon S3-compatible storage provider or Azure Blob Storage.
For additional egress examples using Amazon S3 and Azure, see the
Egress examples
. To learn more about
credentials.json
, see
Cloud storage configurations
.
To modify the
Voice AI quickstart
to record sessions, add the following code:
main.py
from
livekit
import
api
async
def
entrypoint
(
ctx
:
JobContext
)
:
# Add the following code to the top, before calling ctx.connect()
# Load GCP credentials from credentials.json file.
file_contents
=
""
with
open
(
"/path/to/credentials.json"
,
"r"
)
as
f
:
file_contents
=
f
.
read
(
)
# Set up recording
req
=
api
.
RoomCompositeEgressRequest
(
room_name
=
"my-room"
,
layout
=
"speaker"
,
audio_only
=
True
,
segment_outputs
=
[
api
.
SegmentedFileOutput
(
filename_prefix
=
"my-output"
,
playlist_name
=
"my-playlist.m3u8"
,
live_playlist_name
=
"my-live-playlist.m3u8"
,
segment_duration
=
5
,
gcp
=
api
.
GCPUpload
(
credentials
=
file_contents
,
bucket
=
"<your-gcp-bucket>"
,
)
,
)
]
,
)
res
=
await
ctx
.
api
.
egress
.
start_room_composite_egress
(
req
)
# .. The rest of your entrypoint code follows ...
Copy
Text transcripts
Text transcripts are available in realtime via the
llm_node
or the
transcription_node
as detailed in the docs on
Pipeline nodes
. You can use this along with other events and callbacks to record your session and any other data you need.
Additionally, you can access the
session.history
property at any time to get the full conversation history so far. Using the
add_shutdown_callback
method, you can save the conversation history to a file after the user leaves and the room closes.
Example
This example shows how to modify the
Voice AI quickstart
to save the conversation history to a JSON file.
main.py
from
datetime
import
datetime
import
json
def
entrypoint
(
ctx
:
JobContext
)
:
# Add the following code to the top, before calling ctx.connect()
async
def
write_transcript
(
)
:
current_date
=
datetime
.
now
(
)
.
strftime
(
"%Y%m%d_%H%M%S"
)
# This example writes to the temporary directory, but you can save to any location
filename
=
f"/tmp/transcript_
{
ctx
.
room
.
name
}
_
{
current_date
}
.json"
with
open
(
filename
,
'w'
)
as
f
:
json
.
dump
(
session
.
history
.
to_dict
(
)
,
f
,
indent
=
2
)
print
(
f"Transcript for
{
ctx
.
room
.
name
}
saved to
{
filename
}
"
)
ctx
.
add_shutdown_callback
(
write_transcript
)
# .. The rest of your entrypoint code follows ...
Copy
On this page
Overview
Video or audio recording
Example
Text transcripts
Example
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/ops/cost:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
Article coming soon
While docs are still under development, the best place to get the most current reference is on
GitHub
.
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/openai:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
OpenAI ecosystem support
Getting started
LiveKit Agents overview
Realtime API
OpenAI plugin documentation
Try LiveKit.fm
Chat with OpenAI's latest models in a LiveKit demo inspired by OpenAI.fm
OpenAI ecosystem support
OpenAI
provides some of the most powerful AI models and services today, which integrate into LiveKit Agents in the following ways:
Realtime API
: The original production-grade speech-to-speech model. Build lifelike voice assistants with just one model.
GPT 4o, o1-mini, and more
: Smart and creative models for voice AI.
STT models
: From industry-standard
whisper-1
to leading-edge
gpt-4o-transcribe
.
TTS models
: Use OpenAI's latest
gpt-4o-mini-tts
to generate lifelike speech in a voice pipeline.
LiveKit Agents supports OpenAI models through the
OpenAI developer platform
as well as
Azure OpenAI Service
. See the
Azure AI integration guide
for more information on Azure OpenAI.
Getting started
Use the following guide to speak to your own OpenAI-powered voice AI agent in less than 10 minutes.
Voice AI quickstart
Build your first voice AI app with the OpenAI Realtime API or GPT-4o.
Realtime playground
Experiment with the OpenAI Realtime API and personalities like
the
Snarky Teenager
or
Opera Singer
.
LiveKit Agents overview
LiveKit Agents is an open source framework for building realtime AI apps in Python and Node.js. It supports complex voice AI
workflows
with multiple agents and discrete processing steps, and includes built-in load balancing.
LiveKit provides SIP support for
telephony integration
and full-featured
frontend SDKs
in multiple languages. It uses
WebRTC
transport for end-user devices, enabling high-quality, low-latency realtime experiences. To learn more, see
LiveKit Agents
.
Realtime API
LiveKit Agents serves as a bridge between your frontend — connected over WebRTC — and the OpenAI Realtime API — connected over WebSockets. LiveKit automatically converts Realtime API audio response buffers to WebRTC audio streams synchronized with text, and handles business logic like interruption handling automatically. You can add your own logic within your agent, and use LiveKit features for realtime state and data to coordinate with your frontend.
Additional benefits of LiveKit Agents for the Realtime API include:
Noise cancellation
: One line of code to remove background noise and speakers from your input audio.
Telephony
: Inbound and outbound calling using SIP trunks.
Interruption handling
: Automatically handles context truncation on interruption.
Transcription sync
: Realtime API text output is synced to audio playback automatically.
Loading diagram…
Realtime API quickstart
Use the Voice AI quickstart with the Realtime API to get up and running in less than 10 minutes.
Web and mobile frontends
Put your agent in your pocket with a custom web or mobile app.
Telephony integration
Your agent can place and receive calls with LiveKit's SIP integration.
Building voice agents
Comprehensive documentation to build advanced voice AI apps with LiveKit.
Recipes
Get inspired by LiveKit's collection of recipes and example apps.
OpenAI plugin documentation
The following links provide more information on each available OpenAI component in LiveKit Agents.
Realtime API
LiveKit Agents docs for the OpenAI Realtime API.
OpenAI Models
LiveKit Agents docs for
gpt-4o
,
o1-mini
, and other OpenAI LLMs.
OpenAI STT
LiveKit Agents docs for
whisper-1
,
gpt-4o-transcribe
, and other OpenAI STT models.
OpenAI TTS
LiveKit Agents docs for
tts-1
,
gpt-4o-mini-tts
, and other OpenAI TTS models.
On this page
OpenAI ecosystem support
Getting started
LiveKit Agents overview
Realtime API
OpenAI plugin documentation
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/google:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Google AI ecosystem support
Getting started
LiveKit Agents overview
Google plugin documentation
Gemini playground
Play with the Gemini Live API in this LiveKit-powered playground
Google AI ecosystem support
Google AI
provides some of the most powerful AI models and services today, which integrate into LiveKit Agents in the following ways:
Gemini
: A family of general purpose high-performance LLMs.
Google Cloud STT and TTS
: Affordable, production-grade models for transcription and speech synthesis.
Gemini Live API
: A speech-to-speech realtime model with live video input.
LiveKit Agents supports Google AI through the
Gemini API
and
Vertex AI
.
Getting started
Use the Voice AI quickstart to build a voice AI app with Gemini. Select an STT-LLM-TTS pipeline model type and add the following components to build on Gemini.
Voice AI quickstart
Build your first voice AI app with Google Gemini.
Install the Google plugin:
pip
install
"livekit-agents[google]~=1.0"
Copy
Add your Google API key to your
.env.
file:
.env
GOOGLE_API_KEY
=
<
your-google-api-key
>
Copy
Use the Google LLM component to initialize your
AgentSession
:
main.py
from
livekit
.
plugins
import
google
# ...
# in your entrypoint function
session
=
AgentSession
(
llm
=
google
.
LLM
(
model
=
"gemini-2.0-flash"
,
)
,
# ... stt, tts,vad, turn_detection, etc.
)
Copy
LiveKit Agents overview
LiveKit Agents is an open source framework for building realtime AI apps in Python and Node.js. It supports complex voice AI
workflows
with multiple agents and discrete processing steps, and includes built-in load balancing.
LiveKit provides SIP support for
telephony integration
and full-featured
frontend SDKs
in multiple languages. It uses
WebRTC
transport for end-user devices, enabling high-quality, low-latency realtime experiences. To learn more, see
LiveKit Agents
.
Google plugin documentation
The following links provide more information on each available Google component in LiveKit Agents.
Gemini LLM
LiveKit Agents docs for Google Gemini models.
Gemini Live API
LiveKit Agents docs for the Gemini Live API.
Google Cloud STT
LiveKit Agents docs for Google Cloud STT.
Google Cloud TTS
LiveKit Agents docs for Google Cloud TTS.
On this page
Google AI ecosystem support
Getting started
LiveKit Agents overview
Google plugin documentation
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/azure:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Azure AI ecosystem support
Getting started
LiveKit Agents overview
Azure plugin documentation
Azure AI ecosystem support
Microsoft's
Azure AI Services
is a large collection of cutting-edge production-ready AI services, which integrate with LiveKit in the following ways:
Azure OpenAI
: Run OpenAI models, including the Realtime API, with the security and reliability of Azure.
Azure Speech
: Speech-to-text and text-to-speech services.
The LiveKit Agents OpenAI plugin supports Azure OpenAI, and the Azure plugin supports Azure Speech.
Getting started
Use the voice AI quickstart to build a voice AI app with Azure OpenAI. Select a realtime model type and add the following components to use the Azure OpenAI Realtime API:
Voice AI quickstart
Build your first voice AI app with Azure OpenAI.
Install the OpenAI plugin:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Add your Azure OpenAI endpoint and API key to your
.env.
file:
.env
AZURE_OPENAI_ENDPOINT
=
<
your-azure-openai-endpoint
>
AZURE_OPENAI_API_KEY
=
<
your-azure-openai-api-key
>
Copy
Use the
with_azure
method to connect to Azure OpenAI:
main.py
from
livekit
.
plugins
import
openai
# ...
# in your entrypoint function
session
=
AgentSession
(
llm
=
openai
.
realtime
.
RealtimeModel
.
with_azure
(
azure_deployment
=
"<model-deployment>"
,
api_version
=
"2024-10-01-preview"
,
voice
=
"alloy"
,
)
,
# ... vad, turn_detection, etc.
)
Copy
LiveKit Agents overview
LiveKit Agents is an open source framework for building realtime AI apps in Python and Node.js. It supports complex voice AI
workflows
with multiple agents and discrete processing steps, and includes built-in load balancing.
LiveKit provides SIP support for
telephony integration
and full-featured
frontend SDKs
in multiple languages. It uses
WebRTC
transport for end-user devices, enabling high-quality, low-latency realtime experiences. To learn more, see
LiveKit Agents
.
Azure plugin documentation
Azure OpenAI Realtime API
LiveKit Agents docs for Azure OpenAI Realtime API.
Azure OpenAI LLM
LiveKit Agents docs for Azure OpenAI LLMs.
Azure Speech STT
LiveKit Agents docs for Azure Speech STT.
Azure Speech TTS
LiveKit Agents docs for Azure Speech TTS.
On this page
Azure AI ecosystem support
Getting started
LiveKit Agents overview
Azure plugin documentation
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/aws:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
AWS AI ecosystem support
Getting started
AWS plugin documentation
AWS AI ecosystem support
Amazon's
AWS AI
is a comprehensive collection of production-ready AI services, which integrate with LiveKit in the following ways:
Amazon Bedrock
: Access to foundation models from leading AI companies.
Amazon Polly
: Text-to-speech service with lifelike voices.
Amazon Transcribe
: Speech-to-text service with high accuracy.
The LiveKit Agents AWS plugin supports these services for building voice AI applications.
Getting started
Use the voice AI quickstart to build a voice AI app with AWS services. Select a pipeline model type and add the following components to use AWS AI services:
Voice AI quickstart
Build your first voice AI app with AWS AI services.
Install the AWS plugin:
pip
install
"livekit-agents[aws]~=1.0"
Copy
Add your AWS credentials to your
.env
file:
.env
AWS_ACCESS_KEY_ID
=
<
your-aws-access-key-id
>
AWS_SECRET_ACCESS_KEY
=
<
your-aws-secret-access-key
>
AWS_REGION
=
<
your-aws-region
>
Copy
Use the AWS services in your application:
main.py
from
livekit
.
plugins
import
aws
# ...
# in your entrypoint function
session
=
AgentSession
(
llm
=
aws
.
LLM
(
model
=
"anthropic.claude-3-5-sonnet-20240620-v1:0"
,
)
,
tts
=
aws
.
TTS
(
voice
=
"Ruth"
,
speech_engine
=
"generative"
,
language
=
"en-US"
,
)
,
stt
=
aws
.
STT
(
session_id
=
"my-session-id"
,
language
=
"en-US"
,
)
,
# ... vad, turn_detection, etc.
)
Copy
AWS plugin documentation
Amazon Bedrock LLM
LiveKit Agents docs for Amazon Bedrock LLM.
Amazon Polly TTS
LiveKit Agents docs for Amazon Polly TTS.
Amazon Transcribe STT
LiveKit Agents docs for Amazon Transcribe STT.
On this page
AWS AI ecosystem support
Getting started
AWS plugin documentation
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/groq:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Groq ecosystem support
Getting started
LiveKit Agents overview
Groq plugin documentation
Try Groq transcription
Experience Groq's fast STT in a LiveKit-powered playground
Groq ecosystem support
Groq
provides fast AI inference in the cloud and on-prem AI compute centers. LiveKit Agents can integrate with the following Groq services:
STT
: Fast and cost-effective English or multilingual transcription based on
whisper-large-v3
.
TTS
: Fast English and Arabic text-to-speech based on
playai-tts
.
LLM
: Fast inference for open models like
llama-3.1-8b-instant
and more.
Getting started
Use the Voice AI quickstart to build a voice AI app with Groq. Select an STT-LLM-TTS pipeline model type and add the following components to build on Groq.
Voice AI quickstart
Build your first voice AI app with Groq.
Install the Groq plugin:
pip
install
"livekit-agents[groq]~=1.0"
Copy
Add your Groq API key to your
.env.
file:
.env
GROQ_API_KEY
=
<
Your Groq API Key
>
Copy
Use Groq components to initialize your
AgentSession
:
main.py
from
livekit
.
plugins
import
groq
# ...
# in your entrypoint function
session
=
AgentSession
(
stt
=
groq
.
STT
(
model
=
"whisper-large-v3-turbo"
,
language
=
"en"
,
)
,
llm
=
groq
.
LLM
(
model
=
"llama3-8b-8192"
)
,
tts
=
groq
.
TTS
(
model
=
"playai-tts"
,
voice
=
"Arista-PlayAI"
,
)
,
# ... vad, turn_detection, etc.
)
Copy
LiveKit Agents overview
LiveKit Agents is an open source framework for building realtime AI apps in Python and Node.js. It supports complex voice AI
workflows
with multiple agents and discrete processing steps, and includes built-in load balancing.
LiveKit provides SIP support for
telephony integration
and full-featured
frontend SDKs
in multiple languages. It uses
WebRTC
transport for end-user devices, enabling high-quality, low-latency realtime experiences. To learn more, see
LiveKit Agents
.
Groq plugin documentation
The following links provide more information on each available Groq component in LiveKit Agents.
Groq STT
LiveKit Agents docs for Groq transcription models.
Groq TTS
LiveKit Agents docs for Groq speech models.
Groq LLM
LiveKit Agents docs for Groq LLM models including Llama 3, DeepSeek, and more.
On this page
Groq ecosystem support
Getting started
LiveKit Agents overview
Groq plugin documentation
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/cerebras:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Cerebras ecosystem support
Getting started
LiveKit Agents overview
Further reading
Try Cerebras AI
Experience Cerebras's fast inference in a LiveKit-powered voice AI playground
Cerebras ecosystem support
Cerebras
provides high-throughput, low-latency AI inference for open models like Llama and DeepSeek. Cerebras is an OpenAI-compatible LLM provider and LiveKit Agents provides full support for Cerebras inference via the OpenAI plugin.
Getting started
Use the Voice AI quickstart to build a voice AI app with Cerebras. Select an STT-LLM-TTS pipeline model type and add the following components to build on Cerebras.
Voice AI quickstart
Build your first voice AI app with Cerebras.
Install the OpenAI plugin:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Add your Cerebras API key to your
.env
file:
.env
CEREBRAS_API_KEY
=
<
your-cerebras-api-key
>
Copy
Use the Cerebras LLM to initialize your
AgentSession
:
main.py
from
livekit
.
plugins
import
openai
# ...
# in your entrypoint function
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_cerebras
(
model
=
"llama-3.3-70b"
,
)
,
)
Copy
For a full list of supported models, including DeepSeek, see the
Cerebras docs
.
LiveKit Agents overview
LiveKit Agents is an open source framework for building realtime AI apps in Python and Node.js. It supports complex voice AI
workflows
with multiple agents and discrete processing steps, and includes built-in load balancing.
LiveKit provides SIP support for
telephony integration
and full-featured
frontend SDKs
in multiple languages. It uses
WebRTC
transport for end-user devices, enabling high-quality, low-latency realtime experiences. To learn more, see
LiveKit Agents
.
Further reading
More information about integrating Llama is available in the following article:
Cerebras integration guide
LiveKit docs on Cerebras integration.
On this page
Cerebras ecosystem support
Getting started
LiveKit Agents overview
Further reading
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llama:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Providers
Overview
Llama
is a family of open source LLMs developed by
Meta AI
. These models are widely deployed by a large number of cloud inference providers, and also support your own local or private deployments.
Providers
LiveKit Agents has out-of-the-box support for all of these Llama inference providers:
Cerebras
Fast text-only inference for the latest Llama models.
Groq
Fast inference for the latest Llama models.
Fireworks
Inference, fine-tuning, and multimodal support for the latest Llama models.
Perplexity
The Sonar family of models are based on Llama 3.1, fine-tuned for search.
Telnyx
Hosted inference for the latest Llama models.
Together AI
Inference, fine-tuning, and multimodal support for the latest Llama models.
Ollama
Run Llama and other open source models locally.
On this page
Overview
Providers
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/realtime/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
How to use
Available providers
Considerations and limitations
Turn detection and VAD
Delayed transcription
Scripted speech output
Overview
Realtime models are capable of consuming and producing speech directly, bypassing the need for a voice pipeline with speech-to-text and text-to-speech components. They can be better at understanding the emotional context of input speech, as well as other verbal cues that may not translate well to text transcription. Additionally, the generated speech can include similar emotional aspects and other improvements over what a text-to-speech model can produce.
The agents framework includes plugins for popular realtime models out of the box. This is a new area in voice AI and LiveKit aims to support new providers as they emerge.
LiveKit is open source and welcomes
new plugin contributions
.
How to use
Realtime model plugins have a constructor method to create a
RealtimeModel
instance. This instance can be passed directly to an
AgentSession
or
Agent
in its constructor, in place of an
LLM plugin
.
from
livekit
.
agents
import
AgentSession
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
realtime
.
RealtimeModel
(
)
)
Copy
For additional information about installing and using plugins, see the
plugins overview
.
Available providers
The following table lists the available realtime model providers.
Provider
Plugin
Azure OpenAI Realtime API
openai
Gemini Live API
google
OpenAI Realtime API
openai
Considerations and limitations
Realtime models bring great benefits due to their wider range of audio understanding and expressive output. However, they also have some limitations and considerations to keep in mind.
Turn detection and VAD
In general, LiveKit recommends using the built-in turn detection capabilities of the realtime model whenever possible. Accurate turn detection relies on both VAD and context gained from realtime speech-to-text, which, as discussed in the following section, isn't available with realtime models. If you need to use the LiveKit
turn detector model
, you must also add a separate STT plugin to provide the necessary interim transcripts.
Delayed transcription
Realtime models don't provide interim transcription results, and in general the user input transcriptions can be considerably delayed and often arrive after the agent's response. If you need realtime transcriptions, you should consider an STT-LLM-TTS pipeline or add a separate STT plugin for realtime transcription.
Scripted speech output
Realtime models don't offer a method to directly generate speech from a text script, such as with the
say
method. You can produce a response with
generate_reply(instructions='...')
and include specific instructions but the output isn't guaranteed to precisely follow any provided script. If you must use a specific script, you should add a separate TTS plugin to your
AgentSession
for use with the
say
method. For the most seamless experience, use a TTS plugin with the same provider and voice configuration as your realtime model.
On this page
Overview
How to use
Available providers
Considerations and limitations
Turn detection and VAD
Delayed transcription
Scripted speech output
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
How to use
Usage in  AgentSession
Standalone usage
Tool usage
Vision usage
Available providers
Further reading
Overview
Large language models (LLMs) are a type of AI model that can generate text output from text input. In voice AI apps, they fit between speech-to-text (STT) and text-to-speech (TTS) and are responsible for tool calls and generating the agent's text response.
The agents framework includes plugins for popular LLM providers out of the box. You can also implement the
LLM node
to provide custom behavior or an alternative provider.
LiveKit is open source and welcomes
new plugin contributions
.
Realtime models
Realtime models like the OpenAI Realtime API and Gemini Live API are capable of consuming and producing speech directly. LiveKit Agents supports them as an alternative to using an LLM plugin, without the need for STT and TTS. To learn more, see
Realtime models
.
How to use
The following sections describe high-level usage only.
For more detailed information about installing and using plugins, see the
plugins overview
.
Usage in
AgentSession
Construct an
AgentSession
or
Agent
with an
LLM
instance created by your desired plugin:
from
livekit
.
agents
import
AgentSession
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
)
Copy
Standalone usage
You can also use an
LLM
instance in a standalone fashion with its simple streaming interface. It expects a
ChatContext
object, which contains the conversation history. The return value is a stream of
ChatChunk
s. This interface is the same across all LLM providers, regardless of their underlying API design:
from
livekit
.
agents
import
ChatContext
from
livekit
.
plugins
import
openai
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
chat_ctx
=
ChatContext
(
)
chat_ctx
.
add_message
(
role
=
"user"
,
content
=
"Hello, this is a test message!"
)
async
with
llm
.
chat
(
chat_ctx
=
chat_ctx
)
as
stream
:
async
for
chunk
in
stream
:
print
(
"Received chunk:"
,
chunk
.
delta
)
Copy
Tool usage
Most LLM providers support tools (sometimes called "functions"). LiveKit Agents has full support for them within an
AgentSession
. For more information, see
the documentation
.
Vision usage
Some LLM providers support vision within their models. LiveKit agents supports vision input from URL or from
realtime video frames
. Consult your model provider for details on compatible image types, external URL support, and other constraints.
from
livekit
.
agents
.
llm
import
ImageContent
chat_ctx
.
add_message
(
role
=
"user"
,
content
=
[
"Describe this image"
,
ImageContent
(
image
=
"https://picsum.photos/200/300"
)
]
)
Copy
Available providers
The following table lists the available LLM providers for LiveKit Agents.
OpenAI API compatibility
Many providers have standardized around the OpenAI API "chat completions" API, even for other models like Llama, DeepSeek, and more. The LiveKit Agents OpenAI plugin includes compatibility with many of these providers, listed in the following table.
Provider
Plugin
Notes
Amazon Bedrock
aws
Wide range of models from Llama, DeepSeek, Mistral, and more.
Anthropic
anthropic
Claude family of models.
Google Gemini
google
Groq
groq
Models from Llama, DeepSeek, and more.
OpenAI
openai
Azure OpenAI
openai
Cerebras
openai
Models from Llama and DeepSeek.
DeepSeek
openai
Fireworks
openai
Wide range of models from Llama, DeepSeek, Mistral, and more.
Perplexity
openai
Telnyx
openai
Models from Llama, DeepSeek, OpenAI, and Mistral, and more.
xAI
openai
Grok family of models.
Ollama
openai
Self-hosted models from Llama, DeepSeek, and more.
Together AI
openai
Models from Llama, DeepSeek, Mistral, and more.
Further reading
Workflows
How to model repeatable, accurate tasks with multiple agents.
Tool definition and usage
Let your agents call external tools and more.
On this page
Overview
How to use
Usage in  AgentSession
Standalone usage
Tool usage
Vision usage
Available providers
Further reading
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/stt/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
How to use
Usage in  AgentSession
Standalone usage
Available providers
Further reading
Overview
Speech-to-text (STT) models process incoming audio and convert it to text in realtime. In voice AI, this text is then processed by an
LLM
to generate a response which is turn turned backed to speech using a
TTS
model.
The agents framework includes plugins for popular STT providers out of the box. You can also implement the
STT node
to provide custom behavior or use an alternative provider.
LiveKit is open source and welcomes
new plugin contributions
.
How to use
The following sections describe high-level usage only.
For more detailed information about installing and using plugins, see the
plugins overview
.
Usage in
AgentSession
Construct an
AgentSession
or
Agent
with an
STT
instance created by your desired plugin:
from
livekit
.
agents
import
AgentSession
from
livekit
.
plugins
import
deepgram
session
=
AgentSession
(
stt
=
deepgram
.
STT
(
model
=
"nova-2"
)
)
Copy
AgentSession
automatically integrates with VAD to detect user turns and know when to start and stop STT.
Standalone usage
You can also use an
STT
instance in a standalone fashion by creating a stream. You can use
push_frame
to add
realtime audio frames
to the stream, and then consume a stream of
SpeechEvent
as output.
Here is an example of a standalone STT app:
main.py
import
asyncio
from
dotenv
import
load_dotenv
from
livekit
import
agents
,
rtc
from
livekit
.
agents
.
stt
import
SpeechEventType
,
SpeechEvent
from
typing
import
AsyncIterable
from
livekit
.
plugins
import
(
deepgram
,
)
load_dotenv
(
)
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
await
ctx
.
connect
(
)
@ctx
.
room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
RemoteTrack
)
:
print
(
f"Subscribed to track:
{
track
.
name
}
"
)
asyncio
.
create_task
(
process_track
(
track
)
)
async
def
process_track
(
track
:
rtc
.
RemoteTrack
)
:
stt
=
deepgram
.
STT
(
model
=
"nova-2"
)
stt_stream
=
stt
.
stream
(
)
audio_stream
=
rtc
.
AudioStream
(
track
)
async
with
asyncio
.
TaskGroup
(
)
as
tg
:
# Create task for processing STT stream
stt_task
=
tg
.
create_task
(
process_stt_stream
(
stt_stream
)
)
# Process audio stream
async
for
audio_event
in
audio_stream
:
stt_stream
.
push_frame
(
audio_event
.
frame
)
# Indicates the end of the audio stream
stt_stream
.
end_input
(
)
# Wait for STT processing to complete
await
stt_task
async
def
process_stt_stream
(
stream
:
AsyncIterable
[
SpeechEvent
]
)
:
try
:
async
for
event
in
stream
:
if
event
.
type
==
SpeechEventType
.
FINAL_TRANSCRIPT
:
print
(
f"Final transcript:
{
event
.
alternatives
[
0
]
.
text
}
"
)
elif
event
.
type
==
SpeechEventType
.
INTERIM_TRANSCRIPT
:
print
(
f"Interim transcript:
{
event
.
alternatives
[
0
]
.
text
}
"
)
elif
event
.
type
==
SpeechEventType
.
START_OF_SPEECH
:
print
(
"Start of speech"
)
elif
event
.
type
==
SpeechEventType
.
END_OF_SPEECH
:
print
(
"End of speech"
)
finally
:
await
stream
.
aclose
(
)
if
__name__
==
"__main__"
:
agents
.
cli
.
run_app
(
agents
.
WorkerOptions
(
entrypoint_fnc
=
entrypoint
)
)
Copy
VAD and StreamAdapter
Some STT providers or models, such as
Whisper
don't support streaming input.
In these cases, your app must determine when a chunk of audio represents a
complete segment of speech. You can do this using VAD together with the
StreamAdapter
class.
The following example modifies the previous example to use VAD and
StreamAdapter
to buffer user speech until
VAD detects the end of speech:
from
livekit
import
agents
,
rtc
from
livekit
.
plugins
import
openai
,
silero
async
def
process_track
(
ctx
:
agents
.
JobContext
,
track
:
rtc
.
Track
)
:
whisper_stt
=
openai
.
STT
(
)
vad
=
silero
.
VAD
.
load
(
min_speech_duration
=
0.1
,
min_silence_duration
=
0.5
,
)
vad_stream
=
vad
.
stream
(
)
# StreamAdapter will buffer audio until VAD emits END_SPEAKING event
stt
=
agents
.
stt
.
StreamAdapter
(
whisper_stt
,
vad_stream
)
stt_stream
=
stt
.
stream
(
)
.
.
.
Copy
Available providers
The following table lists the available STT providers for LiveKit Agents.
Provider
Plugin
Amazon Transcribe
aws
AssemblyAI
assemblyai
Azure AI Speech
azure
Clova
clova
Deepgram
deepgram
fal
fal
Gladia
gladia
Google Cloud
google
Groq
groq
OpenAI
openai
Speechmatics
speechmatics
Further reading
Text and transcriptions
Integrate realtime text features into your agent.
Pipeline nodes
Learn how to customize the behavior of your agent by overriding nodes in the voice pipeline.
On this page
Overview
How to use
Usage in  AgentSession
Standalone usage
Available providers
Further reading
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/tts/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
How to use
Usage in  AgentSession
Standalone usage
Available providers
Further reading
Text-to-speech (TTS) models produce realtime synthetic speech from text input. In voice AI, this allows a text-based
LLM
to speak its response to the user.
The agents framework includes plugins for popular TTS providers out of the box. You can also implement the
TTS node
to provide custom behavior or use an alternative provider.
LiveKit is open source and welcomes
new plugin contributions
.
How to use
The following sections describe high-level usage only.
For more detailed information about installing and using plugins, see the
plugins overview
.
Usage in
AgentSession
Construct an
AgentSession
or
Agent
with a
TTS
instance created by your desired plugin:
from
livekit
.
agents
import
AgentSession
from
livekit
.
plugins
import
cartesia
session
=
AgentSession
(
tts
=
cartesia
.
TTS
(
model
=
"sonic-english"
)
)
Copy
AgentSession
automatically sends LLM responses to the TTS model, and also supports a
say
method for one-off responses.
Standalone usage
You can also use a
TTS
instance in a standalone fashion by creating a stream. You can use
push_text
to add text to the stream, and then consume a stream of
SynthesizedAudio
as to publish as
realtime audio
to another participant.
Here is an example of a standalone TTS app:
main.py
from
livekit
import
agents
,
rtc
from
livekit
.
agents
.
tts
import
SynthesizedAudio
from
livekit
.
plugins
import
cartesia
from
typing
import
AsyncIterable
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
await
ctx
.
connect
(
)
text_stream
:
AsyncIterable
[
str
]
=
.
.
.
# you need to provide a stream of text
audio_source
=
rtc
.
AudioSource
(
44100
,
1
)
track
=
rtc
.
LocalAudioTrack
.
create_audio_track
(
"agent-audio"
,
audio_source
)
await
ctx
.
room
.
local_participant
.
publish_track
(
track
)
tts
=
cartesia
.
TTS
(
model
=
"sonic-english"
)
tts_stream
=
tts
.
stream
(
)
# create a task to consume and publish audio frames
ctx
.
create_task
(
send_audio
(
tts_stream
)
)
# push text into the stream, TTS stream will emit audio frames along with events
# indicating sentence (or segment) boundaries.
async
for
text
in
text_stream
:
tts_stream
.
push_text
(
text
)
tts_stream
.
end_input
(
)
async
def
send_audio
(
audio_stream
:
AsyncIterable
[
SynthesizedAudio
]
)
:
async
for
a
in
audio_stream
:
await
audio_source
.
capture_frame
(
e
.
audio
.
frame
)
if
__name__
==
"__main__"
:
agents
.
cli
.
run_app
(
agents
.
WorkerOptions
(
entrypoint_fnc
=
entrypoint
)
)
Copy
Available providers
The following table lists the available TTS providers for LiveKit Agents.
Provider
Plugin
Amazon Polly
aws
Azure AI Speech
azure
Cartesia
cartesia
Deepgram
deepgram
ElevenLabs
elevenlabs
Google Cloud
google
Groq
groq
Neuphonic
neuphonic
OpenAI
openai
PlayHT
playai
Rime
rime
Further reading
Agent speech docs
Explore the speech capabilities and features of LiveKit Agents.
Pipeline nodes
Learn how to customize the behavior of your agent by overriding nodes in the voice pipeline.
On this page
How to use
Usage in  AgentSession
Standalone usage
Available providers
Further reading
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/cli/cli-setup:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Install LiveKit CLI
Authenticate with Cloud (optional)
Generate access token
Test with LiveKit Meet
Simulating another publisher
Install LiveKit CLI
macOS
Linux
Windows
From Source
brew update && brew install livekit-cli
Copy
lk
is LiveKit's suite of CLI utilities. It lets you conveniently access server APIs, create tokens, and generate test traffic all from your command line. For more details, refer to the docs in the
livekit-cli
GitHub repo
.
Authenticate with Cloud (optional)
For LiveKit Cloud users, you can authenticate the CLI with your Cloud project to create an API key and secret. This allows you to use the CLI without manually providing credentials each time.
lk cloud auth
Copy
Then, follow instructions and log in from a browser.
Tip
If you're looking to explore LiveKit's
Agents
framework, or want to prototype your app against a prebuilt frontend or token server, check out
Sandboxes
.
Generate access token
A participant creating or joining a LiveKit
room
needs an
access token
to do so. For now, let’s generate one via CLI:
Cloud
Localhost
lk token create
\
--api-key
<
PROJECT_KEY
>
--api-secret
<
PROJECT_SECRET
>
\
--join
--room
test_room
--identity
test_user
\
--valid-for 24h
Copy
Alternatively, you can
generate tokens from your project's dashboard
.
Test with LiveKit Meet
Tip
If you're testing a LiveKit Cloud instance, you can find your
Project URL
(it starts with
wss://
) in the project settings.
Use a sample app,
LiveKit Meet
, to preview your new LiveKit instance. Enter the token you
previously generated
in the "Custom" tab. Once connected, your microphone and camera will be streamed in realtime to your new LiveKit instance (and any other participant who connects to the same room)!
If interested, here's the
full source
for this example app.
Simulating another publisher
One way to test a multi-user session is by
generating
a second token (ensure
--identity
is unique), opening our example app in another
browser tab
and connecting to the same room.
Another way is to use the CLI as a simulated participant and publish a prerecorded video to the room. Here's how:
Cloud
Localhost
lk room
join
\
--url
<
PROJECT_SECURE_WEBSOCKET_ADDRESS
>
\
--api-key
<
PROJECT_API_KEY
>
--api-secret
<
PROJECT_SECRET_KEY
>
\
--publish-demo
--identity
bot_user
\
my_first_room
Copy
This command publishes a looped demo video to
my-first-room
. Due to how the file was encoded, expect a short delay before your browser has sufficient data to render frames.
On this page
Install LiveKit CLI
Authenticate with Cloud (optional)
Generate access token
Test with LiveKit Meet
Simulating another publisher
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/cli/templates:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
Note
Before starting, make sure you have created a Cloud account,
installed the LiveKit CLI
, and have authenticated or manually configured your LiveKit project of choice.
The LiveKit CLI can help you bootstrap applications from a number of convenient template repositories, using your project credentials to set up required environment variables and other configuration automatically. To create an application from a template, run the following:
lk app create
--template
<
template_name
>
my-app
Copy
Then follow the CLI prompts to finish your setup.
The
--template
flag may be omitted to see a list of all available templates, or can be chosen from a selection of our first-party templates:
Template Name
Language/Framework
Description
voice-assistant-frontend
TypeScript/Next.js
Voice assistant frontend with integrated token server
meet
TypeScript/Next.js
Video conferencing frontend with integrated token server
multimodal-agent-python
Python
Multimodal agent with speech-to-speech and transcription capabilities
voice-pipeline-agent-python
Python
Voice agent using modular TTS, LLM, and STT capabilities
multimodal-agent-node
Node.js/TypeScript
Multimodal agent with speech-to-speech and transcription capabilities
token-server-node
Node.js/TypeScript
Token server for generating access tokens
android-voice-assistant
Kotlin/Android
Voice assistant mobile application
Tip
If you're looking to explore LiveKit's
Agents
framework, or want to prototype your app against a prebuilt frontend or token server, check out
Sandboxes
.
For more information on templates, see the
LiveKit Template Index
.
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/connect:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Installing the LiveKit SDK
Connecting to a room
Disconnection
Automatic disconnection
Connection reliability
Network changes and reconnection
Overview
Your application will connect to LiveKit using the
Room
object, which is the base construct in LiveKit. Think of it like a conference call — multiple participants can join a room and share realtime audio, video, and data with each other.
Depending on your application, each participant might represent a user, an AI agent, a connected device, or some other program you've created. There is no limit on the number of participants in a room and each participant can publish audio, video, and data to the room.
Installing the LiveKit SDK
LiveKit includes open-source SDKs for every major platform including JavaScript, Swift, Android, React Native, Flutter, and Unity.
LiveKit also has SDKs for realtime backend apps in Python, Node.js, Go, and Rust. These are designed to be used with the
Agents framework
for realtime AI applications.
JavaScript
Swift
Android
React Native
Flutter
Install the LiveKit SDK and optional React Components library:
npm
install
livekit-client @livekit/components-react @livekit/components-styles
--save
Copy
The SDK is also available using
yarn
or
pnpm
.
Check out the dedicated quickstarts for
React
or
Next.js
if you're using one of those platforms.
If your SDK is not listed above, check out the full list of
platform-specific quickstarts
and
SDK reference docs
for more details.
Connecting to a room
Rooms are identified by their name, which can be any unique string. The room itself is created automatically when the first participant joins, and is closed when the last participant leaves.
You must use a participant identity when you connect to a room. This identity can be any string, but must be unique to each participant.
Connecting to a room always requires two parameters:
wsUrl
: The WebSocket URL of your LiveKit server.
LiveKit Cloud users can find theirs on the
Project Settings page
.
Self-hosted users who followed
this guide
can use
ws://localhost:7880
while developing.
token
: A unique
access token
which each participant must use to connect.
The token encodes the room name, the participant's identity, and their permissions.
For help generating tokens, see
this guide
.
JavaScript
React
Swift
Android
React Native
Flutter
const
room
=
new
Room
(
)
;
await
room
.
connect
(
wsUrl
,
token
)
;
Copy
Upon successful connection, the
Room
object will contain two key attributes: a
localParticipant
object, representing the current user, and
remoteParticipants
, an array of other participants in the room.
Once connected, you can
publish
and
subscribe
to realtime media tracks or
exchange data
with other participants.
LiveKit also emits a number of events on the
Room
object, such as when new participants join or tracks are published. For details, see
Handling Events
.
Disconnection
Call
Room.disconnect()
to leave the room. If you terminate the application without calling
disconnect()
, your participant disappears after 15 seconds.
Note
On some platforms, including JavaScript and Swift,
Room.disconnect
is called automatically when the application exits.
Automatic disconnection
Participants might get disconnected from a room due to server-initiated actions. This can happen if the room is closed using the
DeleteRoom
API or if a participant is removed with the
RemoveParticipant
API.
In such cases, a
Disconnected
event is emitted, providing a reason for the disconnection. Common
disconnection reasons
include:
DUPLICATE_IDENTITY: Disconnected because another participant with the same identity joined the room.
ROOM_DELETED: The room was closed via the
DeleteRoom
API.
PARTICIPANT_REMOVED: Removed from the room using the
RemoveParticipant
API.
JOIN_FAILURE: Failure to connect to the room, possibly due to network issues.
ROOM_CLOSED: The room was closed because all
Standard and Ingress participants
left.
Connection reliability
LiveKit enables reliable connectivity in a wide variety of network conditions. It tries the following WebRTC connection types in descending order:
ICE over UDP: ideal connection type, used in majority of conditions
TURN with UDP (3478): used when ICE/UDP is unreachable
ICE over TCP: used when network disallows UDP (i.e. over VPN or corporate firewalls)
TURN with TLS: used when firewall only allows outbound TLS connections
Cloud
Self-hosted
LiveKit Cloud supports all of the above connection types. TURN servers with TLS are provided and maintained by LiveKit Cloud.
Network changes and reconnection
With WiFi and cellular networks, users may sometimes run into network changes that cause the connection to the server to be interrupted. This could include switching from WiFi to cellular or going through spots with poor connection.
When this happens, LiveKit will attempt to resume the connection automatically. It reconnects to the signaling WebSocket and initiates an
ICE restart
for the WebRTC connection.
This process usually results in minimal or no disruption for the user. However, if media delivery over the previous connection fails, users might notice a temporary pause in video, lasting a few seconds, until the new connection is established.
In scenarios where an ICE restart is not feasible or unsuccessful, LiveKit will execute a full reconnection. As full reconnections take more time and might be more disruptive, a
Reconnecting
event is triggered. This allows your application to respond, possibly by displaying a UI element, during the reconnection process.
This sequence goes like the following:
ParticipantDisconnected
fired for other participants in the room
If there are tracks unpublished, you will receive
LocalTrackUnpublished
for them
Emits
Reconnecting
Performs full reconnect
Emits
Reconnected
For everyone currently in the room, you will receive
ParticipantConnected
Local tracks are republished, emitting
LocalTrackPublished
events
In essence, the full reconnection sequence is identical to everyone else having left the room, and came back.
On this page
Overview
Installing the LiveKit SDK
Connecting to a room
Disconnection
Automatic disconnection
Connection reliability
Network changes and reconnection
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/tracks/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Audio tracks
Video tracks
Example use cases
AI voice agent
Video conference
Livestreaming
AI camera monitoring
Overview
LiveKit provides realtime media exchange between participants using tracks. Each participant can
publish
and
subscribe
to as many tracks as makes sense for your application.
Audio tracks
Audio tracks are typically published from your microphone and played back on the other participants' speakers. You can also produce custom audio tracks, for instance to add background music or other audio effects.
AI agents can consume an audio track to perform speech-to-text, and can publish their own audio track with synthesized speech or other audio effects.
Video tracks
Video tracks are usually published from a webcam or other video source, and rendered on the other participants' screens within your application's UI. LiveKit also supports screen sharing, which commonly results in two video tracks from the same participant.
AI agents can subscribe to video tracks to perform vision-based tasks, and can publish their own video tracks with synthetic video or other visual effects.
Example use cases
The following examples demonstrate how to model your application for different use cases.
AI voice agent
Each room has two participants: an end-user and an AI agent. They can have a natural conversation with the following setup:
End-user
: publishes their microphone track and subscribes to the AI agent's audio track
AI agent
: subscribes to the user's microphone track and publishes its own audio track with synthesized speech
The UI may be a simple audio visualizer showing that the AI agent is speaking.
Video conference
Each room has multiple users. Each user publishes audio and/or video tracks and subscribes to all tracks published by others. In the UI, the room is typically displayed as a grid of video tiles.
Livestreaming
Each room has one broadcaster and a significant number of viewers. The broadcaster publishes audio and video tracks. The viewers subscribe to the broadcaster's tracks but do not publish their own. Interaction is typically performed with a chat component.
An AI agent may also join the room to publish live captions.
AI camera monitoring
Each room has one camera participant that publishes its video track, and one agent that monitors the camera feed and calls out to an external API to take action based on contents of the video feed (e.g. send an alert).
Alternatively, one room can have multiple cameras and an agent that monitors all of them, or an end-user could also optionally join the room to monitor the feeds alongside the agent.
On this page
Overview
Audio tracks
Video tracks
Example use cases
AI voice agent
Video conference
Livestreaming
AI camera monitoring
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/data/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
Sending text
Use text streams to send and receive text data, such as LLM responses or chat messages.
Sending files & bytes
Use byte streams to transfer files, images, or any other binary data.
Remote method calls
Use RPC to execute custom methods on other participants in the room and await a response.
For low-level control over individual packet behavior, LiveKit also includes a simple
data packets
API.
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/state/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
Participant attributes & metadata
A key-value store for every participant that can be used for managing online status, user preferences, and more.
Room metadata
A freeform string for room-wide state, ideal for room configuration and shared settings.
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/events:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Declarative UI
Events
Overview
The LiveKit SDKs use events to communicate with the application changes that are taking place in the room.
There are two kinds of events,
room events
and
participant events
. Room events are emitted from the main
Room
object, reflecting any change in the room. Participant events are emitted from each
Participant
, when that specific participant has changed.
Room events are generally a superset of participant events. As you can see, some events are fired on both
Room
and
Participant
; this is intentional. This duplication is designed to make it easier to componentize your application. For example, if you have a UI component that renders a participant, it should only listen to events scoped to that participant.
Declarative UI
Event handling can be quite complicated in a realtime, multi-user system. Participants could be joining and leaving, each publishing tracks or muting them. To simplify this, LiveKit offers built-in support for
declarative UI
for most platforms.
With declarative UI you specify the how the UI should look given a particular state, without having to worry about the sequence of transformations to apply. Modern frameworks are highly efficient at detecting changes and rendering only what's changed.
React
SwiftUI
Android Compose
Flutter
We offer a few hooks and components that makes working with React much simpler.
useParticipant
- maps participant events to state
useTracks
- returns the current state of the specified audio or video track
VideoTrack
- React component that renders a video track
RoomAudioRenderer
- React component that renders the sound of all audio tracks
const
Stage
=
(
)
=>
{
const
tracks
=
useTracks
(
[
Track
.
Source
.
Camera
,
Track
.
Source
.
ScreenShare
]
)
;
return
(
<
LiveKitRoom
{
/* ... */
}
>
// Render all video
{
tracks
.
map
(
(
track
)
=>
{
<
VideoTrack
trackRef
=
{
track
}
/>
;
}
)
}
// ...and all audio tracks.
<
RoomAudioRenderer
/>
</
LiveKitRoom
>
)
;
}
;
function
ParticipantList
(
)
{
// Render a list of all participants in the room.
const
participants
=
useParticipants
(
)
;
<
ParticipantLoop
participants
=
{
participants
}
>
<
ParticipantName
/>
</
ParticipantLoop
>
;
}
Copy
Events
This table captures a consistent set of events that are available across platform SDKs. In addition to what's listed here, there may be platform-specific events on certain platforms.
Event
Description
Room Event
Participant Event
ParticipantConnected
A RemoteParticipant joins
after
the local participant.
✔️
ParticipantDisconnected
A RemoteParticipant leaves
✔️
Reconnecting
The connection to the server has been interrupted and it's attempting to reconnect.
✔️
Reconnected
Reconnection has been successful
✔️
Disconnected
Disconnected from room due to the room closing or unrecoverable failure
✔️
TrackPublished
A new track is published to room after the local participant has joined
✔️
✔️
TrackUnpublished
A RemoteParticipant has unpublished a track
✔️
✔️
TrackSubscribed
The LocalParticipant has subscribed to a track
✔️
✔️
TrackUnsubscribed
A previously subscribed track has been unsubscribed
✔️
✔️
TrackMuted
A track was muted, fires for both local tracks and remote tracks
✔️
✔️
TrackUnmuted
A track was unmuted, fires for both local tracks and remote tracks
✔️
✔️
LocalTrackPublished
A local track was published successfully
✔️
✔️
LocalTrackUnpublished
A local track was unpublished
✔️
✔️
ActiveSpeakersChanged
Current active speakers has changed
✔️
IsSpeakingChanged
The current participant has changed speaking status
✔️
ConnectionQualityChanged
Connection quality was changed for a Participant
✔️
✔️
ParticipantAttributesChanged
A participant's attributes were updated
✔️
✔️
ParticipantMetadataChanged
A participant's metadata was updated
✔️
✔️
RoomMetadataChanged
Metadata associated with the room has changed
✔️
DataReceived
Data received from another participant or server
✔️
✔️
TrackStreamStateChanged
Indicates if a subscribed track has been paused due to bandwidth
✔️
✔️
TrackSubscriptionPermissionChanged
One of subscribed tracks have changed track-level permissions for the current participant
✔️
✔️
ParticipantPermissionsChanged
When the current participant's permissions have changed
✔️
✔️
On this page
Overview
Declarative UI
Events
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/quickstarts/nextjs-13:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
1. Create a Next.js app
2. Install LiveKit SDK
3. Keys and Configuration
4. Create token endpoint
5. Make a page in your web app
6. Load the page and connect
Next steps
Note
This guide is for Next.js 13+. On an older version? Check out the
quickstart for Next.js 12
.
This quickstart tutorial walks you through the steps to build a video-conferencing application using NextJS. It uses LiveKit's
React Components
to render the UI and communicate with LiveKit servers via WebRTC. By the end, you will have a basic video-conferencing application you can run with multiple participants.
1. Create a Next.js app
If you're working with an existing app, skip to the next step.
npx create-next-app
<
your_app_name
>
Copy
Change directory into your app directory:
cd
<
your_app_name
>
Copy
2. Install LiveKit SDK
Install both frontend and backend LiveKit SDKs:
yarn
npm
yarn
add
livekit-server-sdk @livekit/components-react @livekit/components-styles
Copy
3. Keys and Configuration
To start, your app needs an LiveKit API key and secret, as well as your LiveKit project URL.
In your project root create the file
.env.local
with the following contents. Do not commit this file because it contains your secrets!
LIVEKIT_API_KEY
=
<
your API Key
>
LIVEKIT_API_SECRET
=
<
your API Secret
>
LIVEKIT_URL
=
<
your LiveKit server URL
>
Reveal API Key and Secret
Copy
4. Create token endpoint
Create a new file at
/app/api/token/route.ts
with the following content:
import
{
NextRequest
,
NextResponse
}
from
'next/server'
;
import
{
AccessToken
}
from
'livekit-server-sdk'
;
// Do not cache endpoint result
export
const
revalidate
=
0
;
export
async
function
GET
(
req
:
NextRequest
)
{
const
room
=
req
.
nextUrl
.
searchParams
.
get
(
'room'
)
;
const
username
=
req
.
nextUrl
.
searchParams
.
get
(
'username'
)
;
if
(
!
room
)
{
return
NextResponse
.
json
(
{
error
:
'Missing "room" query parameter'
}
,
{
status
:
400
}
)
;
}
else
if
(
!
username
)
{
return
NextResponse
.
json
(
{
error
:
'Missing "username" query parameter'
}
,
{
status
:
400
}
)
;
}
const
apiKey
=
process
.
env
.
LIVEKIT_API_KEY
;
const
apiSecret
=
process
.
env
.
LIVEKIT_API_SECRET
;
const
wsUrl
=
process
.
env
.
LIVEKIT_URL
;
if
(
!
apiKey
||
!
apiSecret
||
!
wsUrl
)
{
return
NextResponse
.
json
(
{
error
:
'Server misconfigured'
}
,
{
status
:
500
}
)
;
}
const
at
=
new
AccessToken
(
apiKey
,
apiSecret
,
{
identity
:
username
}
)
;
at
.
addGrant
(
{
room
,
roomJoin
:
true
,
canPublish
:
true
,
canSubscribe
:
true
}
)
;
return
NextResponse
.
json
(
{
token
:
await
at
.
toJwt
(
)
}
,
{
headers
:
{
"Cache-Control"
:
"no-store"
}
}
,
)
;
}
Copy
5. Make a page in your web app
Make a new file at
/app/room/page.tsx
with the following content:
'use client'
;
import
{
ControlBar
,
GridLayout
,
ParticipantTile
,
RoomAudioRenderer
,
useTracks
,
RoomContext
,
}
from
'@livekit/components-react'
;
import
{
Room
,
Track
}
from
'livekit-client'
;
import
'@livekit/components-styles'
;
import
{
useEffect
,
useState
}
from
'react'
;
export
default
function
Page
(
)
{
// TODO: get user input for room and name
const
room
=
'quickstart-room'
;
const
name
=
'quickstart-user'
;
const
[
roomInstance
]
=
useState
(
(
)
=>
new
Room
(
{
// Optimize video quality for each participant's screen
adaptiveStream
:
true
,
// Enable automatic audio/video quality optimization
dynacast
:
true
,
}
)
)
;
useEffect
(
(
)
=>
{
let
mounted
=
true
;
(
async
(
)
=>
{
try
{
const
resp
=
await
fetch
(
`
/api/token?room=
${
room
}
&username=
${
name
}
`
)
;
const
data
=
await
resp
.
json
(
)
;
if
(
!
mounted
)
return
;
if
(
data
.
token
)
{
await
roomInstance
.
connect
(
process
.
env
.
NEXT_PUBLIC_LIVEKIT_URL
,
data
.
token
)
;
}
}
catch
(
e
)
{
console
.
error
(
e
)
;
}
}
)
(
)
;
return
(
)
=>
{
mounted
=
false
;
roomInstance
.
disconnect
(
)
;
}
;
}
,
[
roomInstance
]
)
;
if
(
token
===
''
)
{
return
<
div
>
Getting token...
</
div
>
;
}
return
(
<
RoomContext.Provider
value
=
{
roomInstance
}
>
<
div
data-lk-theme
=
"
default
"
style
=
{
{
height
:
'100dvh'
}
}
>
{
/* Your custom component with basic video conferencing functionality. */
}
<
MyVideoConference
/>
{
/* The RoomAudioRenderer takes care of room-wide audio for you. */
}
<
RoomAudioRenderer
/>
{
/* Controls for the user to start/stop audio, video, and screen share tracks */
}
<
ControlBar
/>
</
div
>
</
RoomContext.Provider
>
)
;
}
function
MyVideoConference
(
)
{
// `useTracks` returns all camera and screen share tracks. If a user
// joins without a published camera track, a placeholder track is returned.
const
tracks
=
useTracks
(
[
{
source
:
Track
.
Source
.
Camera
,
withPlaceholder
:
true
}
,
{
source
:
Track
.
Source
.
ScreenShare
,
withPlaceholder
:
false
}
,
]
,
{
onlySubscribed
:
false
}
,
)
;
return
(
<
GridLayout
tracks
=
{
tracks
}
style
=
{
{
height
:
'calc(100vh - var(--lk-control-bar-height))'
}
}
>
{
/* The GridLayout accepts zero or one child. The child is used
as a template to render all passed in tracks. */
}
<
ParticipantTile
/>
</
GridLayout
>
)
;
}
Copy
6. Load the page and connect
Start your server with:
yarn
npm
yarn
dev
Copy
And then open
localhost:3000/room
in your browser.
Next steps
If you're new to LiveKit, you might want to read the following introductory topics:
Learn more about what LiveKit is and how it works in the
Intro to LiveKit
.
Learn more about rooms, participants, and tracks in
Core concepts
.
If you're looking to dive deeper into building your LiveKit app with React, check out the
React Components
reference section. There you'll find a comprehensive list of available components and React hooks, along with examples of how to use them.
If you're interested in building LiveKit apps with voice agents, see
LiveKit Agents
or check out the
Voice AI quickstart
.
On this page
1. Create a Next.js app
2. Install LiveKit SDK
3. Keys and Configuration
4. Create token endpoint
5. Make a page in your web app
6. Load the page and connect
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/quickstarts/react:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
1. Install LiveKit SDK
2. Join a room
Next steps
1. Install LiveKit SDK
Install the LiveKit SDK:
yarn
npm
yarn
add
@livekit/components-react @livekit/components-styles livekit-client
Copy
2. Join a room
Update the
serverUrl
and
token
values and copy and paste the following into your
src/App.tsx
file. To generate a
token for this example, see
Creating a token
.
Note
This example hardcodes a token. In a real app, your server generates a token for you.
import
{
ControlBar
,
GridLayout
,
ParticipantTile
,
RoomAudioRenderer
,
useTracks
,
RoomContext
,
}
from
'@livekit/components-react'
;
import
{
Room
,
Track
}
from
'livekit-client'
;
import
'@livekit/components-styles'
;
import
{
useState
}
from
'react'
;
const
serverUrl
=
'<your LiveKit server URL>'
;
const
token
=
'<generate a token>'
;
export
default
function
App
(
)
{
const
[
room
]
=
useState
(
(
)
=>
new
Room
(
{
// Optimize video quality for each participant's screen
adaptiveStream
:
true
,
// Enable automatic audio/video quality optimization
dynacast
:
true
,
}
)
)
;
// Connect to room
useEffect
(
(
)
=>
{
let
mounted
=
true
;
const
connect
=
async
(
)
=>
{
if
(
mounted
)
{
await
room
.
connect
(
serverUrl
,
token
)
;
}
}
;
connect
(
)
;
return
(
)
=>
{
mounted
=
false
;
room
.
disconnect
(
)
;
}
;
}
,
[
room
]
)
;
return
(
<
RoomContext.Provider
value
=
{
room
}
>
<
div
data-lk-theme
=
"
default
"
style
=
{
{
height
:
'100vh'
}
}
>
{
/* Your custom component with basic video conferencing functionality. */
}
<
MyVideoConference
/>
{
/* The RoomAudioRenderer takes care of room-wide audio for you. */
}
<
RoomAudioRenderer
/>
{
/* Controls for the user to start/stop audio, video, and screen share tracks */
}
<
ControlBar
/>
</
div
>
</
RoomContext.Provider
>
)
;
}
function
MyVideoConference
(
)
{
// `useTracks` returns all camera and screen share tracks. If a user
// joins without a published camera track, a placeholder track is returned.
const
tracks
=
useTracks
(
[
{
source
:
Track
.
Source
.
Camera
,
withPlaceholder
:
true
}
,
{
source
:
Track
.
Source
.
ScreenShare
,
withPlaceholder
:
false
}
,
]
,
{
onlySubscribed
:
false
}
,
)
;
return
(
<
GridLayout
tracks
=
{
tracks
}
style
=
{
{
height
:
'calc(100vh - var(--lk-control-bar-height))'
}
}
>
{
/* The GridLayout accepts zero or one child. The child is used
as a template to render all passed in tracks. */
}
<
ParticipantTile
/>
</
GridLayout
>
)
;
}
Copy
Next steps
Set up a server to generate tokens for your app at runtime by following this guide:
Generating Tokens
.
If you're looking to dive deeper into building your LiveKit app with React, check out the
React Components
reference section. There you'll find a comprehensive list of available components and React hooks, along with examples of how to use them. This is a great resource for building more complex and advanced apps.
Happy coding!
On this page
1. Install LiveKit SDK
2. Join a room
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/quickstarts/javascript:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
1. Install LiveKit SDK
2. Join a room
3. Next Steps
Tip
Check out the dedicated quickstarts for
React
or
Next.js
if you're using one of those platforms.
1. Install LiveKit SDK
Install the LiveKit SDK:
yarn
npm
yarn
add
livekit-client
Copy
2. Join a room
Note that this example hardcodes a token. In a real app, you’ll need your server to generate a token for you.
import
{
Room
}
from
'livekit-client'
;
const
wsURL
=
'<your LiveKit server URL>'
;
const
token
=
'<generate a token>'
;
const
room
=
new
Room
(
)
;
await
room
.
connect
(
wsURL
,
token
)
;
console
.
log
(
'connected to room'
,
room
.
name
)
;
// Publish local camera and mic tracks
await
room
.
localParticipant
.
enableCameraAndMicrophone
(
)
;
Copy
3. Next Steps
Set up a server to generate tokens for your app at runtime by following this guide:
Generating Tokens
.
View the
full SDK reference
and
GitHub repository
for more documentation and examples.
Happy coding!
On this page
1. Install LiveKit SDK
2. Join a room
3. Next Steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/quickstarts/unity-web:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
1. Install LiveKit SDK
2. Connect to a room
3. Publish video & audio
4. Display a video on a RawImage
5. Next Steps
1. Install LiveKit SDK
Click the Add
+
menu in the Package Manager toolbar, select
Add package from git URL
, and enter:
https://github.com/livekit/client-sdk-unity-web.git
For more details, see the
Unity docs on installing packages from Git URLs
.
2. Connect to a room
Note that this example hardcodes a token. In a real app, you’ll need your server to generate a token for you.
public
class
MyObject
:
MonoBehaviour
{
public
Room
Room
;
IEnumerator
Start
(
)
{
Room
=
new
Room
(
)
;
var
c
=
Room
.
Connect
(
"<your LiveKit server URL>"
,
"<generate a token>"
)
;
yield
return
c
;
if
(
!
c
.
IsError
)
{
// Connected
}
}
}
Copy
3. Publish video & audio
yield
return
Room
.
LocalParticipant
.
EnableCameraAndMicrophone
(
)
;
Copy
4. Display a video on a RawImage
RawImage
image
=
GetComponent
<
RawImage
>
(
)
;
Room
.
TrackSubscribed
+=
(
track
,
publication
,
participant
)
=>
{
if
(
track
.
Kind
==
TrackKind
.
Video
)
{
var
video
=
track
.
Attach
(
)
as
HTMLVideoElement
;
video
.
VideoReceived
+=
tex
=>
{
// VideoReceived is called every time the video resolution changes
image
.
texture
=
tex
;
}
;
}
}
;
Copy
5. Next Steps
Set up a server to generate tokens for your app at runtime by following this guide:
Generating Tokens
.
View the
full SDK reference
and
GitHub repository
for more documentation and examples.
Happy coding!
On this page
1. Install LiveKit SDK
2. Connect to a room
3. Publish video & audio
4. Display a video on a RawImage
5. Next Steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/quickstarts/swift:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
1. Install LiveKit SDK
2. Declare permissions
3. Join a Room
4. Next Steps
Tip
This guide uses the Swift Components library for the easiest way to get started on iOS.
LiveKit also supports macOS, tvOS, and visionOS. More documentation for the core Swift SDK is
on GitHub
.
1. Install LiveKit SDK
Package.swift
Xcode
let
package
=
Package
(
...
dependencies
:
[
.
package
(
url
:
"https://github.com/livekit/client-sdk-swift.git"
,
from
:
"2.0.14"
)
,
// Core SDK
.
package
(
url
:
"https://github.com/livekit/components-swift.git"
,
from
:
"0.1.0"
)
,
// UI Components
]
,
targets
:
[
.
target
(
name
:
"MyApp"
,
dependencies
:
[
.
product
(
name
:
"LiveKitComponents"
,
package
:
"components-swift"
)
,
]
)
]
)
Copy
2. Declare permissions
Camera and microphone usage need to be declared in your
Info.plist
file.
<
dict
>
...
<
key
>
NSCameraUsageDescription
</
key
>
<
string
>
$(PRODUCT_NAME) uses your camera
</
string
>
<
key
>
NSMicrophoneUsageDescription
</
key
>
<
string
>
$(PRODUCT_NAME) uses your microphone
</
string
>
...
</
dict
>
Copy
Your application can still run a voice call when it is switched to the background if the background mode is enabled. Select the app target in Xcode, click the Capabilities tab, enable Background Modes, and check
Audio, AirPlay, and Picture in Picture
.
Your
Info.plist
should have the following entries:
<
dict
>
...
<
key
>
UIBackgroundModes
</
key
>
<
array
>
<
string
>
audio
</
string
>
</
array
>
Copy
3. Join a Room
Note that this example hardcodes a token we generated for you that expires in 2 hours. In a real app, you’ll need your server to generate a token for you.
// !! Note !!
// This sample hardcodes a token which expires in 2 hours.
let
wsURL
=
"<your LiveKit server URL>"
let
token
=
"<generate a token>"
// In production you should generate tokens on your server, and your client
// should request a token from your server.
import
LiveKit
import
LiveKitComponents
import
SwiftUI
@main
struct
ComponentsExampleApp
:
App
{
var
body
:
some
Scene
{
WindowGroup
{
RoomScope
(
url
:
wsURL
,
token
:
token
,
connect
:
true
,
enableCamera
:
true
)
{
LazyVStack
{
ForEachParticipant
{
_
in
VStack
{
ForEachTrack
(
filter
:
.
video
)
{
trackReference
in
VideoTrackView
(
trackReference
:
trackReference
)
.
frame
(
width
:
500
,
height
:
500
)
}
}
}
}
}
}
}
}
Copy
For more details, you can reference
the components example app
.
4. Next Steps
Set up a server to generate tokens for your app at runtime by following this guide:
Generating Tokens
.
Visit the
GitHub repository
for more documentation and examples.
Happy coding!
On this page
1. Install LiveKit SDK
2. Declare permissions
3. Join a Room
4. Next Steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/quickstarts/android-compose:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
1. Install LiveKit Components SDK
2. Permissions
3. Join a room
4. Next Steps
1. Install LiveKit Components SDK
LiveKit Components for Android Compose is available as a Maven package.
...
dependencies {
implementation "io.livekit:livekit-android-compose-components:<current version>"
}
Copy
See the
releases page
for information on the latest version of the SDK.
You'll also need JitPack as one of your repositories. In your
settings.gradle
file:
dependencyResolutionManagement {
repositories {
google()
mavenCentral()
//...
maven { url 'https://jitpack.io' }
}
}
Copy
2. Permissions
LiveKit relies on the
RECORD_AUDIO
and
CAMERA
permissions to use the microphone and camera.
These permission must be requested at runtime, like so:
/**
* Checks if the RECORD_AUDIO and CAMERA permissions are granted.
*
* If not granted, will request them. Will call onPermissionGranted if/when
* the permissions are granted.
*/
fun
ComponentActivity
.
requireNeededPermissions
(
onPermissionsGranted
:
(
(
)
->
Unit
)
?
=
null
)
{
val
requestPermissionLauncher
=
registerForActivityResult
(
ActivityResultContracts
.
RequestMultiplePermissions
(
)
)
{
grants
->
// Check if any permissions weren't granted.
for
(
grant
in
grants
.
entries
)
{
if
(
!
grant
.
value
)
{
Toast
.
makeText
(
this
,
"Missing permission:
${
grant
.
key
}
"
,
Toast
.
LENGTH_SHORT
)
.
show
(
)
}
}
// If all granted, notify if needed.
if
(
onPermissionsGranted
!=
null
&&
grants
.
all
{
it
.
value
}
)
{
onPermissionsGranted
(
)
}
}
val
neededPermissions
=
listOf
(
Manifest
.
permission
.
RECORD_AUDIO
,
Manifest
.
permission
.
CAMERA
)
.
filter
{
ContextCompat
.
checkSelfPermission
(
this
,
it
)
==
PackageManager
.
PERMISSION_DENIED
}
.
toTypedArray
(
)
if
(
neededPermissions
.
isNotEmpty
(
)
)
{
requestPermissionLauncher
.
launch
(
neededPermissions
)
}
else
{
onPermissionsGranted
?
.
invoke
(
)
}
}
Copy
3. Join a room
Note that this example hardcodes a token we generated for you that expires in 2 hours. In a real app, you’ll need your server to generate a token for you.
// !! Note !!
// This sample hardcodes a token which expires in 2 hours.
const
val
wsURL
=
"<your LiveKit server URL>"
const
val
token
=
"<generate a token>"
// In production you should generate tokens on your server, and your frontend
// should request a token from your server.
class
MainActivity
:
ComponentActivity
(
)
{
override
fun
onCreate
(
savedInstanceState
:
Bundle
?
)
{
super
.
onCreate
(
savedInstanceState
)
requireNeededPermissions
{
setContent
{
RoomScope
(
url
=
wsURL
,
token
=
token
,
audio
=
true
,
video
=
true
,
connect
=
true
,
)
{
// Get all the tracks in the room.
val
trackRefs
=
rememberTracks
(
)
// Display the video tracks.
// Audio tracks are automatically played.
LazyColumn
(
modifier
=
Modifier
.
fillMaxSize
(
)
)
{
items
(
trackRefs
.
size
)
{
index
->
VideoTrackView
(
trackReference
=
trackRefs
[
index
]
,
modifier
=
Modifier
.
fillParentMaxHeight
(
0.5f
)
)
}
}
}
}
}
}
}
Copy
(For more details, you can reference
the complete quickstart app
.)
4. Next Steps
Set up a server to generate tokens for your app at runtime by following this guide:
Generating Tokens
.
View the
full SDK reference
and
GitHub repository
for more documentation and examples.
Happy coding!
On this page
1. Install LiveKit Components SDK
2. Permissions
3. Join a room
4. Next Steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/quickstarts/android:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
1. Install LiveKit SDK
2. Permissions
3. Connect to a Room using a VideoView
4. Next Steps
Tip
This quickstart is for Android apps using the traditional view-based system. If you are using Jetpack Compose, check out the
Compose Quickstart Guide
.
1. Install LiveKit SDK
LiveKit for Android is available as a Maven package.
...
dependencies {
implementation "io.livekit:livekit-android:<current version>"
}
Copy
See the
releases page
for information on the latest version of the SDK.
You'll also need JitPack as one of your repositories. In your
settings.gradle
file:
dependencyResolutionManagement {
repositories {
google()
mavenCentral()
//...
maven { url 'https://jitpack.io' }
}
}
Copy
2. Permissions
LiveKit relies on the
RECORD_AUDIO
and
CAMERA
permissions to use the microphone and camera.
These permission must be requested at runtime, like so:
private
fun
requestPermissions
(
)
{
val
requestPermissionLauncher
=
registerForActivityResult
(
ActivityResultContracts
.
RequestMultiplePermissions
(
)
)
{
grants
->
for
(
grant
in
grants
.
entries
)
{
if
(
!
grant
.
value
)
{
Toast
.
makeText
(
this
,
"Missing permission:
${
grant
.
key
}
"
,
Toast
.
LENGTH_SHORT
)
.
show
(
)
}
}
}
val
neededPermissions
=
listOf
(
Manifest
.
permission
.
RECORD_AUDIO
,
Manifest
.
permission
.
CAMERA
)
.
filter
{
ContextCompat
.
checkSelfPermission
(
this
,
it
)
==
PackageManager
.
PERMISSION_DENIED
}
.
toTypedArray
(
)
if
(
neededPermissions
.
isNotEmpty
(
)
)
{
requestPermissionLauncher
.
launch
(
neededPermissions
)
}
}
Copy
3. Connect to a Room using a VideoView
LiveKit uses
SurfaceViewRenderer
to render video tracks. A
TextureView
implementation is also
provided through
TextureViewRenderer
. Subscribed audio tracks are automatically played.
Note that this example hardcodes a token we generated for you that expires in 2 hours. In a real app, you’ll need your server to generate a token for you.
// !! Note !!
// This sample hardcodes a token which expires in 2 hours.
const
val
wsURL
=
"<your LiveKit server URL>"
const
val
token
=
"<generate a token>"
// In production you should generate tokens on your server, and your frontend
// should request a token from your server.
class
MainActivity
:
AppCompatActivity
(
)
{
lateinit
var
room
:
Room
override
fun
onCreate
(
savedInstanceState
:
Bundle
?
)
{
super
.
onCreate
(
savedInstanceState
)
setContentView
(
R
.
layout
.
activity_main
)
// Create Room object.
room
=
LiveKit
.
create
(
applicationContext
)
// Setup the video renderer
room
.
initVideoRenderer
(
findViewById
<
SurfaceViewRenderer
>
(
R
.
id
.
renderer
)
)
connectToRoom
(
)
}
private
fun
connectToRoom
(
)
{
lifecycleScope
.
launch
{
// Setup event handling.
launch
{
room
.
events
.
collect
{
event
->
when
(
event
)
{
is
RoomEvent
.
TrackSubscribed
->
onTrackSubscribed
(
event
)
else
->
{
}
}
}
}
// Connect to server.
room
.
connect
(
wsURL
,
token
,
)
// Publish audio/video to the room
val
localParticipant
=
room
.
localParticipant
localParticipant
.
setMicrophoneEnabled
(
true
)
localParticipant
.
setCameraEnabled
(
true
)
}
}
private
fun
onTrackSubscribed
(
event
:
RoomEvent
.
TrackSubscribed
)
{
val
track
=
event
.
track
if
(
track
is
VideoTrack
)
{
attachVideo
(
track
)
}
}
private
fun
attachVideo
(
videoTrack
:
VideoTrack
)
{
videoTrack
.
addRenderer
(
findViewById
<
SurfaceViewRenderer
>
(
R
.
id
.
renderer
)
)
findViewById
<
View
>
(
R
.
id
.
progress
)
.
visibility
=
View
.
GONE
}
}
Copy
(For more details, you can reference
the complete sample app
.)
4. Next Steps
Set up a server to generate tokens for your app at runtime by following this guide:
Generating Tokens
.
View the
full SDK reference
and
GitHub repository
for more documentation and examples.
Happy coding!
On this page
1. Install LiveKit SDK
2. Permissions
3. Connect to a Room using a VideoView
4. Next Steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/quickstarts/flutter:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
1. Install the LiveKit SDK
2. Add required permissions
2. Connect to a room, publish video & audio
3. Next Steps
1. Install the LiveKit SDK
flutter pub
add
livekit_client
Copy
2. Add required permissions
You'll need to request camera and/or microphone permissions (depending on your use case). This must be done within your platform-specific code:
Android
Web
Windows
iOS
macOS
Permissions are configured in
AppManifest.xml
. In addition to camera and microphone, you may need to add networking and bluetooth permissions.
<
uses-feature
android:
name
=
"
android.hardware.camera
"
/>
<
uses-feature
android:
name
=
"
android.hardware.camera.autofocus
"
/>
<
uses-permission
android:
name
=
"
android.permission.CAMERA
"
/>
<
uses-permission
android:
name
=
"
android.permission.RECORD_AUDIO
"
/>
<
uses-permission
android:
name
=
"
android.permission.ACCESS_NETWORK_STATE
"
/>
<
uses-permission
android:
name
=
"
android.permission.CHANGE_NETWORK_STATE
"
/>
<
uses-permission
android:
name
=
"
android.permission.MODIFY_AUDIO_SETTINGS
"
/>
<
uses-permission
android:
name
=
"
android.permission.BLUETOOTH
"
android:
maxSdkVersion
=
"
30
"
/>
<
uses-permission
android:
name
=
"
android.permission.BLUETOOTH_ADMIN
"
android:
maxSdkVersion
=
"
30
"
/>
Copy
2. Connect to a room, publish video & audio
final
roomOptions
=
RoomOptions
(
adaptiveStream
:
true
,
dynacast
:
true
,
// ... your room options
)
final
room
=
Room
(
)
;
await
room
.
connect
(
url
,
token
,
roomOptions
:
roomOptions
)
;
try
{
// video will fail when running in ios simulator
await
room
.
localParticipant
.
setCameraEnabled
(
true
)
;
}
catch
(
error
)
{
print
(
'Could not publish video, error:
$
error
'
)
;
}
await
room
.
localParticipant
.
setMicrophoneEnabled
(
true
)
;
Copy
3. Next Steps
Set up a server to generate tokens for your app at runtime by following this guide:
Generating Tokens
.
View the
full SDK reference
and
GitHub repository
for more documentation and examples.
Happy coding!
On this page
1. Install the LiveKit SDK
2. Add required permissions
2. Connect to a room, publish video & audio
3. Next Steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/quickstarts/react-native:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
1. Install LiveKit SDK
2. Connect to a room, publish video & audio
3. Create a backend server to generate tokens
3. Next Steps
1. Install LiveKit SDK
LiveKit provides a
client SDK for React Native
. Install the package and its dependency with:
npm
install
@livekit/react-native @livekit/react-native-webrtc
Copy
Note
If you're planning to integrate LiveKit into an Expo app, see the
quickstart guide for Expo instead
.
Android
Swift
This library depends on @livekit/react-native-webrtc, which has additional installation instructions for
Android
.
Once the @livekit/react-native-webrtc dependency is installed, one last step is required. In your MainApplication.java file:
import
com
.
livekit
.
reactnative
.
LiveKitReactNative
;
import
com
.
livekit
.
reactnative
.
audio
.
AudioType
;
public
class
MainApplication
extends
Application
implements
ReactApplication
{
@Override
public
void
onCreate
(
)
{
// Place this above any other RN related initialization
// When the AudioType is omitted, it'll default to CommunicationAudioType.
// Use AudioType.MediaAudioType if user is only consuming audio, and not publishing
LiveKitReactNative
.
setup
(
this
,
new
AudioType
.
CommunicationAudioType
(
)
)
;
//...
}
}
Copy
If you are using Expo, LiveKit is available on Expo through development builds.
See the instructions found here
.
Finally, in your index.js file, setup the LiveKit SDK by calling
registerGlobals()
. This sets up the required WebRTC libraries for use in Javascript, and is needed for LiveKit to work.
import
{
registerGlobals
}
from
'@livekit/react-native'
;
// ...
registerGlobals
(
)
;
Copy
2. Connect to a room, publish video & audio
import
*
as
React
from
'react'
;
import
{
StyleSheet
,
View
,
FlatList
,
ListRenderItem
,
}
from
'react-native'
;
import
{
useEffect
}
from
'react'
;
import
{
AudioSession
,
LiveKitRoom
,
useTracks
,
TrackReferenceOrPlaceholder
,
VideoTrack
,
isTrackReference
,
registerGlobals
,
}
from
'@livekit/react-native'
;
import
{
Track
}
from
'livekit-client'
;
// !! Note !!
// This sample hardcodes a token which expires in 2 hours.
const
wsURL
=
"<your LiveKit server URL>"
const
token
=
"<generate a token>"
export
default
function
App
(
)
{
// Start the audio session first.
useEffect
(
(
)
=>
{
let
start
=
async
(
)
=>
{
await
AudioSession
.
startAudioSession
(
)
;
}
;
start
(
)
;
return
(
)
=>
{
AudioSession
.
stopAudioSession
(
)
;
}
;
}
,
[
]
)
;
return
(
<
LiveKitRoom
serverUrl
=
{
wsURL
}
token
=
{
token
}
connect
=
{
true
}
options
=
{
{
// Use screen pixel density to handle screens with differing densities.
adaptiveStream
:
{
pixelDensity
:
'screen'
}
,
}
}
audio
=
{
true
}
video
=
{
true
}
>
<
RoomView
/>
</
LiveKitRoom
>
)
;
}
;
const
RoomView
=
(
)
=>
{
// Get all camera tracks.
const
tracks
=
useTracks
(
[
Track
.
Source
.
Camera
]
)
;
const
renderTrack
:
ListRenderItem
<
TrackReferenceOrPlaceholder
>
= ({item}) =>
{
// Render using the VideoTrack component.
if
(
isTrackReference
(
item
)
)
{
return
(
<
VideoTrack
trackRef
=
{
item
}
style
=
{
styles
.
participantView
}
/>
)
}
else
{
return
(
<
View
style
=
{
styles
.
participantView
}
/>
)
}
}
;
return (
<
View
style
=
{
styles
.
container
}
>
<
FlatList
data
=
{
tracks
}
renderItem
=
{
renderTrack
}
/>
</
View
>
);
};
const styles = StyleSheet.create(
{
container
:
{
flex
:
1
,
alignItems
:
'stretch'
,
justifyContent
:
'center'
,
}
,
participantView
:
{
height
:
300
,
}
,
}
);
Copy
3. Create a backend server to generate tokens
Set up a server to generate tokens for your app at runtime by following this guide:
Generating Tokens
.
3. Next Steps
Set up a server to generate tokens for your app at runtime by following this guide:
Generating Tokens
.
Visit the
GitHub repository
for more documentation and examples.
Happy coding!
On this page
1. Install LiveKit SDK
2. Connect to a room, publish video & audio
3. Create a backend server to generate tokens
3. Next Steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/quickstarts/expo:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
1. Install LiveKit SDK
2. Connect to a room, publish video & audio
3. Create a backend server to generate tokens
1. Install LiveKit SDK
LiveKit provides a
React Native SDK
and corresponding Expo config plugin. Install the packages and dependencies with:
npm
install
@livekit/react-native @livekit/react-native-expo-plugin @livekit/react-native-webrtc @config-plugins/react-native-webrtc
Copy
Note
The LiveKit SDK is not compatible with the Expo Go app due to the native code required. Using
expo-dev-client
and
building locally
will allow you to create development builds compatible with LiveKit.
In your root folder's app.json, add the expo plugins like so:
{
"expo"
:
{
"plugins"
:
[
"@livekit/react-native-expo-plugin"
,
"@config-plugins/react-native-webrtc"
]
}
}
Copy
Finally, in your App.js file, setup the LiveKit SDK by calling
registerGlobals()
. This sets up the required WebRTC libraries for use in Javascript, and is needed for LiveKit to work.
import
{
registerGlobals
}
from
'@livekit/react-native'
;
registerGlobals
(
)
;
Copy
2. Connect to a room, publish video & audio
import
*
as
React
from
'react'
;
import
{
StyleSheet
,
View
,
FlatList
,
ListRenderItem
,
}
from
'react-native'
;
import
{
useEffect
}
from
'react'
;
import
{
AudioSession
,
LiveKitRoom
,
useTracks
,
TrackReferenceOrPlaceholder
,
VideoTrack
,
isTrackReference
,
registerGlobals
,
}
from
'@livekit/react-native'
;
import
{
Track
}
from
'livekit-client'
;
registerGlobals
(
)
;
// !! Note !!
// This sample hardcodes a token which expires in 2 hours.
const
wsURL
=
"<your LiveKit server URL>"
const
token
=
"<generate a token>"
export
default
function
App
(
)
{
// Start the audio session first.
useEffect
(
(
)
=>
{
let
start
=
async
(
)
=>
{
await
AudioSession
.
startAudioSession
(
)
;
}
;
start
(
)
;
return
(
)
=>
{
AudioSession
.
stopAudioSession
(
)
;
}
;
}
,
[
]
)
;
return
(
<
LiveKitRoom
serverUrl
=
{
wsURL
}
token
=
{
token
}
connect
=
{
true
}
options
=
{
{
// Use screen pixel density to handle screens with differing densities.
adaptiveStream
:
{
pixelDensity
:
'screen'
}
,
}
}
audio
=
{
true
}
video
=
{
true
}
>
<
RoomView
/>
</
LiveKitRoom
>
)
;
}
;
const
RoomView
=
(
)
=>
{
// Get all camera tracks.
const
tracks
=
useTracks
(
[
Track
.
Source
.
Camera
]
)
;
const
renderTrack
:
ListRenderItem
<
TrackReferenceOrPlaceholder
>
= ({item}) =>
{
// Render using the VideoTrack component.
if
(
isTrackReference
(
item
)
)
{
return
(
<
VideoTrack
trackRef
=
{
item
}
style
=
{
styles
.
participantView
}
/>
)
}
else
{
return
(
<
View
style
=
{
styles
.
participantView
}
/>
)
}
}
;
return (
<
View
style
=
{
styles
.
container
}
>
<
FlatList
data
=
{
tracks
}
renderItem
=
{
renderTrack
}
/>
</
View
>
);
};
const styles = StyleSheet.create(
{
container
:
{
flex
:
1
,
alignItems
:
'stretch'
,
justifyContent
:
'center'
,
}
,
participantView
:
{
height
:
300
,
}
,
}
);
Copy
See the
quickstart example repo
for a fully configured app using Expo.
3. Create a backend server to generate tokens
Set up a server to generate tokens for your app at runtime by following this guide:
Generating Tokens
.
On this page
1. Install LiveKit SDK
2. Connect to a room, publish video & audio
3. Create a backend server to generate tokens
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/server/generating-tokens:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
1. Install LiveKit Server SDK
2. Keys and Configuration
3. Make an endpoint that returns a token
4. Create a frontend app to connect
In order for frontend apps to connect to LiveKit rooms, they need a token generated by your backend
server. In this guide, we'll walk through how to set up a server to generate tokens for your frontend.
1. Install LiveKit Server SDK
Go
Node.js
Ruby
Python
Rust
PHP
go get github.com/livekit/server-sdk-go/v2
Copy
2. Keys and Configuration
Create a new file at
development.env
and with your API Key and Secret:
export
LIVEKIT_API_KEY
=
<
your API Key
>
export
LIVEKIT_API_SECRET
=
<
your API Secret
>
Reveal API Key and Secret
Copy
3. Make an endpoint that returns a token
Create a server:
Go
Node.js
Ruby
Python
Rust
PHP
// server.go
import
(
"net/http"
"log"
"time"
"os"
"github.com/livekit/protocol/auth"
)
func
getJoinToken
(
room
,
identity
string
)
string
{
at
:=
auth
.
NewAccessToken
(
os
.
Getenv
(
"LIVEKIT_API_KEY"
)
,
os
.
Getenv
(
"LIVEKIT_API_SECRET"
)
)
grant
:=
&
auth
.
VideoGrant
{
RoomJoin
:
true
,
Room
:
room
,
}
at
.
AddGrant
(
grant
)
.
SetIdentity
(
identity
)
.
SetValidFor
(
time
.
Hour
)
token
,
_
:=
at
.
ToJWT
(
)
return
token
}
func
main
(
)
{
http
.
HandleFunc
(
"/getToken"
,
func
(
w http
.
ResponseWriter
,
r
*
http
.
Request
)
{
w
.
Write
(
[
]
byte
(
getJoinToken
(
"my-room"
,
"identity"
)
)
)
}
)
log
.
Fatal
(
http
.
ListenAndServe
(
":8080"
,
nil
)
)
}
Copy
Load the environment variables and run the server:
Go
Node.js
Ruby
Python
Rust
PHP
$
source
development.env
$ go run server.go
Copy
Note
See the
Authentication
page for more information on how to generate tokens with custom permissions.
4. Create a frontend app to connect
Create a frontend app that fetches a token from the server we just made, then uses it to connect to a LiveKit room:
iOS
Android
Flutter
React Native
React
Unity (web)
JavaScript
On this page
1. Install LiveKit Server SDK
2. Keys and Configuration
3. Make an endpoint that returns a token
4. Create a frontend app to connect
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/server/managing-rooms:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Initialize RoomServiceClient
Create a room
List rooms
Delete a room
Initialize RoomServiceClient
Room management is done with a RoomServiceClient, created like so:
Go
Python
Node.js
import
(
lksdk
"github.com/livekit/server-sdk-go"
livekit
"github.com/livekit/protocol/livekit"
)
// ...
host
:=
"https://my.livekit.host"
roomClient
:=
lksdk
.
NewRoomServiceClient
(
host
,
"api-key"
,
"secret-key"
)
Copy
Create a room
Go
Python
Node.js
LiveKit CLI
room
,
_
:=
roomClient
.
CreateRoom
(
context
.
Background
(
)
,
&
livekit
.
CreateRoomRequest
{
Name
:
"myroom"
,
EmptyTimeout
:
10
*
60
,
// 10 minutes
MaxParticipants
:
20
,
}
)
Copy
List rooms
Go
Python
Node.js
LiveKit CLI
rooms
,
_
:=
roomClient
.
ListRooms
(
context
.
Background
(
)
,
&
livekit
.
ListRoomsRequest
{
}
)
Copy
Delete a room
Deleting a room causes all Participants to be disconnected.
Go
Python
Node.js
LiveKit CLI
_
,
_
=
roomClient
.
DeleteRoom
(
context
.
Background
(
)
,
&
livekit
.
DeleteRoomRequest
{
Room
:
"myroom"
,
}
)
Copy
On this page
Initialize RoomServiceClient
Create a room
List rooms
Delete a room
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/server/managing-participants:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Initialize RoomServiceClient
List Participants
Get details on a Participant
Updating permissions
Updating metadata
Remove a Participant
Mute/unmute a Participant's Track
Initialize RoomServiceClient
Participant management is done with a RoomServiceClient, created like so:
Go
Python
Node.js
import
(
lksdk
"github.com/livekit/server-sdk-go"
livekit
"github.com/livekit/protocol/livekit"
)
// ...
host
:=
"https://my.livekit.host"
roomClient
:=
lksdk
.
NewRoomServiceClient
(
host
,
"api-key"
,
"secret-key"
)
Copy
List Participants
Go
Python
Node.js
LiveKit CLI
res
,
err
:=
roomClient
.
ListParticipants
(
context
.
Background
(
)
,
&
livekit
.
ListParticipantsRequest
{
Room
:
roomName
,
}
)
Copy
Get details on a Participant
Go
Python
Node.js
LiveKit CLI
res
,
err
:=
roomClient
.
GetParticipant
(
context
.
Background
(
)
,
&
livekit
.
RoomParticipantIdentity
{
Room
:
roomName
,
Identity
:
identity
,
}
)
Copy
Updating permissions
You can modify a participant's permissions on-the-fly using
UpdateParticipant
.
When there's a change in permissions, connected clients will be notified through the
ParticipantPermissionChanged
event.
This comes in handy, for instance, when transitioning an audience member to a speaker role within a room.
Note that if you revoke the
CanPublish
permission from a participant, all tracks they've published will be automatically unpublished.
Go
Python
Node.js
LiveKit CLI
// Promotes an audience member to a speaker
res
,
err
:=
c
.
UpdateParticipant
(
context
.
Background
(
)
,
&
livekit
.
UpdateParticipantRequest
{
Room
:
roomName
,
Identity
:
identity
,
Permission
:
&
livekit
.
ParticipantPermission
{
CanSubscribe
:
true
,
CanPublish
:
true
,
CanPublishData
:
true
,
}
,
}
)
// ...and later move them back to audience
res
,
err
:=
c
.
UpdateParticipant
(
context
.
Background
(
)
,
&
livekit
.
UpdateParticipantRequest
{
Room
:
roomName
,
Identity
:
identity
,
Permission
:
&
livekit
.
ParticipantPermission
{
CanSubscribe
:
true
,
CanPublish
:
false
,
CanPublishData
:
true
,
}
,
}
)
Copy
Updating metadata
You can modify a Participant's metadata whenever necessary. Once changed, connected
clients will receive a
ParticipantMetadataChanged
event.
Go
Python
Node.js
LiveKit CLI
data
,
err
:=
json
.
Marshal
(
values
)
_
,
err
=
c
.
UpdateParticipant
(
context
.
Background
(
)
,
&
livekit
.
UpdateParticipantRequest
{
Room
:
roomName
,
Identity
:
identity
,
Metadata
:
string
(
data
)
,
}
)
Copy
Remove a Participant
RemoteParticipant
will forcibly disconnect the participant from the room. However, this action doesn't invalidate the participant's token.
To prevent the participant from rejoining the same room, consider the following measures:
Generate access tokens with a short TTL (Time-To-Live).
Refrain from providing a new token to the same participant via your application's backend.
Go
Python
Node.js
LiveKit CLI
res
,
err
:=
roomClient
.
RemoveParticipant
(
context
.
Background
(
)
,
&
livekit
.
RoomParticipantIdentity
{
Room
:
roomName
,
Identity
:
identity
,
}
)
Copy
Mute/unmute a Participant's Track
To mute a particular Track from a Participant, first get the TrackSid from
GetParticipant
(above), then call
MutePublishedTrack
:
Go
Python
Node.js
LiveKit CLI
res
,
err
:=
roomClient
.
MutePublishedTrack
(
context
.
Background
(
)
,
&
livekit
.
MuteRoomTrackRequest
{
Room
:
roomName
,
Identity
:
identity
,
TrackSid
:
"track_sid"
,
Muted
:
true
,
}
)
Copy
You may also unmute the track by setting
muted
to
false
.
Note
Being remotely unmuted can catch users by surprise, so it's disabled by default.
To allow remote unmute, select the
Admins can remotely unmute tracks
option in your
project settings
.
If you're self-hosting, configure
room.enable_remote_unmute: true
in your config YAML.
On this page
Initialize RoomServiceClient
List Participants
Get details on a Participant
Updating permissions
Updating metadata
Remove a Participant
Mute/unmute a Participant's Track
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/server/webhooks:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Receiving webhooks
Delivery and retries
Events
Room Started
Room Finished
Participant Joined
Participant Left
Track Published
Track Unpublished
Egress Started
Egress Updated
Egress Ended
Ingress Started
Ingress Ended
Overview
LiveKit can be configured to notify your server when room events take place. This can be helpful for your backend to know when a room has finished, or when a participant leaves.
With Cloud, webhooks can be configured in the Settings section of your project's dashboard.
When self-hosting, webhooks can be enabled by setting the
webhook
section in your config.
For Egress, extra webhooks can also be
configured inside Egress requests
.
webhook
:
# The API key to use in order to sign the message
# This must match one of the keys LiveKit is configured with
api_key
:
'api-key-to-sign-with'
urls
:
-
'https://yourhost'
Copy
Receiving webhooks
Webhook requests are HTTP POST requests sent to URLs that you specify in your config or Cloud dashboard. A
WebhookEvent
is encoded as JSON and sent in the body of the request.
The
Content-Type
header of the request is set to
application/webhook+json
. Please ensure your webserver is configured to receive payloads with this content type.
In order to ensure webhook requests are coming from LiveKit, these requests have an
Authorization
header containing a signed JWT token. The token includes a sha256 hash of the payload.
LiveKit's server SDKs provide webhook receiver libraries which should help with validation and decoding of the payload.
Node.js
Go
Java
import
{
WebhookReceiver
}
from
'livekit-server-sdk'
;
const
receiver
=
new
WebhookReceiver
(
'apikey'
,
'apisecret'
)
;
// In order to use the validator, WebhookReceiver must have access to the raw
// POSTed string (instead of a parsed JSON object). If you are using express
// middleware, ensure that `express.raw` is used for the webhook endpoint
// app.use(express.raw({type: 'application/webhook+json'}));
app
.
post
(
'/webhook-endpoint'
,
async
(
req
,
res
)
=>
{
// Event is a WebhookEvent object
const
event
=
await
receiver
.
receive
(
req
.
body
,
req
.
get
(
'Authorization'
)
)
;
}
)
;
Copy
Delivery and retries
Webhooks are HTTP requests initiated by LiveKit and sent to your backend. Due to the protocol's push-based nature, there are no guarantees around delivery.
LiveKit aims to mitigate transient failures by retrying a webhook request multiple times. Each message will undergo several delivery attempts before being abandoned. If multiple events are queued for delivery, LiveKit will properly sequence them; only delivering newer events after older ones have been delivered or abandoned.
Events
In addition to the fields below, all webhook events will include the following fields:
id
- a UUID identifying the event
createdAt
- UNIX timestamp in seconds
Room Started
interface
WebhookEvent
{
event
:
'room_started'
;
room
:
Room
;
}
Copy
Room Finished
interface
WebhookEvent
{
event
:
'room_finished'
;
room
:
Room
;
}
Copy
Participant Joined
interface
WebhookEvent
{
event
:
'participant_joined'
;
room
:
Room
;
participant
:
ParticipantInfo
;
}
Copy
Participant Left
interface
WebhookEvent
{
event
:
'participant_left'
;
room
:
Room
;
participant
:
ParticipantInfo
;
}
Copy
Track Published
In the Room and Participant objects, only sid, identity, and name are sent.
interface
WebhookEvent
{
event
:
'track_published'
;
room
:
Room
;
participant
:
ParticipantInfo
;
track
:
TrackInfo
;
}
Copy
Track Unpublished
In the Room and Participant objects, only sid, identity, and name are sent.
interface
WebhookEvent
{
event
:
'track_unpublished'
;
room
:
Room
;
participant
:
ParticipantInfo
;
track
:
TrackInfo
;
}
Copy
Egress Started
interface
WebhookEvent
{
event
:
'egress_started'
;
egressInfo
:
EgressInfo
;
}
Copy
Egress Updated
interface
WebhookEvent
{
event
:
'egress_updated'
;
egressInfo
:
EgressInfo
;
}
Copy
Egress Ended
interface
WebhookEvent
{
event
:
'egress_ended'
;
egressInfo
:
EgressInfo
;
}
Copy
Ingress Started
interface
WebhookEvent
{
event
:
'ingress_started'
;
ingressInfo
:
IngressInfo
;
}
Copy
Ingress Ended
interface
WebhookEvent
{
event
:
'ingress_ended'
;
ingressInfo
:
IngressInfo
;
}
Copy
On this page
Overview
Receiving webhooks
Delivery and retries
Events
Room Started
Room Finished
Participant Joined
Participant Left
Track Published
Track Unpublished
Egress Started
Egress Updated
Egress Ended
Ingress Started
Ingress Ended
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/egress/composite-recording:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Composite recording
RoomComposite Egress
Default layouts
Output options
Audio-only composite
Web Egress
Examples
Composite recording
Composite recordings use a web-based recorder to capture a composited view of a room, including all participants, interactions, and any customized UI elements from the application.
We provide two options for composite recording:
RoomComposite
: A composite recording that is tied to a room's lifecycle. When all of the participants leave the room, the recording would stop automatically.
Web
: A standalone composite recording can be started and stopped independently of a room’s lifecycle. Web Egress can be used to record any web-based content, even if it’s not part of a LiveKit room.
RoomComposite Egress
One common requirement when recording a room is to capture all of the participants and interactions that take place.
This can be challenging in a multi-user application, where different users may be joining, leaving, or turning their cameras on and off.
It may also be desirable for the recording to look as close to the actual application experience as possible, capturing the richness and interactivity of your application.
A
RoomComposite
Egress uses a web app to create the composited view, rendering the output with an instance of headless Chromium.
In most cases, your existing LiveKit application can be used as a compositing template with few modifications.
Default layouts
We provide a few default compositing layouts that works out of the box. They'll be used
by default if a custom template URL is not passed in. These templates are deployed
alongside and served by the Egress service (
source
).
While it's a great starting point, you can easily
create your own layout
using standard web technologies that you are already familiar with.
Layout
Preview
grid
speaker
single-speaker
Additionally, you can use a
-light
suffix to change background color to white. i.e.
grid-light
.
Output options
Composite recordings can output to a wide variety of formats and destinations.
The options are described in detail in
Output options
.
Audio-only composite
If your application is audio-only, you can export a mixed audio file containing audio from all participants in the room.
To start an audio-only composite, pass
audio_only=true
when starting an Egress.
Web Egress
Web egress allows you to record or stream any website. Similar to room composite egress, it uses headless Chromium to render output.
Unlike room composite egress, you can supply any url, and the lifecycle of web egress is not attached to a LiveKit room.
Examples
For examples on using composite recordings, see
Egress examples
.
On this page
Composite recording
RoomComposite Egress
Default layouts
Output options
Audio-only composite
Web Egress
Examples
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/egress/participant:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Participant Egress
TrackComposite Egress
Examples
Some use cases require participants to be recorded individually instead of compositing them. LiveKit offers two options for recording participants individually. Both options support a wide range of
output options
.
See the
Egress examples
page for example usage.
Participant Egress
Participant Egress allows you to record a participant's audio and video tracks by providing the participant's identity. Participant Egress is designed to simplify the workflow of recording participants in a realtime session, and handles the changes in track state, such as when a track is muted.
When a Participant Egress is requested, the Egress service joins the room and waits for the participant to join and publish tracks. Recording begins as soon as either audio or video tracks are published. The service automatically handles muted or unpublished tracks and stops recording when the participant leaves the room.
You can also record a participant’s screen share along with the screen share's audio. To enable this, pass
screen_share=true
when starting the Egress. The Egress service will identify tracks based on their
source
setting.
TrackComposite Egress
TrackComposite combines an audio and video track for output, as the name suggests.
It’s a more advanced version of Participant Egress, allowing you to specify which tracks to record — useful when precise control over track IDs is needed.
One key difference with TrackComposite is that tracks must be published before starting the Egress. As a result, there may be a slight delay between when the track is published and when recording begins.
Examples
For examples on using Participant or Track Composite Egress, please reference
Egress examples
.
On this page
Participant Egress
TrackComposite Egress
Examples
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/egress/track:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Example
Stream audio to WebSocket
Overview
TrackEgress is the simplest way to export individual tracks to cloud storage or a server via WebSocket.
Tracks are exported as is, without transcoding. The following containers will be used depending on track codec:
H.264: MP4
VP8: WebM
Opus: Ogg
Note: Track Egress only exports one track, either video or audio. If you want to export video and audio together, use
Track Composite Egress
.
Example
See an example of exporting to Azure Blob Storage on the
example page
.
Stream audio to WebSocket
You can add custom stream processing by starting a TrackEgress to your WebSocket server. This will give you a realtime streaming export of your audio tracks. (WebSocket streaming is only available for audio tracks).
The tracks will be exported as raw PCM data. This format is compatible with most transcription services.
Format:
pcm_s16le
Content-Type:
audio/x-raw
Sample rate: matches incoming, typically 48kHz
When a
TrackEgressRequest
is started with a WebSocket URL, we'll initiate a WebSocket session to the designated URL. We recommend using query parameters in the URL in order to help you identify the track.
For example:
wss://your-server.com/egress?trackID=<trackID>&participant=<participantIdentity>
We'll send a combination of binary and text frames. Binary frames would contain audio data. Text frames will contain end user events on the tracks. For example: if the
track was muted, you will receive the following:
{
"muted"
:
true
}
Copy
And when unmuted:
{
"muted"
:
false
}
Copy
The WebSocket connection will terminate when the track is unpublished (or if the participant leaves the room).
JavaScript
Go
Ruby
LiveKit CLI
const
info
=
await
egressClient
.
startTrackEgress
(
'my-room'
,
'wss://my-websocket-server.com'
,
trackID
,
)
;
const
egressID
=
info
.
egressId
;
Copy
On this page
Overview
Example
Stream audio to WebSocket
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/egress/outputs:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Supported Outputs
Composite and Participant Egress Outputs
RTMP/SRT Streaming
File/Segment outputs
Image output
Cloud storage configurations
S3
Google Cloud Storage
Azure
Supported Outputs
Egress Type
Transcoded
Pass-through (mp4, webm, ogg)
HLS Segments
RTMP stream
SRT stream
WebSocket stream
Room Composite
✅
✅
✅
✅
Web
✅
✅
✅
✅
Participant
✅
✅
✅
✅
Track Composite
✅
✅
✅
✅
Track
✅
✅ (audio-only)
Note
A very long-running Egress may hit our
Egress time limits
.
Composite and Participant Egress Outputs
Since Composite and Participant Egress are transcoded, they can be output to a wide range of formats and destinations.
Egress is optimized to transcode once while sending output to multiple destinations.
For example, from the same Egress you may simultaneously:
Stream to one or more RTMP endpoints.
Record as HLS.
Record as MP4.
Generate thumbnails.
When creating a new Egress, set one or more of the following configuration fields:
Field
Description
file_outputs
Record to a MP4 file.
stream_outputs
Stream to RTMP or SRT server.
segment_outputs
Record as HLS segments.
image_outputs
Generate thumbnails.
Note
While each output type is a list (
*_outputs
), Egress supports only a single item per type. i.e. It's not possible to output to two different files, but it is possible to output to both a
file
and a HLS
segment
.
LiveKit CLI
JavaScript
Go
Ruby
Python
Java
{
...
// source details
"file_outputs"
:
[
{
"filepath"
:
"my-test-file.mp4"
,
"s3"
:
{
...
}
,
"gcp"
:
{
...
}
,
"azure"
:
{
...
}
,
"aliOSS"
:
{
...
}
}
]
,
"stream_outputs"
:
[
{
"protocol"
:
"rtmp"
,
"urls"
:
[
"rtmp://my-rtmp-endpoint/path/stream-key"
]
}
]
,
"segment_outputs"
:
[
{
"filename_prefix"
:
"my-output"
,
"playlist_name"
:
"my-output.m3u8"
,
// when provided, we'll generate a playlist containing only the last few segments
"live_playlist_name"
:
"my-output-live.m3u8"
,
"segment_duration"
:
2
,
"s3"
:
{
...
}
,
"gcp"
:
{
...
}
,
"azure"
:
{
...
}
,
"aliOSS"
:
{
...
}
}
]
,
"image_outputs"
:
[
{
"capture_interval"
:
5
,
"filename_prefix"
:
"my-image"
,
"filename_suffix"
:
"IMAGE_SUFFIX_INDEX"
,
"s3"
:
{
...
}
,
"gcp"
:
{
...
}
,
"azure"
:
{
...
}
,
"aliOSS"
:
{
...
}
}
]
}
Copy
RTMP/SRT Streaming
Choosing RTMP ingest endpoints
RTMP streams do not perform well over long distances. Some stream providers include a region or location as part of your
stream url, while others might use region-based routing.
When self-hosting, choose stream endpoints that are close to where your Egress servers are deployed.
With Cloud Egress, we will route your Egress request to a server closest to your RTMP endpoints.
Adding streams to non-streaming egress
Streams can be added and removed on the fly using the
UpdateStream API
.
To use the UpdateStream API, your initial request must include a
StreamOutput
. If the stream will start later, include a
StreamOutput
in the initial request with the correct
protocol
and an empty
urls
array.
Integration with Mux
Mux is LiveKit's preferred partner for HLS streaming. To start a
Mux
stream, all you need is your stream key. You can then use
mux://<stream_key>
as
a url in your
StreamOutput
.
File/Segment outputs
Filename templating
When outputing to files, the
filepath
and
filename_prefix
fields support templated variables. The below templates can be used in request filename/filepath parameters:
Egress Type
{room_id}
{room_name}
{time}
{publisher_identity}
{track_id}
{track_type}
{track_source}
Room Composite
✔️
✔️
✔️
Web
✔️
Participant
✔️
✔️
✔️
✔️
Track Composite
✔️
✔️
✔️
✔️
Track
✔️
✔️
✔️
✔️
✔️
✔️
✔️
If no filename is provided with a request, one will be generated in the form of
"{room_name}-{time}"
.
If your filename ends with a
/
, a file will be generated in that directory.
If your filename is missing an extension or includes the wrong extension, the correct one will be added.
Examples:
Request filename
Output filename
""
testroom-2022-10-04T011306.mp4
"livekit-recordings/"
livekit-recordings/testroom-2022-10-04T011306.mp4
"{room_name}/{time}"
testroom/2022-10-04T011306.mp4
"{room_id}-{publisher_identity}.mp4"
10719607-f7b0-4d82-afe1-06b77e91fe12-david.mp4
"{track_type}-{track_source}-{track_id}"
audio-microphone-TR_SKasdXCVgHsei.ogg
Image output
Image output allows you to create periodic snapshots from a recording or stream, useful for generating thumbnails or running moderation workflows in your application.
The configuration options are:
Field
Description
capture_interval
The interval in seconds between each snapshot.
filename_prefix
The prefix for each image file.
filename_suffix
The suffix for each image file. This can be a timestamp or an index.
width
and
height
The dimensions of the image. If not provided, the image is the same size as the video frame.
Cloud storage configurations
S3
Egress supports any S3-compatible storage provider, including the following:
MinIO
Oracle Cloud
CloudFlare R2
Digital Ocean
Akamai Linode
Backblaze
When using non-AWS storage, set
force_path_style
to
true
. This ensures the bucket name is used in the path, rather than as a subdomain.
Configuration fields:
Field
Description
access_key
The access key for your S3 account.
secret
The secret key for your S3 account.
region
The region where your S3 bucket is located (required when
endpoint
is not set).
bucket
The name of the bucket where the file will be stored.
endpoint
The endpoint for your S3-compatible storage provider (optional). Must start with
https://
.
metadata
Key/value pair to set as S3 metadata.
content_disposition
Content-Disposition header when the file is downloaded.
proxy
HTTP proxy to use when uploading files. {url: "", username: "", password: ""}.
Note
If the
endpoint
field is left empty, it uses AWS's regional endpoints. The
region
field is required when
endpoint
is not set.
Google Cloud Storage
For Egress to upload to Google Cloud Storage, you'll need to provide credentials in JSON.
This can be obtained by first creating a
service account
that has permissions to create storage objects (i.e.
Storage Object Creator
).
Then
create a key
for that account and export as a JSON file.
We'll refer to this file as
credentials.json
.
Configuration fields:
Field
Description
credentials
Service account credentials serialized in a JSON file named
credentials.json
.
bucket
The name of the bucket where the file will be stored.
proxy
HTTP proxy to use when uploading files. {url: "", username: "", password: ""}.
Azure
In order to upload to Azure Blob Storage, you'll need the account's shared access key.
Configuration fields:
Field
Description
account_name
The name of the Azure account.
account_key
The shared access key for the Azure account.
container_name
The name of the container where the file will be stored.
On this page
Supported Outputs
Composite and Participant Egress Outputs
RTMP/SRT Streaming
File/Segment outputs
Image output
Cloud storage configurations
S3
Google Cloud Storage
Azure
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/egress/autoegress:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Start recordings automatically
Examples
Automatically record all tracks to S3
Record each room to HLS on GCP
Start recordings automatically
Sometimes it's desirable to record every track published to the room, or to start recording the room as soon as it's created.
Autoegress is designed to simplify these workflows. When a room is created with
CreateRoom
, you can set the
egress
field to have it automatically record the room as a composite as well as each published track separately.
Examples
Automatically record all tracks to S3
curl
-X
POST
<
your-host
>
/twirp/livekit.RoomService/CreateRoom
\
-H
"Authorization: Bearer <token-with-roomCreate>"
\
-H
'Content-Type: application/json'
\
--data-binary @-
<<
EOF
{
"name": "my-room",
"egress": {
"tracks": {
"filepath": "bucket-path/{room_name}-{publisher_identity}-{time}"
"s3": {
"access_key": "",
"secret": "",
"bucket": "mybucket",
"region": "",
}
}
}
}
EOF
Copy
Record each room to HLS on GCP
curl
-X
POST
<
your-host
>
/twirp/livekit.RoomService/CreateRoom
\
-H
"Authorization: Bearer <token-with-roomCreate>"
\
-H
'Content-Type: application/json'
\
--data-binary @-
<<
EOF
{
"name": "my-room",
"egress": {
"room": {
"customBaseUrl": "https://your-template-url"
"segments": {
"filename_prefix": "path-in-bucket/myfile",
"segment_duration": 3,
"gcp": {
"credentials": "<json-encoded-credentials>",
"bucket": "mybucket"
}
}
}
}
}
EOF
Copy
On this page
Start recordings automatically
Examples
Automatically record all tracks to S3
Record each room to HLS on GCP
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/egress/custom-template:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Built-in LiveKit Recording View
Building a Custom Recording View
Building your Application
Deploying your Application
Testing Your Application
Using the Custom Recording View in Production
Recording Process
Overview
LiveKit
Room Composite Egress
enables recording of all participants' tracks in a room. This document explains its functionality and customization options.
Built-in LiveKit Recording View
The recording feature in LiveKit is built on a web-based architecture, utilizing a headless Chrome instance to render and capture output. The default view is built using LiveKit's
React Components
. There are a handful of configuration options available including:
layout
to control how the participants are arranged in the view. (You can set or change the layout using either
StartRoomCompositeEgress()
or
UpdateLayout()
.)
Encoding options
to control the quality of the audio and/or video captured
For more advanced customization, LiveKit supports configuring the URL of the web application that will generate the page to be recorded, allowing full customization of the recording view.
Building a Custom Recording View
While you can use any web framework, it's often easiest to start with the built-in React-based application and modify it to meet your requirements. The source code can be found in the
template-default
folder
of the
LiveKit egress repository
. The main files include:
Room.tsx
: the main component that renders the recording view
SpeakerLayout.tsx
,
SingleSpeakerLayout.tsx
: components used for the
speaker
and
single-speaker
layouts
App.tsx
,
index.tsx
: the main entry points for the application
App.css
,
index.css
: the CSS files for the application
Note
The built-in
Room.tsx
component uses the
template SDK
, for common tasks like:
Retrieving query string arguments (Example:
App.tsx
)
Starting a recording (Example:
Room.tsx
)
Ending a recording (Example:
EgressHelper.setRoom()
)
If you are not using
Room.tsx
as a starting point, be sure to leverage the template SDK to handle these and other common tasks.
Building your Application
Make a copy of the above files and modify tnem to meet your requirements.
Example: Move non-speaking participants to the right side of the Speaker view
By default the
Speaker
view shows the non-speaking participants on the left and the speaker on the right. Let's change this so the speaker is on the left and the non-speaking participants are on the right.
Copy the default components and CSS files into a new location
Modify
SpeakerLayout.tsx
to move the
FocusLayout
above
CarouselLayout
so it looks like this:
return
(
<
div
className
=
"
lk-focus-layout
"
>
<
FocusLayout
trackRef
=
{
mainTrack
as
TrackReference
}
/>
<
CarouselLayout
tracks
=
{
remainingTracks
}
>
<
ParticipantTile
/>
</
CarouselLayout
>
</
div
>
)
;
Copy
Modify
App.css
to fix the
grid-template-columns
value (reverse the values). It should look like this:
.lk-focus-layout
{
height
:
100
%
;
grid-template-columns
:
5
fr
1
fr
;
}
Copy
Deploying your Application
Once your app is ready for testing or deployment, you'll need to host it on a web server. We recommend using
Vercel
for its simplicity, but many other options are available.
Testing Your Application
We have created the
egress test-egress-template
subcommand in the
LiveKit CLI
to make it easier to test.
The
egress test-egress-template
subcommand:
Creates a room
Adds the desired number of virtual publishers who will publish simulated video streams
Opens a browser instance to your app URL with the correct parameters
Once you have your application deployed, you can use this command to test it out.
Usage
export
LIVEKIT_API_SECRET
=
SECRET
export
LIVEKIT_API_KEY
=
KEY
export
LIVEKIT_URL
=
YOUR_LIVEKIT_URL
lk egress test-template
\
--base-url YOUR_WEB_SERVER_URL
\
--room
ROOM_NAME
\
--layout
LAYOUT
\
--publishers
PUBLISHER_COUNT
Copy
This command launches a browser and opens:
YOUR_WEB_SERVER_URL?url=<LIVEKIT_INSTANCE WSS URL>&token=<RECORDER TOKEN>&layout=LAYOUT
Example
export
LIVEKIT_API_SECRET
=
SECRET
export
LIVEKIT_API_KEY
=
KEY
export
LIVEKIT_URL
=
YOUR_LIVEKIT_URL
lk egress test-template
\
--base-url http://localhost:3000/lk-recording-view
\
--room
my-room
\
--layout
grid
\
--publishers
3
Copy
This command launches a browser and opens:
http://localhost:3000/lk-recording-view?url=wss%3A%2F%2Ftest-1234567890.livekit.cloud&token=<RECORDER TOKEN>&layout=grid
Using the Custom Recording View in Production
Set the
custom_base_url
parameter on the
StartRoomCompositeEgress()
API to the URL where your custom recording application is deployed.
For additional authentication, most customers attach URL parameters to the
custom_base_url
. For example:
https://your-template-url.example.com/?yourparam={auth_info}
(and set this as your
custom_base_url
).
Recording Process
For those curious about the workflow of a recording, the basic steps are:
The
Egress.StartRoomCompositeEgress()
API is invoked
LiveKit assigns an available egress instance to handle the request
The egress recorder creates necessary connection & authentication details
A URL for the rendering web page is constructed with these parameters:
url
: URL of LiveKit Server
token
: Access token for joining the room as a recorder
layout
: Desired layout passed to
StartRoomCompositeEgress()
The egress recorder launches a headless Chrome instance with the constructed URL
The recorder waits for the web page to log
START_RECORDING
to the console
The recording begins
The recorder waits for the web page to log
END_RECORDING
to the console
The recording is terminated
On this page
Overview
Built-in LiveKit Recording View
Building a Custom Recording View
Building your Application
Deploying your Application
Testing Your Application
Using the Custom Recording View in Production
Recording Process
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/egress/api:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
API
StartRoomCompositeEgress
StartTrackCompositeEgress
StartTrackEgress
StartWebEgress
UpdateLayout
UpdateStream
ListEgress
StopEgress
Types
EncodedFileOutput
DirectFileOutput
SegmentedFileOutput
StreamOutput
ImageOutput
S3Upload
GCPUpload
AzureBlobUpload
AliOSSUpload
EncodingOptions
EncodingOptionsPreset
ProxyConfig
WebhookConfig
API
The Egress API is available within our server SDKs and CLI:
Go Egress Client
JS Egress Client
Ruby Egress Client
Python Egress Client
Java Egress Client
CLI
Important
Requests to the Egress API need the
roomRecord
permission on the
access token
.
You can also use
curl
to interact with the Egress APIs. To do so,
POST
the arguments in JSON format to:
https://<your-livekit-host>/twirp/livekit.Egress/<MethodName>
For example:
%
curl
-X
POST https://
<
your-livekit-host
>
/twirp/livekit.Egress/StartRoomCompositeEgress
\
-H
'Authorization: Bearer <livekit-access-token>'
\
-H
'Content-Type: application/json'
\
-d
'{"room_name": "your-room", "segments": {"filename_prefix": "your-hls-playlist.m3u8", "s3": {"access_key": "<key>", "secret": "<secret>", "bucket": "<bucket>", "region": "<bucket-region>"}}}'
Copy
{
"egress_id"
:
"EG_MU4QwhXUhWf9"
,
"room_id"
:
"<room-id>"
,
"room_name"
:
"your-room"
,
"status"
:
"EGRESS_STARTING"
..
.
}
Copy
Tip
All RPC definitions and options can be found
here
.
StartRoomCompositeEgress
Starts a new
Composite Recording
using a web browser as the rendering engine.
Parameter
Type
Required
Description
room_name
string
yes
name of room to record
layout
string
layout parameter that is passed to the template
audio_only
bool
true if resulting output should only contain audio
video_only
bool
true if resulting output should only contain video
custom_base_url
string
URL to the page that would composite tracks, uses embedded templates if left blank
file_outputs
EncodedFileOutput
[]
output to MP4 file. currently only supports a single entry
segment_outputs
SegmentedFileOutput
[]
output to HLS segments. currently only supports a single entry
stream_outputs
StreamOutput
[]
output to a stream. currently only supports a single entry, though it could includ multiple destination URLs
image_outputs
ImageOutput
[]
output to a succession of snapshot images taken at a given interval (thumbnails). Currently only supports a single entry.
preset
EncodingOptionsPreset
encoding preset to use. only one of preset or advanced could be set
advanced
EncodingOptions
advanced encoding options. only one of preset or advanced could be set
webhooks
WebhookConfig
[]
extra webhooks to send on egress events for this request
StartTrackCompositeEgress
Starts a new
Track Composite
Parameter
Type
Required
Description
room_name
string
yes
name of room to record
audio_track_id
string
ID of audio track to composite
video_track_id
string
ID of video track to composite
file_outputs
EncodedFileOutput
[]
output to MP4 file. currently only supports a single entry
segment_outputs
SegmentedFileOutput
[]
output to HLS segments. currently only supports a single entry
stream_outputs
StreamOutput
[]
output to a stream. currently only supports a single entry, though it could includ multiple destination URLs
image_outputs
ImageOutput
[]
output to a succession of snapshot images taken at a given interval (thumbnails). Currently only supports a single entry.
preset
EncodingOptionsPreset
encoding preset to use. only one of preset or advanced could be set
advanced
EncodingOptions
advanced encoding options. only one of preset or advanced could be set
webhooks
WebhookConfig
[]
extra webhooks to send on egress events for this request
StartTrackEgress
Starts a new
Track Egress
Parameter
Type
Required
Description
room_name
string
yes
name of room to record
track_id
string
layout parameter that is passed to the template
file
DirectFileOutput
only one of file or websocket_url can be set
websocket_url
string
url to websocket to receive audio output. only one of file or websocket_url can be set
webhooks
WebhookConfig
[]
extra webhooks to send on egress events for this request
StartWebEgress
Starts a new
Web Egress
Parameter
Type
Required
Description
url
string
yes
URL of the web page to record
audio_only
bool
true if resulting output should only contain audio
video_only
bool
true if resulting output should only contain video
file_outputs
EncodedFileOutput
[]
output to MP4 file. currently only supports a single entry
segment_outputs
SegmentedFileOutput
[]
output to HLS segments. currently only supports a single entry
stream_outputs
StreamOutput
[]
output to a stream. currently only supports a single entry, though it could includ multiple destination URLs
image_outputs
ImageOutput
[]
output to a succession of snapshot images taken at a given interval (thumbnails). Currently only supports a single entry.
preset
EncodingOptionsPreset
encoding preset to use. only one of preset or advanced could be set
advanced
EncodingOptions
advanced encoding options. only one of preset or advanced could be set
webhooks
WebhookConfig
[]
extra webhooks to send on egress events for this request
UpdateLayout
Used to change the web layout on an active RoomCompositeEgress.
Parameter
Type
Required
Description
egress_id
string
yes
Egress ID to update
layout
string
yes
layout to update to
JavaScript
Go
Ruby
Java
LiveKit CLI
const
info
=
await
egressClient
.
updateLayout
(
egressID
,
'grid-light'
)
;
Copy
UpdateStream
Used to add or remove stream urls from an active stream
Note: you can only add outputs to an Egress that was started with
stream_outputs
set.
Parameter
Type
Required
Description
egress_id
string
yes
Egress ID to update
add_output_urls
string[]
URLs to add to the egress as output destinations
remove_output_urls
string[]
URLs to remove from the egress
JavaScript
Go
Ruby
Java
LiveKit CLI
const
streamOutput
=
new
StreamOutput
(
{
protocol
:
StreamProtocol
.
RTMP
,
urls
:
[
'rtmp://live.twitch.tv/app/<stream-key>'
]
,
}
)
;
var
info
=
await
egressClient
.
startRoomCompositeEgress
(
'my-room'
,
{
stream
:
streamOutput
}
)
;
const
streamEgressID
=
info
.
egressId
;
info
=
await
egressClient
.
updateStream
(
streamEgressID
,
[
'rtmp://a.rtmp.youtube.com/live2/stream-key'
,
]
)
;
Copy
ListEgress
Used to list active egress. Does not include completed egress.
JavaScript
Go
Ruby
Java
LiveKit CLI
const
res
=
await
egressClient
.
listEgress
(
)
;
Copy
StopEgress
Stops an active egress.
JavaScript
Go
Ruby
Java
LiveKit CLI
const
info
=
await
egressClient
.
stopEgress
(
egressID
)
;
Copy
Types
EncodedFileOutput
Field
Type
Description
filepath
string
default {room_name}-{time}
disable_manifest
bool
by default, Egress outputs a {filepath}.json with metadata of the file
s3
S3Upload
set if uploading to S3 compatible storage. only one storage output can be set
gcp
GCPUpload
set if uploading to GCP
azure
AzureBlobUpload
set if uploading to Azure
aliOSS
AliOSSUpload
set if uploading to AliOSS
DirectFileOutput
Field
Type
Description
filepath
string
default {track_id}-{time}
disable_manifest
bool
by default, Egress outputs a {filepath}.json with metadata of the file
s3
S3Upload
set if uploading to S3 compatible storage. only one storage output can be set
gcp
GCPUpload
set if uploading to GCP
azure
AzureBlobUpload
set if uploading to Azure
aliOSS
AliOSSUpload
set if uploading to AliOSS
SegmentedFileOutput
Field
Type
Description
filename_prefix
string
prefix used in each segment (include any paths here)
playlist_name
string
name of the m3u8 playlist. when empty, matches filename_prefix
segment_duration
uint32
length of each segment (defaults to 4s)
filename_suffix
SegmentedFileSuffix
INDEX (1, 2, 3) or TIMESTAMP (in UTC)
disable_manifest
bool
s3
S3Upload
set if uploading to S3 compatible storage. only one storage output can be set
gcp
GCPUpload
set if uploading to GCP
azure
AzureBlobUpload
set if uploading to Azure
aliOSS
AliOSSUpload
set if uploading to AliOSS
StreamOutput
Field
Type
Description
protocol
SreamProtocol
(optional) only RTMP is supported
urls
string[]
list of URLs to send stream to
ImageOutput
Field
Type
Description
capture_interval
uint32
time in seconds between each snapshot
width
int32
width of the snapshot images (optional, the original width will be used if not provided)
height
int32
height of the snapshot images (optional, the original width will be used if not provided)
filename_prefix
string
prefix used in each image filename (include any paths here)
filename_suffix
ImageFileSuffix
INDEX (1, 2, 3) or TIMESTAMP (in UTC)
image_codec
ImageCodec
IC_DEFAULT or IC_JPEG (optional, both options will cause JPEGs to be generated currently)
disable_manifest
bool
by default, Egress outputs a {filepath}.json with a list of exported snapshots
s3
S3Upload
set if uploading to S3 compatible storage. only one storage output can be set
gcp
GCPUpload
set if uploading to GCP
azure
AzureBlobUpload
set if uploading to Azure
aliOSS
AliOSSUpload
set if uploading to AliOSS
S3Upload
Field
Type
Description
access_key
string
secret
string
S3 secret key
bucket
string
destination bucket
region
string
region of the S3 bucket (optional)
endpoint
string
URL to use for S3 (optional)
force_path_style
bool
leave bucket in the path and never to sub-domain (optional)
metadata
map<string, string>
metadata key/value pairs to store (optional)
tagging
string
(optional)
proxy
ProxyConfig
Proxy server to use when uploading(optional)
GCPUpload
Field
Type
Description
credentials
string
Contents of credentials.json
bucket
string
destination bucket
proxy
ProxyConfig
Proxy server to use when uploading(optional)
AzureBlobUpload
Field
Type
Description
account_name
string
account_key
string
container_name
string
destination container
AliOSSUpload
Field
Type
Description
access_key
string
secret
string
bucket
string
region
string
endpoint
string
EncodingOptions
Field
Type
Description
width
int32
height
int32
depth
int32
default 24
framerate
int32
default 30
audio_codec
AudioCodec
default AAC
audio_bitrate
int32
128
audio_frequency
int32
44100
video_codec
VideoCodec
default H264_MAIN
video_bitrate
int32
default 4500
key_frame_interval
int32
default 4s
EncodingOptionsPreset
Enum, valid values:
H264_720P_30: 0
H264_720P_60: 1
H264_1080P_30: 2
H264_1080P_60: 3
PORTRAIT_H264_720P_30: 4
PORTRAIT_H264_720P_60: 5
PORTRAIT_H264_1080P_30: 6
PORTRAIT_H264_1080P_60: 7
ProxyConfig
For S3 and GCP, you can specify a proxy server for Egress to use when uploading files.
This can be helpful to avoid network restrictions on the destination buckets.
Field
Type
Description
url
string
URL of the proxy
username
string
username for basic auth (optional)
password
string
password for basic auth (optional)
WebhookConfig
Extra webhooks can be configured for a specific Egress request. These webhooks are called for Egress lifecycle events in addition to the project wide webhooks. To learn more, see
Webhooks
.
Field
Type
Description
url
string
URL of the webhook
signing_key
string
API key to use to sign the request, must be defined for the project
On this page
API
StartRoomCompositeEgress
StartTrackCompositeEgress
StartTrackEgress
StartWebEgress
UpdateLayout
UpdateStream
ListEgress
StopEgress
Types
EncodedFileOutput
DirectFileOutput
SegmentedFileOutput
StreamOutput
ImageOutput
S3Upload
GCPUpload
AzureBlobUpload
AliOSSUpload
EncodingOptions
EncodingOptionsPreset
ProxyConfig
WebhookConfig
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/egress/examples:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Recording Room Composite as HLS
Recording Web In Portrait
SRT Streaming With Thumbnails
Adding RTMP To Track Composite Egress
Exporting Individual Tracks Without Transcode
Recording Room Composite as HLS
This example records a room composite layout as HLS segments to a S3 bucket.
LiveKit CLI
JavaScript
Go
Ruby
Python
Java
Note
When
live_playlist_name
is provided, we'll generate a playlist containing only the last few segments.
This can be useful to live-stream the recording via HLS.
{
"room_name"
:
"my-room"
,
"layout"
:
"grid"
,
"preset"
:
"H264_720P_30"
,
"custom_base_url"
:
"https://my-custom-template.com"
,
"audio_only"
:
false
,
"segment_outputs"
:
[
{
"filename_prefix"
:
"path/to/my-output"
,
"playlist_name"
:
"my-output.m3u8"
,
"live_playlist_name"
:
"my-output-live.m3u8"
,
"segment_duration"
:
2
,
"s3"
:
{
"access_key"
:
""
,
"secret"
:
""
,
"region"
:
""
,
"bucket"
:
"my-bucket"
,
"force_path_style"
:
true
}
}
]
}
Copy
lk egress start
--type
room-composite egress.json
Copy
Recording Web In Portrait
This example records a web page in portrait mode to Google Cloud Storage and streaming to RTMP.
Portrait orientation can be specified by either setting a preset or advanced options. Egress will resize the Chrome compositor to your specified resolution. However, keep in mind:
Chrome has a minimum browser width limit of 500px.
Your application should maintain a portrait layout, even when the browser reports a width larger than typical mobile phones. (e.g., 720px width or higher).
LiveKit CLI
JavaScript
Go
Ruby
Python
Java
{
"url"
:
"https://my-page.com"
,
"preset"
:
"PORTRAIT_H264_720P_30"
,
"audio_only"
:
false
,
"file_outputs"
:
[
{
"filepath"
:
"my-test-file.mp4"
,
"gcp"
:
{
"credentials"
:
"{\"type\": \"service_account\", ...}"
,
"bucket"
:
"my-bucket"
}
}
]
,
"stream_outputs"
:
[
{
"protocol"
:
"RTMP"
,
"urls"
:
[
"rtmps://my-rtmp-server.com/live/stream-key"
]
}
]
}
Copy
lk egress start
--type
web egress.json
Copy
SRT Streaming With Thumbnails
This examples shows streaming a Participant Egress to a SRT server, and generating thumbnails every 5 seconds. Thumbnails are stored in Azure.
LiveKit CLI
JavaScript
Go
Ruby
Python
Java
{
"room_name"
:
"my-room"
,
"identity"
:
"participant-to-record"
,
"screen_share"
:
false
,
"advanced"
:
{
"width"
:
1280
,
"height"
:
720
,
"framerate"
:
30
,
"audioCodec"
:
"AAC"
,
"audioBitrate"
:
128
,
"videoCodec"
:
"H264_HIGH"
,
"videoBitrate"
:
5000
,
"keyFrameInterval"
:
2
}
,
"stream_outputs"
:
[
{
"protocol"
:
"SRT"
,
"urls"
:
[
"srt://my-srt-server.com:9999"
]
}
]
,
"image_outputs"
:
[
{
"capture_interval"
:
5
,
"width"
:
1280
,
"height"
:
720
,
"filename_prefix"
:
"{room_name}/{publisher_identity}"
,
"filename_suffix"
:
"IMAGE_SUFFIX_TIMESTAMP"
,
"disable_manifest"
:
true
,
"azure"
:
{
"account_name"
:
"my-account"
,
"account_key"
:
"my-key"
,
"container_name"
:
"my-container"
}
}
]
}
Copy
lk egress start
--type
participant egress.json
Copy
Adding RTMP To Track Composite Egress
This example demonstrates a TrackComposite Egress that starts by saving to HLS, with RTMP output added later.
LiveKit CLI
JavaScript
Go
Ruby
Python
Java
{
"room_name"
:
"my-room"
,
"audio_track_id"
:
"TR_AUDIO_ID"
,
"video_track_id"
:
"TR_VIDEO_ID"
,
"stream_outputs"
:
[
{
"protocol"
:
"RTMP"
,
"urls"
:
[
]
}
]
,
"segment_outputs"
:
[
{
"filename_prefix"
:
"path/to/my-output"
,
"playlist_name"
:
"my-output.m3u8"
,
"segment_duration"
:
2
,
"s3"
:
{
"access_key"
:
""
,
"secret"
:
""
,
"region"
:
""
,
"bucket"
:
"my-bucket"
}
}
]
}
Copy
lk egress start
--type
track-composite egress.json
# later, to add a RTMP output
lk egress update-stream
--id
<
egress-id
>
--add-urls rtmp://new-server.com/live/stream-key
# to remove RTMP output
lk egress update-stream
--id
<
egress-id
>
--remove-urls rtmp://new-server.com/live/stream-key
Copy
Exporting Individual Tracks Without Transcode
This example exports video tracks to Azure Blob Storage without transcoding.
Note: video and audio tracks must be exported separately using Track Egress.
LiveKit CLI
JavaScript
Go
Ruby
Python
Java
{
"room_name"
:
"my-room"
,
"track_id"
:
"TR_TRACK_ID"
,
"filepath"
:
"{room_name}/{track_id}"
,
"azure"
:
{
"account_name"
:
"my-account"
,
"account_key"
:
"my-key"
,
"container_name"
:
"my-container"
}
}
Copy
lk egress start
--type
track egress.json
Copy
On this page
Recording Room Composite as HLS
Recording Web In Portrait
SRT Streaming With Thumbnails
Adding RTMP To Track Composite Egress
Exporting Individual Tracks Without Transcode
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/ingress/configure-streaming-software:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
OBS
FFmpeg
GStreamer
The
IngressInfo
object returned by most Ingress APIs contains a full list of the ingress parameters. In particular, the
url
and
stream_key
fields provide the settings required to configure encoders to send media to the Ingress service. Refer to the documentation of any RTMP or WHIP-capable streaming software for more information about how to provide these parameters. Two common examples are OBS and FFmpeg:
OBS
The
OBS Project
releases OBS Studio, a powerful cross platform broadcasting software that can be fully configured through a graphical user interface, and capable of sending complex video compositions to LiveKit WebRTC via Ingress. In order to configure OBS for LiveKit, in the main window, select the
Settings
option, and then the
Stream
tab. In the window, select the
Custom...
Service and enter the URL from the
StreamInfo
in the
Server
field, and the stream key in the
Stream Key
field.
FFmpeg
FFmpeg
is a powerful media processing command-line tool that can be used to stream media to LiveKit Ingress. The following command can be used for that purpose:
% ffmpeg
-re
-i
<
input definition
>
-c:v
libx254
-b:v
3M
-preset
veryfast
-profile
high
-c:a
libfdk_aac
-b:a
128k
-f
flv
"<url from the stream info>/<stream key>"
Copy
For instance:
% ffmpeg
-re
-i
my_file.mp4
-c:v
libx264
-b:v
3M
-preset
veryfast
-profile:v
high
-c:a
libfdk_aac
-b:a
128k
-f
flv rtmps://my-project.livekit.cloud/x/1234567890ab
Copy
Refer to the
FFmpeg documentation
for a list of the supported inputs, and how to use them.
GStreamer
GStreamer
is multi platform multimedia framework that can be used either directly using command line tools provided as part of the distribution, or integrated in other applications using their API. GStreamer supports streaming media to LiveKit Ingress both over RTMP and WHIP.
For RTMP, the following sample command and pipeline definition can be used:
% gst-launch-1.0 flvmux
name
=
mux
!
rtmp2sink
location
=
"<url from the stream info>/<stream key>"
audiotestsrc
wave
=
sine-table
!
faac
!
mux. videotestsrc is-live
=
true
!
video/x-raw,width
=
1280
,height
=
720
!
x264enc speed-preset
=
3
tune
=
zerolatency
!
mux.
Copy
WHIP requires the following GStreamer plugins to be installed:
nicesink
webrtcbin
whipsink
Some these plugins are distributed as part of
libnice
or the
Rust GStreamer plugins package
and may not always be present. This can be verified using the
gst-inspect-1.0
command. LiveKit provides a Docker image based on Ubuntu that includes all the required GStreamer plugins at
livekit/gstreamer:1.22.8-prod-rs
.
gst-launch-1.0 audiotestsrc
wave
=
sine-table
!
opusenc
!
rtpopuspay
!
'application/x-rtp,media=audio,encoding-name=OPUS,payload=96,clock-rate=48000,encoding-params=(string)2'
!
whip.sink_0 videotestsrc is-live
=
true
!
video/x-raw,width
=
1280
,height
=
720
!
x264enc speed-preset
=
3
tune
=
zerolatency
!
rtph264pay
!
'application/x-rtp,media=video,encoding-name=H264,payload=97,clock-rate=90000'
!
whip.sink_1 whipsink
name
=
whip whip-endpoint
=
"<url from the stream info>/<stream key>"
Copy
These 2 sample command lines use the
audiotestsrc
and
videotestsrc
sources to generate test audio and video pattern. These can be replaced with other GStreamer sources to stream any media supported by GStreamer.
On this page
OBS
FFmpeg
GStreamer
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/cloud/architecture:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Built with LiveKit SFU
Distributed Mesh Architecture
Multi-home
No SPOF
Globally distributed
Designed to scale
Built with LiveKit SFU
LiveKit Cloud
builds on our open-source
SFU
. This means it supports the exact same SDKs and APIs as the open-source
stack
.
Maintaining compatibility with LiveKit's Open Source stack (OSS) is important to us. We didn't want any developer locked into using Cloud, or needing to integrate a different set of features, APIs or SDKs for their applications to work with it. Our design goal: a developer should be able to switch between Cloud or self-hosted without changing a line of code.
Distributed Mesh Architecture
In contrast to traditional
WebRTC architectures
, LiveKit Cloud runs multiple SFU instances in a mesh formation. We've developed capabilities for media servers to discover and connect to one another, in order to relay media between servers. This key capability allows us to bypass the single-server limitation that exists in traditional SFU and MCU architectures.
Multi-home
With a multi-home architecture, participants no longer need to connect to the same server. When participants from different regions join the same meeting, they'll each connect to the SFU closest to them, minimizing latency and transmission loss between the participant and SFU.
Each SFU instance establishes connections to other instances over optimized inter-data center networks. Inter-data center networks often run close to internet backbones, delivering high throughput with a minimal number of network hops.
No SPOF
Anything that can fail, will. LiveKit Cloud is designed to anticipate (and recover from) failures in every software and hardware component.
Layers of redundancy are built into the system. A media server failure is recovered from by moving impacted participants to another instance. We isolate shared infrastructure, like our message bus, to individual data centers.
When an entire data center fails, customer traffic is automatically migrated to the next closest data center. LiveKit's SDKs will perform a "session migration": moving existing WebRTC sessions to a different media server without service interruption for your users.
Globally distributed
To serve end users around the world, our infrastructure runs across multiple Cloud vendors and data centers, delivering under 100ms of latency in each region. Today, we have data centers in the following regions:
North America (US East, US Central, US West)
South America (Brazil)
Oceania (Australia)
East Asia (Japan)
Southeast Asia (Singapore)
South Asia (India)
Middle East (UAE)
Africa (South Africa)
Europe (France, Germany, UK)
Designed to scale
When you need to support many viewers on a media track, such as in a livestream, LiveKit Cloud dynamically manages that capacity by forming a distribution mesh, similar to a CDN. This process occurs automatically as your session scales, with no special configurations required. Every LiveKit Cloud project scales seamlessly to accommodate millions of concurrent users in any session.
For a deeper look into the design decisions we've made for LiveKit Cloud, you can
read more
on our blog.
On this page
Built with LiveKit SFU
Distributed Mesh Architecture
Multi-home
No SPOF
Globally distributed
Designed to scale
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/cloud/sandbox:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Getting started
Moving to production
Community templates
Overview
LiveKit Sandboxes
are hosted components that help you prototype your ideas without having to copy and paste code or manage deployments. They're integrated with our CLI, and ready to work with your LiveKit account out of the box. You can use a sandbox to:
Build and customize an AI voice assistant you can share with others, without building and deploying a frontend.
Prototype a mobile or web app without having to set up and deploy a backend server with a token endpoint.
Set up video conferencing rooms with a single click, and share the link with friends and colleagues.
Getting started
Once you've created a LiveKit Cloud account, you can head to the
Sandboxes
page to create a new sandbox, choosing from one of our templates.
Create a LiveKit Cloud account and
Install the LiveKit CLI
.
If you're setting up the CLI for the first time, authenticate with your LiveKit Cloud account:
lk cloud auth
Copy
Navigate to the
Sandboxes
page to create a new sandbox, choosing from one of our templates.
Some templates (for example,
Voice assistant
) require you to run some code on your local machine. This might be an AI agent, a web server, or some other component depending on that template's use case. If present, follow the instructions under the
Code
tab to clone and set up the component:
lk app create
\
--template
<
template-name
>
\
--sandbox
<
my-sandbox-id
>
Copy
Moving to production
When you're ready to move on from the prototyping stage and own the code yourself, every sandbox app can be cloned to your local machine, ready for customization. The quickest way to do this is via the
LiveKit CLI
:
lk app create
--template
<
template-name
>
Copy
You'll notice this is similar to the process for cloning agents and other local templates. That's because all sandboxes, and many other templates at
github.com/livekit-examples
, are simple git repositories with a few conventions around environment variables and make them ready to work with your LiveKit account and the CLI.
Community templates
If you're interested in creating and sharing your own templates with the larger community of LiveKit users, check out the
Template Index
repository for more information on contributing.
On this page
Overview
Getting started
Moving to production
Community templates
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/cloud/quotas-and-limits:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Free quotas
Connection limits
Build plan
Ship plan
Scale plan
Custom plan
Egress time limits
Media subscription limits
API request rate limits
Free quotas
Every LiveKit project gets the following for free:
50GB data transfer
5,000 connection minutes
60 minutes of transcoding (for
Stream import (ingress)
or
Composite recording (egress)
)
Connection limits
LiveKit projects have limitations on the number of concurrent connections of various types in order
to ensure the stability of the network and to prevent abuse. This is similar to rate limiting for an HTTP
service, but for a continuous service with long-lived connections. Like rate limiting, the primary
purpose of these connection limits is to prevent abuse.
You can view the current connection limits on your project at any time in the
LiveKit Cloud
dashboard by navigating to
Settings
and selecting
the
Project
tab.
For pricing information for any of the following plans, see the
pricing guide
.
Build plan
Projects on the
Build
(free) plan have the following limits:
100 concurrent participants.
2 concurrent
egress requests
at a time.
2 concurrent
ingress requests
at a time.
When these limits are reached, new connections of the same type fail.
Ship plan
Projects on the
Ship
plan have the following limits:
1,000 concurrent participants.
100 concurrent egress requests.
100 concurrent ingress requests.
When these limits are reached, new connections of the same type fail.
Scale plan
Projects on the
Scale
plan have the following limits:
Unlimited concurrent participants.
100 concurrent egress requests.
100 concurrent ingress requests.
When these limits are reached, new connections of the same type fail.
An admin for your project can request an increase for a particular limit in
your
project settings
.
Custom plan
LiveKit can work with you to ensure your project has the capacity it needs.
Contact the sales team
with your project details.
Egress time limits
Egress has time limits, depending on the output type:
Egress output
Time limit
File output (MP4, OGG, WebM)
3 hours
HLS segments
12 hours
HLS/RTMP streaming
12 hours
When these time limits are reached, any in-progress egress automatically ends with the status
LIMIT_REACHED
.
You can listen for this status change using the
egress_ended
webhook
.
Media subscription limits
Each participant may subscribe to a limited number of media tracks. Currently, the limits are
as follows:
Up to 100 video tracks.
Up to 100 audio tracks.
For high volume video use cases, consider using pagination and
selective subscriptions
to keep the number of
subscriptions within these limits.
API request rate limits
All projects have a 1000 requests per minute rate limit on API requests. The limit only applies to
Server API
requests (for example,
RoomService
or
EgressService
API requests) and doesn't apply to SDK methods like joining a room or sending data packets.
LiveKit doesn't anticipate any project exceeding this rate limit. However, you can reach out to
support
to request an increase. Include the
Project URL
in your email.
You can find your project URL in the LiveKit Cloud dashboard in your
Project Settings
page.
On this page
Free quotas
Connection limits
Build plan
Ship plan
Scale plan
Custom plan
Egress time limits
Media subscription limits
API request rate limits
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/cloud/billing:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Billing
How we meter
Billing cycle
Invoices
Downloading invoices
Customizing invoices
Overview
Refer to our latest
blog post
and
pricing page
for information about our current pricing.
Billing
How we meter
We meter all projects and bill for resources consumed. This table shows the resources we meter and the increments we bill in:
Resource
Unit
Minimum increment
Outbound transfer
GB
0.01 GB
Realtime connection
minute
1 minute
SIP connection
minute
1 minute
Egress Transcode
minute
1 minute
Ingress Transcode
minute
1 minute
Billing cycle
LiveKit Cloud bills monthly. At the end of each month, we calculate the total resources consumed by your project and bill you for the resources consumed.
Invoices
Downloading invoices
Paying projects can download previous months' invoices as PDFs on the project's
billing page
(accessible only to project admins) and clicking the "PDF" link in the "Statements" section.
Customizing invoices
By default, the invoice only lists your project name. Some customers require more information on the invoice, such as a billing address or VAT number. You can add this information to your invoice by clicking the
...
menu to the right of the PDF link, then clicking
Customize “Invoice to:” field
.
This field is a plain text field that accepts any text. Newlines will be preserved on the invoice PDF. For example, you could include your business name and address like so, and the invoice PDF will have line breaks in the same places:
Acme
Inc
.
404
Nowhere
Ln
.
New
York
,
NY
10001
Copy
After saving your “Invoice to:” field, you can click the
PDF
link to re-download the invoice PDF and it will include the new information.
On this page
Overview
Billing
How we meter
Billing cycle
Invoices
Downloading invoices
Customizing invoices
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/cloud/firewall:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Corporate firewalls
Minimum requirements
Corporate firewalls
LiveKit uses WebSocket and WebRTC to transmit data and media. All transmissions are encrypted with
TLS
and
DTLS
.
LiveKit Cloud requires access to a few domains in order to establish a connection. If you are behind a corporate firewall, please ensure outbound traffic is allowed to the following addresses and ports:
Host
Port
Purpose
*.livekit.cloud
TCP: 443
Signal connection over secure WebSocket
*.turn.livekit.cloud
TCP: 443
TURN
/TLS. Used when UDP connection isn't viable
*.host.livekit.cloud
UDP: 3478
TURN/UDP servers that assist in establishing connectivity
all hosts (optional)
UDP: 50000-60000
UDP connection for WebRTC
all hosts (optional)
TCP: 7881
TCP connection for WebRTC
In order to obtain the best audio and video quality, we recommend allowing access to the UDP ports listed above. Additionally, please ensure UDP hole-punching is enabled (or disable symmetric NAT). This helps machines behind the firewall to establish a direct connection to a LiveKit Cloud media server.
Minimum requirements
If wildcard hostnames are not allowed by your firewall or security policy, the following are the mimimum set of hostnames required to connect to LiveKit Cloud:
Host
Port
<your-subdomain>.livekit.cloud
TCP 443
<your-subdomain>.sfo3.production.livekit.cloud
TCP 443
<your-subdomain>.dsfo3a.production.livekit.cloud
TCP 443
<your-subdomain>.dsfo3b.production.livekit.cloud
TCP 443
<your-subdomain>.dfra1a.production.livekit.cloud
TCP 443
<your-subdomain>.dfra1b.production.livekit.cloud
TCP 443
<your-subdomain>.dblr1a.production.livekit.cloud
TCP 443
<your-subdomain>.dblr1b.production.livekit.cloud
TCP 443
<your-subdomain>.dsgp1a.production.livekit.cloud
TCP 443
<your-subdomain>.dsgp1b.production.livekit.cloud
TCP 443
<your-subdomain>.dsyd1a.production.livekit.cloud
TCP 443
<your-subdomain>.dsyd1b.production.livekit.cloud
TCP 443
<your-subdomain>.osaopaulo1a.production.livekit.cloud
TCP 443
<your-subdomain>.osaopaulo1b.production.livekit.cloud
TCP 443
<your-subdomain>.oashburn1a.production.livekit.cloud
TCP 443
<your-subdomain>.oashburn1b.production.livekit.cloud
TCP 443
<your-subdomain>.omarseille1a.production.livekit.cloud
TCP 443
<your-subdomain>.omarseille1b.production.livekit.cloud
TCP 443
<your-subdomain>.otokyo1a.production.livekit.cloud
TCP 443
<your-subdomain>.otokyo1b.production.livekit.cloud
TCP 443
<your-subdomain>.ophoenix1a.production.livekit.cloud
TCP 443
<your-subdomain>.ophoenix1b.production.livekit.cloud
TCP 443
<your-subdomain>.olondon1a.production.livekit.cloud
TCP 443
<your-subdomain>.olondon1b.production.livekit.cloud
TCP 443
<your-subdomain>.ochicago1a.production.livekit.cloud
TCP 443
<your-subdomain>.ochicago1b.production.livekit.cloud
TCP 443
<your-subdomain>.osingapore1a.production.livekit.cloud
TCP 443
<your-subdomain>.osingapore1b.production.livekit.cloud
TCP 443
<your-subdomain>.odubai1a.production.livekit.cloud
TCP 443
<your-subdomain>.odubai1b.production.livekit.cloud
TCP 443
<your-subdomain>.ojohannesburg1a.production.livekit.cloud
TCP 443
<your-subdomain>.ojohannesburg1b.production.livekit.cloud
TCP 443
<your-subdomain>.omumbai1a.production.livekit.cloud
TCP 443
<your-subdomain>.omumbai1b.production.livekit.cloud
TCP 443
<your-subdomain>.ofrankfurt1a.production.livekit.cloud
TCP 443
<your-subdomain>.ofrankfurt1b.production.livekit.cloud
TCP 443
<your-subdomain>.ojerusalem1a.production.livekit.cloud
TCP 443
<your-subdomain>.ojerusalem1b.production.livekit.cloud
TCP 443
<your-subdomain>.turn.livekit.cloud
TCP 443
sfo3.turn.livekit.cloud
TCP 443
dsfo3a.turn.livekit.cloud
TCP 443
dsfo3b.turn.livekit.cloud
TCP 443
dfra1a.turn.livekit.cloud
TCP 443
dfra1b.turn.livekit.cloud
TCP 443
dblr1a.turn.livekit.cloud
TCP 443
dblr1b.turn.livekit.cloud
TCP 443
dsgp1a.turn.livekit.cloud
TCP 443
dsgp1b.turn.livekit.cloud
TCP 443
dsyd1a.turn.livekit.cloud
TCP 443
dsyd1b.turn.livekit.cloud
TCP 443
osaopaulo1a.turn.livekit.cloud
TCP 443
osaopaulo1b.turn.livekit.cloud
TCP 443
oashburn1a.turn.livekit.cloud
TCP 443
oashburn1b.turn.livekit.cloud
TCP 443
omarseille1a.turn.livekit.cloud
TCP 443
omarseille1b.turn.livekit.cloud
TCP 443
otokyo1a.turn.livekit.cloud
TCP 443
otokyo1b.turn.livekit.cloud
TCP 443
ophoenix1a.turn.livekit.cloud
TCP 443
ophoenix1b.turn.livekit.cloud
TCP 443
olondon1a.turn.livekit.cloud
TCP 443
olondon1b.turn.livekit.cloud
TCP 443
ochicago1a.turn.livekit.cloud
TCP 443
ochicago1b.turn.livekit.cloud
TCP 443
osingapore1a.turn.livekit.cloud
TCP 443
osingapore1b.turn.livekit.cloud
TCP 443
odubai1a.turn.livekit.cloud
TCP 443
odubai1b.turn.livekit.cloud
TCP 443
ojohannesburg1a.turn.livekit.cloud
TCP 443
ojohannesburg1b.turn.livekit.cloud
TCP 443
omumbai1a.turn.livekit.cloud
TCP 443
omumbai1b.turn.livekit.cloud
TCP 443
ofrankfurt1a.turn.livekit.cloud
TCP 443
ofrankfurt1b.turn.livekit.cloud
TCP 443
ojerusalem1a.turn.livekit.cloud
TCP 443
ojerusalem1b.turn.livekit.cloud
TCP 443
Note
This list of domains is subject to change. Last updated 2025-02-27.
On this page
Corporate firewalls
Minimum requirements
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/cloud/analytics-api:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Generate an access token for Analytics requests
List sessions
Query parameters
List session details
Generate an access token for Analytics requests
Analytics API requests are authorized with a LiveKit token. This is generated by a server side SDK,much like
generating a token for joining Rooms
, except that the token needs the
roomList
grant.
Note
Analytics API is only available to LiveKit Cloud customers with a
Scale plan or higher
.
LiveKit CLI
Node.js
lk token create
\
--api-key
$LIVEKIT_API_KEY
\
--api-secret
$LIVEKIT_SECRET_KEY
\
--list
\
--valid-for 24h
Copy
Tip
To streamline your workflow with the
CLI
, add your projects using the command
lk project add
. This approach spares you from repeatedly entering your
--url
,
--api-key
, and
--api-secret
for each command you execute.
List sessions
To make a request, you'll need to know your project id, which you can see in the URL for your project dashboard. It's the part after
/projects/
that starts with
p_
.
Node.js
Shell
async
function
listLiveKitSessions
(
)
{
const
endpoint
=
`
https://cloud-api.livekit.io/api/project/
${
PROJECT_ID
}
/sessions/
`
;
try
{
const
response
=
await
fetch
(
endpoint
,
{
method
:
'GET'
,
headers
:
{
Authorization
:
`
Bearer
${
token
}
`
,
'Content-Type'
:
'application/json'
,
}
,
}
)
;
if
(
!
response
.
ok
)
throw
new
Error
(
'Network response was not ok'
)
;
const
data
=
await
response
.
json
(
)
;
console
.
log
(
data
)
;
// or do whatever you like here
}
catch
(
error
)
{
console
.
log
(
'There was a problem:'
,
error
.
message
)
;
}
}
listLiveKitSessions
(
)
;
Copy
This will return a JSON object like this:
{
sessions
:
[
{
sessionId
,
// string
roomName
,
// string
createdAt
,
// Timestamp
lastActive
,
// Timestamp
bandwidthIn
,
// bytes of bandwidth uploaded
bandwidthOut
,
// bytes of bandwidth downloaded
egress
,
// 0 = never started, 1 = active, 2 = ended
numParticipants
,
// int
numActiveParticipants
,
// int
}
,
// ...
]
}
Copy
Query parameters
limit
int
Required
#
You can limit the number of returned sessions by adding the limit query parameter like
?limit=100
.
Caution
Higher
limit
values may result in a timeout from the Analytics API.
page
int
Required
#
You can page through the results by adding
?page=n&limit=100
to the endpoint URL to get the
n
th page of results with
100
sessions per page. Pagination starts from
0
.
start
string
Required
#
Specify the start date for the request time range in the format
YYYY-MM-DD
. Sessions starting on the specified start date will be included in the response.
Note
The start date must be within 7 days of the current date.
end
string
Required
#
Specify the end date for the request time range using the format
YYYY-MM-DD
. Sessions up to and including this end date will be included in the response.
Examples
# Get the first page and limit the number of sessions to 100.
curl
-H
"Authorization: Bearer
$TOKEN
"
\
"https://cloud-api.livekit.io/api/project/
$PROJECT_ID
/sessions\
?page=0&limit=100"
# Fetch sessions from a specified time range.
curl
-H
"Authorization: Bearer
$TOKEN
"
\
"https://cloud-api.livekit.io/api/project/
$PROJECT_ID
/sessions\
?start=2024-01-12&end=2024-01-13"
Copy
List session details
To get more details about a specific session, you can use the session_id returned from the list sessions request.
Node.js
Shell
async
function
getLiveKitSessionDetails
(
)
{
const
endpoint
=
`
https://cloud-api.livekit.io/api/project/
${
PROJECT_ID
}
/sessions/
${
SESSION_ID
}
`
;
try
{
const
response
=
await
fetch
(
endpoint
,
{
method
:
'GET'
,
headers
:
{
Authorization
:
`
Bearer
${
token
}
`
,
'Content-Type'
:
'application/json'
,
}
,
}
)
;
if
(
!
response
.
ok
)
throw
new
Error
(
'Network response was not ok'
)
;
const
data
=
await
response
.
json
(
)
;
console
.
log
(
data
)
;
// or do whatever you like here
}
catch
(
error
)
{
console
.
log
(
'There was a problem:'
,
error
.
message
)
;
}
}
getLiveKitSessionDetails
(
)
;
Copy
This will return a JSON object like this:
{
roomId
,
// string
roomName
,
// string
bandwidth
,
// billable bytes of bandwidth used
startTime
,
// Timestamp
endTime
,
// Timestamp
numParticipants
,
// int
participants
:
[
{
participantIdentity
,
// string
participantName
,
// string
roomId
,
// string
joinedAt
,
// Timestamp
leftAt
,
// Timestamp
publishedSources
:
{
cameraTrack
,
// boolean
microphoneTrack
,
// boolean
screenShareTrack
,
// boolean
screenShareAudio
,
// boolean
}
,
sessions
:
[
{
sessionId
,
// string
joinedAt
,
// Timestamp
leftAt
,
// Timestamp
}
,
// ...
]
,
}
,
// ...
]
,
}
Copy
Timestamp
objects are
Protobuf Timestamps
.
On this page
Generate an access token for Analytics requests
List sessions
Query parameters
List session details
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/cloud/noise-cancellation:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Supported platforms
Usage instructions
Overview
LiveKit cloud offers access to advanced models licensed from
Krisp
to remove background noise and ensure the best possible audio quality. The models run locally, with no audio data sent to Krisp servers as part of this process and negligible impact on audio latency or quality.
The filter is available in either an outbound configuration (where noise is removed by the sender) or in an inbound configuration (where noise is removed by the receiver).
Background voice cancellation (BVC) is available for inbound server-side SDKs, such as Python. This model removes extra background speakers in addition to background noise, providing the best possible experience for voice AI applications.
Recommendation
For most 1:1 voice AI applications, LiveKit recommends using the BVC model in the LiveKit Agents Python SDK as the most effective solution.
Here is a comparison to illustrate the improvement over standard WebRTC noise suppression:
Original
WebRTC noiseSuppression
LiveKit Cloud enhanced noise cancellation
Supported platforms
Consult the following table for a complete overview of feature support.
Platform
Outbound
Inbound
BVC
Package
Web
✅
❌
❌
@livekit/krisp-noise-filter
Swift
✅
❌
❌
LiveKitKrispNoiseFilter
Android
✅
❌
❌
io.livekit:krisp-noise-filter
Flutter
✅
❌
❌
livekit_noise_filter
React Native
✅
❌
❌
@livekit/react-native-krisp-noise-filter
Unity
❌
❌
❌
N/A
Python
❌
✅
✅
livekit-plugins-noise-cancellation
Node.js
❌
❌
❌
N/A
Telephony
✅
✅
❌
LiveKit SIP documentation
Usage instructions
Use the following instructions to integrate the filter into your app.
Python
JavaScript
Android
Swift
React Native
Flutter
SIP
Installation
Install the noise cancellation package from PyPI:
pip
install
"livekit-plugins-noise-cancellation~=0.2"
Copy
Usage in LiveKit Agents
Include the filter in
RoomInputOptions
when starting your
AgentSession
:
from
livekit
.
plugins
import
noise_cancellation
# ...
await
session
.
start
(
# ...,
room_input_options
=
room_io
.
RoomInputOptions
(
noise_cancellation
=
noise_cancellation
.
BVC
(
)
,
)
,
)
# ...
Copy
Agents v0.12 compatibility
In LiveKit Agents v0.12, pass the
noise_cancellation
parameter to the
VoicePipelineAgent
or
MultimodalAgent
constructor.
Usage with AudioStream
Apply the filter to any individual inbound AudioStream:
stream
=
rtc
.
AudioStream
.
from_track
(
track
=
track
,
noise_cancellation
=
noise_cancellation
.
NC
(
)
,
)
Copy
Available models
There are two noise cancellation models available:
# Standard enhanced noise cancellation
noise_cancellation
.
NC
(
)
# Background voice cancellation (NC + removes non-primary voices
# that would confuse transcription or turn detection)
noise_cancellation
.
BVC
(
)
Copy
Compatibility
The Python noise filter is currently supported only on Mac and Linux platforms.
On this page
Overview
Supported platforms
Usage instructions
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/self-hosting/local:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Install LiveKit Server
Start the server in dev mode
Install LiveKit Server
macOS
Linux
Windows
brew update && brew install livekit
Copy
Start the server in dev mode
You can start LiveKit in development mode by running:
livekit-server --dev
Copy
This will start an instance using the following API key/secret pair:
API key: devkey
API secret: secret
Copy
To customize your setup for production, refer to our
deployment guides
.
Tip
By default LiveKit's signal server binds to
127.0.0.1:7880
. If you'd like to access it from other devices on your network, pass in
--bind 0.0.0.0
On this page
Install LiveKit Server
Start the server in dev mode
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/self-hosting/deployment:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Domain, SSL certificates, and load balancer
Improving connectivity with TURN
TURN/TLS
TURN/UDP
Configuration
Resources
Domain, SSL certificates, and load balancer
In order to have a secure LiveKit deployment, you will need a domain as well as a SSL certificate for that domain. This domain will be used as the primary endpoint for LiveKit SDKs, for example:
wss://livekit.yourhost.com
. The SSL certificate must be signed by a trusted certificate authority; self-signed certs do not work here.
You will also need to set up HTTPS/SSL termination with a load balancer or reverse proxy.
If you are using TURN, then a separate TURN domain and SSL cert will be needed, as well.
Improving connectivity with TURN
Certain corporate firewalls block not only UDP traffic, but non-secure TCP traffic, as well. In those cases, it's helpful to use a TURN server.
Here's
a good resource if you're interested in reading more about how TURN is used.
The good news is LiveKit includes an embedded TURN server. It's a secure TURN implementation that has integrated authentication with the rest of LiveKit. The authentication layer ensures that only clients that have already established a signal connection could connect to our TURN server.
TURN/TLS
To firewalls, TLS traffic looks no different from regular HTTPS traffic to websites. Enabling TURN/TLS gives you the broadest coverage in client connectivity, including those behind corporate firewalls. TURN/TLS can be enabled with:
turn
:
enabled
:
true
tls_port
:
5349
domain
:
turn.myhost.com
cert_file
:
/path/to/turn.crt
key_file
:
/path/to/turn.key
Copy
LiveKit will perform TLS termination, so you will have to specify the certificates in the config. When running multiple LiveKit instances, you can place a layer 4 load balancer in front of the TCP port.
If you are not using a load balancer,
turn.tls_port
needs to be set to 443, as that will be the port that's advertised to clients.
TURN/UDP
As QUIC (HTTP/3) gains adoption, some firewalls started allowing UDP traffic to pass through port 443. In those cases, it helps to use TURN/UDP on port 443. UDP is preferred over TCP for WebRTC traffic, as it has better control over congestion and latency. TURN/UDP can be enabled with:
turn
:
enabled
:
true
udp_port
:
443
Copy
Configuration
For production deploys, we recommend using a config file. The config file can be passed in via
--config
flag, or the body of the YAML can be set with a
LIVEKIT_CONFIG
environment variable.
Below is a recommended config for a production deploy. To view other customization options, see
config-sample.yaml
port
:
7880
log_level
:
info
rtc
:
tcp_port
:
7881
port_range_start
:
50000
port_range_end
:
60000
# use_external_ip should be set to true for most cloud environments where
# the host has a public IP address, but is not exposed to the process.
# LiveKit will attempt to use STUN to discover the true IP, and advertise
# that IP with its clients
use_external_ip
:
true
redis
:
# redis is recommended for production deploys
address
:
my
-
redis
-
server.name
:
6379
keys
:
# key-value pairs
# your_api_key: <your_api_secret>
# When enabled, LiveKit will expose prometheus metrics on :6789/metrics
#prometheus_port: 6789
turn
:
enabled
:
true
# domain must match tls certificate
domain
:
<turn.myhost.com
>
# defaults to 3478. If not using a load balancer, must be set to 443.
tls_port
:
3478
Copy
Resources
The scalability of LiveKit is bound by CPU and bandwidth. We recommend running production setups on 10Gbps ethernet or faster.
When deploying to cloud providers, compute-optimized instance types are the most suitable for LiveKit.
If running in a Dockerized environment, host networking should be used for optimal performance.
On this page
Domain, SSL certificates, and load balancer
Improving connectivity with TURN
TURN/TLS
TURN/UDP
Configuration
Resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/self-hosting/vm:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Pre-requisites
Generate configuration
Deploy to a VM
Firewall
DNS
Upgrading
Troubleshooting
Checking TLS certificates
Ensure DNS is pointed at your domain
Instance started before networking
Instance firewall
This configuration utilizes Docker Compose and Caddy. Your LiveKit server will support a broad spectrum of connectivity options, including those behind VPN and firewalls (via TURN/TLS)
You do not need separate SSL certificates for this set up, we will provision them automatically with Caddy. (by using Let's Encrypt or ZeroSSL)
If desired, the generator can also assist with setting up LiveKit
Ingress
and
Egress
. This gives you the ability to ingest media from other sources, as well as enabling recording capabilities.
Pre-requisites
To start, you'll need:
A domain that you own
The ability to add DNS records for subdomains for your new LiveKit server
Generate configuration
Use our configuration generation tool to create a customized configuration for your domain. This script should be run on your development machine:
docker
pull livekit/generate
docker
run
--rm
-it
-v
$PWD
:/output livekit/generate
Copy
It creates a folder with the name of domain you provided, containing the following files:
caddy.yaml
docker-compose.yaml
livekit.yaml
redis.conf
init_script.sh
OR
cloud_init.xxxx.yaml
Deploy to a VM
Depending on your cloud provider, there are a couple of options:
Cloud Init
Startup Script
This is the easiest method for deploying LiveKit Server. AWS, Azure, Digital Ocean, and others support
cloud-init
.
We have tested our scripts on Ubuntu and Amazon Linux, but it's possible the same scripts may work on other platforms.
(Please let us know in Slack or open a PR!)
When starting a VM, paste the contents of the file
cloud-init.xxxx.yaml
into the
User data
field.
That's it! When the machine starts up, it'll execute the cloud-init protocol and install LiveKit.
When the install script is finished, your instance should be set up. It will have installed:
docker
docker-compose
generated configuration to
/opt/livekit
systemd service
livekit-docker
To start/stop the service via systemctl:
systemctl stop livekit-docker
systemctl start livekit-docker
Copy
Firewall
Ensure that the following ports are open on your firewall and accessible on the instance:
443
- primary HTTPS and TURN/TLS
80
- TLS issuance
7881
- WebRTC over TCP
3478/UDP
- TURN/UDP
50000-60000/UDP
- WebRTC over UDP
And if Ingress is desired
1935
- RTMP Ingress
7885/UDP
- WebRTC for WHIP Ingress
DNS
Both primary and TURN domains must point to the IP address of your instance.
This is required for Caddy to start provisioning your TLS certificates.
Upgrading
To upgrade your install to new LiveKit releases, edit the docker compose file:
/opt/livekit/docker-compose.yaml
Change the image field under
livekit
to
livekit/livekit-server:v<version>
Alternatively, to always run the latest version, set the image field to
livekit/livekit-server:latest
and run:
docker
pull livekit/livekit-server
Copy
Troubleshooting
If something is not working as expected, SSH in to your server and use the following commands to investigate:
systemctl status livekit-docker
cd
/opt/livekit
sudo
docker-compose
logs
Copy
Checking TLS certificates
If certificate acquisition process has been successful, you should see the following log entry:
livekit-caddy-1
|
{
"level"
:
"info"
,
"ts"
:1642786068.3883107,
"logger"
:
"tls.obtain"
,
"msg"
:
"certificate obtained successfully"
,
"identifier"
:
"<yourhost>"
}
Copy
If you don't see these messages, it means your server could not be reached from the internet.
Ensure DNS is pointed at your domain
Running
host <yourdomain>
should show the IP address of your server. Ensure that it matches the IP address of your server.
Instance started before networking
When using cloud-init, it's possible that the instance started up before networking was available to the machine. This is commonly the case on EC2 instances. When this happens, your cloud-init scripts will be stuck in a bad state. To fix this, you can SSH into the machine and trigger a re-run:
sudo
cloud-init clean
--logs
sudo
reboot
now
Copy
Instance firewall
Certain Linux distributions ship with an instance-specific firewall enabled. To check if this is the case, run:
sudo
firewall-cmd --list-all
Copy
If firewall is enabled, you could add the following rules to it and restart the firewall:
sudo
firewall-cmd
--zone
public
--permanent
--add-port
80
/tcp
sudo
firewall-cmd
--zone
public
--permanent
--add-port
443
/tcp
sudo
firewall-cmd
--zone
public
--permanent
--add-port
7881
/tcp
sudo
firewall-cmd
--zone
public
--permanent
--add-port
443
/udp
sudo
firewall-cmd
--zone
public
--permanent
--add-port
50000
-60000/udp
sudo
firewall-cmd
--reload
Copy
When the ports are successfully opened, running
curl http://<yourdomain>
should return a 404 response. (instead of hanging)
On this page
Pre-requisites
Generate configuration
Deploy to a VM
Firewall
DNS
Upgrading
Troubleshooting
Checking TLS certificates
Ensure DNS is pointed at your domain
Instance started before networking
Instance firewall
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/self-hosting/kubernetes:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Understanding the deployment
Graceful restarts
Using the Helm Chart
Pre-requisites
Importing SSL Certificates
Install & Upgrade
Firewall
LiveKit streamlines deployment to Kubernetes. We publish a
Helm chart
that help you set up a distributed deployment of LiveKit, along with a Service and Ingress to correctly route traffic. Our Helm chart supports Google GKE, Amazon EKS, and Digital Ocean DOKS out of the box, and can serve as a guide on your custom Kubernetes installations.
Important
LiveKit does not support deployment to serverless and/or private clusters. Private clusters have additional layers of NAT that make it unsuitable for WebRTC traffic.
Understanding the deployment
LiveKit pods requires direct access to the network with host networking. This means that the rtc.udp/tcp ports that are open on those nodes are directly handled by LiveKit server. With that direct requirement of specific ports, it means we'll be limited to one LiveKit pod per node. It's possible to run other workload on those nodes.
Termination of TLS/SSL is left as a responsibility of the Ingress. Our Helm chart will configure TLS termination for GKE and ALB load balancers. To use ALB on EKS, AWS Load Balancer Controller needs to be
installed separately
.
Graceful restarts
During an upgrade deployment, older pods will need to be terminated. This could be extremely disruptive if there are active sessions running on those pods. LiveKit handles this by allowing that instance to drain prior to shutting down.
We also set
terminationGracePeriodSeconds
to 5 hours in the helm chart, ensuring Kubernetes gives sufficient time for the pod to gracefully shut down.
Using the Helm Chart
Pre-requisites
To deploy a multi-node cluster that autoscales, you'll need:
a Redis instance
SSL certificates for primary domain and TURN/TLS
a Kubernetes cluster on AWS, GCloud, or DO
Helm
is installed on your machine.
Then add the LiveKit repo
$ helm repo
add
livekit https://helm.livekit.io
Copy
Depending on your cloud provider, the following pre-requisites may be required
AWS
Digital Ocean
On AWS, it's recommended to use ALB Ingress Controller as the main load balancer for LiveKit's signal connection. You can find installation instructions
here
.
With ALB, you could also used ACM to handle TLS termination for the primary domain. However, a SSL certificate is still needed in order to use the embedded TURN/TLS server.
Create a values.yaml for your deployment, using
server-sample.yaml
as a template.
Checkout
Helm examples
for AWS, Google Cloud, and Digital Ocean.
Importing SSL Certificates
In order to set up TURN/TLS and HTTPS on the load balancer, you may need to import your SSL certificate(s) into as a Kubernetes Secret. This can be done with:
kubectl create secret tls
<
NAME
>
--cert
<
CERT-FILE
>
--key
<
KEY-FILE
>
--namespace
<
NAMESPACE
>
Copy
Note, please ensure that the secret is created in the same namespace as the deployment.
Install & Upgrade
helm
install
<
INSTANCE_NAME
>
livekit/livekit-server
--namespace
<
NAMESPACE
>
--values
values.yaml
Copy
We'll publish new version of the chart with new server releases. To fetch these updates and upgrade your installation, perform
helm repo update
helm upgrade
<
INSTANCE_NAME
>
livekit/livekit-server
--namespace
<
NAMESPACE
>
--values
values.yaml
Copy
If any configuration has changed, you may need to trigger a restart of the deployment. Kubernetes triggers a restart only when the pod itself has changed, but does not when the changes took place in the ConfigMap.
Firewall
Ensure that your
firewall
is configured properly to allow traffic into LiveKit ports.
On this page
Understanding the deployment
Graceful restarts
Using the Helm Chart
Pre-requisites
Importing SSL Certificates
Install & Upgrade
Firewall
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/self-hosting/distributed:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Multi-node routing
Downscaling and draining
Multi-region support
Configuration
Multi-node routing
When Redis is configured, LiveKit automatically switches to a distributed setup by using Redis for room data as well as a message bus. In this mode, each node periodically reports their stats to Redis; this enables them to be aware of the entire cluster and make routing decisions based on availability and load. We recommend this setup for a redundant deployment.
When a new room is created, the node that received this request is able to choose an available node from the cluster to host the room.
When a client establishes a signal connection to LiveKit, it creates a persistent WebSocket connection with one of the instances. That instance will then acts as a signaling bridge, proxying messages between the node where the room is hosted and the client.
In a multi-node setup, LiveKit can support a large number of concurrent rooms. However, there are limits to the number of participants in a room since, for now, a room must fit on a single node.
Downscaling and draining
It's simple to scale up instances, but what about scaling down? Terminating an instance while it's hosting active sessions would be extremely disruptive to the end user.
LiveKit solves this problem by providing connection draining natively. When it receives a request to terminate (via
SIGTERM
,
SIGINT
, or
SIGQUIT
) and there are participants currently connected, it will put itself into draining mode. While draining, the instance would:
allow active rooms to run as usual
accept traffic for new participants to active rooms
reject participants trying to join new rooms
When all participants have disconnected, the server will complete draining and shut down.
Multi-region support
It's possible to deploy LiveKit to multiple data centers, allowing users located in different regions to connect to a server that's closest to them.
LiveKit supports this via a
region-aware, load aware node selector
. It's designed to be used in conjunction with region-aware load balancing of the signal connection.
Here's how it works:
Geo or latency aware DNS service (such as Route53 or Cloudflare) returns IP of load balancer closest to the user
User connects load balancer in that region
Then connects to an instance of LiveKit in that region
If the room doesn't already exist, LiveKit will use node selector to choose an available node
The selection criteria is
node must have lower utilization than
sysload_limit
nodes are in the region closest to the signaling instance
a node satisfying the above is chosen at random
Configuration
node_selector
:
kind
:
regionaware
sysload_limit
:
0.5
# List of regions and their lat/lon coordinates
regions
:
-
name
:
us
-
west
-
2
lat
:
37.64046607830567
lon
:
-120.88026233189062
-
name
:
us
-
east
lat
:
40.68914362140307
lon
:
-74.04445748616385
Copy
On this page
Multi-node routing
Downscaling and draining
Multi-region support
Configuration
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/self-hosting/ports-firewall:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Ports
Firewall
Ports
LiveKit uses several ports to communicate with clients. Exposed ports below need to be open on the firewall.
Port
Default
Config
Exposed
Description
API, WebSocket
7880
port
no
This port should be placed behind a load balancer that can terminate SSL. LiveKit APIs are homogenous: any client could connect to any backend instance, regardless of the room they are in.
ICE/UDP
50000-60000
rtc.port_range_start
,
rtc.port_range_end
yes
LiveKit advertises these ports as WebRTC host candidates (each participant in the room will use two ports)
ICE/TCP
7881
rtc.tcp_port
yes
Used when the client could not connect via UDP (e.g. VPN, corporate firewalls)
ICE/UDP Mux
7882
rtc.udp_port
yes
(optional) It's possible to handle all UDP traffic on a single port. When this is set, rtc.port_range_start/end are not used
TURN/TLS
5349
turn.tls_port
when not using LB
(optional) For a distributed setup, use a network load balancer in front of the port. If not using LB, this port needs to be set to 443.
TURN/UDP
3478
turn.udp_port
yes
(optional) To use the embedded TURN/UDP server. When enabled, it also serves as a STUN server.
SIP/UDP
5060
sip_port
yes
(optional) UDP signaling port for LiveKit SIP. Available in
sip/config.yml
.
SIP/TCP
5060
sip_port
yes
(optional) TCP signaling port for LiveKit SIP. Available in
sip/config.yml
.
SIP/TLS
5061
tls.port
yes
(optional) TLS signaling port for LiveKit SIP. Available in
sip/config.yml
.
SIP RTP/UDP
10000-20000
rtp_port
yes
(optional) RTP media port range for LiveKit SIP. Available in
sip/config.yml
.
Firewall
When hosting in cloud environments, the ports configured above will have to be opened in the firewall.
AWS
Digital Ocean
Google Cloud
Navigate to the VPC dashboard, choose
Security Groups
, and select the security group that LiveKit is deployed to.
Open the
Inbound rules
tab and select
Edit Inbound Rules
Then add the following rules (assuming use of default ports):
On this page
Ports
Firewall
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/self-hosting/benchmark:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Measuring performance
Load testing
Benchmarks
Audio only
Video room
Measuring performance
LiveKit can scale to many simulteneous rooms by running a distributed setup across multiple nodes. However, each room must fit within a single node. For this reason, benchmarks below will be focused on stressing the number of concurrent users in a room.
With WebRTC SFUs, a few factors determine the amount of work a server could perform:
Number of tracks published
Number of subscribers
Amount of data sent to each subscriber
An SFU needs to perform work to receive every track - this means receiving tens of packets per second. It then needs to forward that received data to every subscriber. That adds up to a significant amount of work in decryption and encryption, packet processing, and data forwarding.
Due to these variations, it can be difficult to understand the capacity of the SFU for an specific application. We provide tooling that help with simulating workload according to your specifications.
Load testing
The LiveKit
CLI
includes the
lk load-test
subcommand, which can simulate real-world loading conditions for various scenarios. It uses the Go SDK to simulate publishers and subscribers in a room.
When publishing, it could send both video and audio tracks:
video: looping video clips at 720p, with keyframes every ~3s (simulcast enabled)
audio: sends blank packets that aren't audible, but would simulate a target bitrate.
As a subscriber, it can simulate an application that takes advantage of adaptive stream, rendering a specified number of remote streams on-screen.
When benchmarking with the load tester, be sure to run it on a machine with plenty of CPU and bandwidth, and ensure it has sufficient file handles (
ulimit -n 65535
). You can also run the load tester from multiple machines.
Caution
Load testing traffic on your cloud instance
will
count toward your
quotas
, and is subject to the limits of your plan.
Benchmarks
We've run benchmarks for a few common scenarios to give a general understanding of performance. All benchmarks below are to demonstrate max number of participants supported in a single room.
All benchmarks were ran with the server running on a 16-core, compute optimized instance on Google Cloud. (
c2-standard-16
)
In the tables below:
Pubs
- Number of publishers
Subs
- Number of subscribers
Audio only
This simulates an audio only experience with a large number of listeners in the room. It uses an average
audio bitrate of 3kbps. In large audio sessions, only a small number of people are usually speaking (while everyone are on mute). We use 10 as the approximate number of speakers here.
Use case
Pubs
Subs
Bytes/s in/out
Packets/s in/out
CPU utilization
Large audio rooms
10
3000
7.3 kBps / 23 MBps
305 / 959,156
80%
Command:
lk load-test
\
--url
<
YOUR-SERVER-URL
>
\
--api-key
<
YOUR-KEY
>
\
--api-secret
<
YOUR-SECRET
>
\
--room
load-test
\
--audio-publishers
10
\
--subscribers
1000
Copy
Video room
Default video resolution of 720p was used in the load tests.
Use case
Pubs
Subs
Bytes/s in/out
Packets/s in/out
CPU utilization
Large meeting
150
150
50 MBps / 93 MBps
51,068 / 762,749
85%
Livestreaming
1
3000
233 kBps / 531 MBps
246 / 560,962
92%
To simulate large meeting:
lk load-test
\
--url
<
YOUR-SERVER-URL
>
\
--api-key
<
YOUR-KEY
>
\
--api-secret
<
YOUR-SECRET
>
\
--room
load-test
\
--video-publishers
150
\
--subscribers
150
Copy
To simulate livestreaming:
lk load-test
\
--url
<
YOUR-SERVER-URL
>
\
--api-key
<
YOUR-KEY
>
\
--api-secret
<
YOUR-SECRET
>
\
--room
load-test
\
--video-publishers
1
\
--subscribers
3000
\
Copy
On this page
Measuring performance
Load testing
Benchmarks
Audio only
Video room
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/self-hosting/egress:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Requirements
Config
Running locally
Helm
Ensuring availability
Autoscaling with Helm
Chrome sandboxing
When multiple Egress workers are deployed, they will automatically load-balance and ensure requests are distributed across worker instances.
Requirements
Certain kinds of Egress operations can be resource-intensive. We recommend giving each Egress instance at least
4 CPUs
and
4 GB
of memory.
An Egress worker may process one or more jobs at once, depending on their resource requirements. For example, a TrackEgress job consumes minimal resources because it doesn't need to transcode. Consequently, hundreds of simulteneous TrackEgress jobs can run on a single instance.
Note
As of
v1.7.6
, chrome sandboxing is enabled for increased security.
As a result, the service is no longer run as the
root
user inside docker, and all Egress deployments (even local) require
--cap-add=SYS_ADMIN
in your
docker run
command.
Without it, all web and room composite egress will fail with a
chrome failed to start
error.
Config
The Egress service takes a yaml config file:
# Required fields
api_key
:
livekit server api key. LIVEKIT_API_KEY env can be used instead
api_secret
:
livekit server api secret. LIVEKIT_API_SECRET env can be used instead
ws_url
:
livekit server websocket url. LIVEKIT_WS_URL can be used instead
redis
:
address
:
must be the same redis address used by your livekit server
username
:
redis username
password
:
redis password
db
:
redis db
# Optional fields
health_port
:
if used
,
will open an http port for health checks
template_port
:
port used to host default templates (default 7980)
prometheus_port
:
port used to collect prometheus metrics. Used for autoscaling
log_level
:
debug
,
info
,
warn
,
or error (default info)
template_base
:
can be used to host custom templates (default http
:
//localhost
:
<template_port
>
/)
enable_chrome_sandbox
:
if true
,
egress will run Chrome with sandboxing enabled. This requires a specific Docker setup
,
see below.
insecure
:
can be used to connect to an insecure websocket (default false)
# File upload config - only one of the following. Can be overridden per-request
s3
:
access_key
:
AWS_ACCESS_KEY_ID env can be used instead
secret
:
AWS_SECRET_ACCESS_KEY env can be used instead
region
:
AWS_DEFAULT_REGION env can be used instead
endpoint
:
optional custom endpoint
bucket
:
bucket to upload files to
azure
:
account_name
:
AZURE_STORAGE_ACCOUNT env can be used instead
account_key
:
AZURE_STORAGE_KEY env can be used instead
container_name
:
container to upload files to
gcp
:
credentials_json
:
GOOGLE_APPLICATION_CREDENTIALS env can be used instead
bucket
:
bucket to upload files to
Copy
The config file can be added to a mounted volume with its location passed in the EGRESS_CONFIG_FILE env var, or its body can be passed in the EGRESS_CONFIG_BODY env var.
Running locally
These changes are
not
recommended for a production setup.
To run against a local livekit server, you'll need to do the following:
open
/usr/local/etc/redis.conf
and comment out the line that says
bind 127.0.0.1
change
protected-mode yes
to
protected-mode no
in the same file
find your IP as seen by docker
ws_url
needs to be set using the IP as Docker sees it
on linux, this should be
172.17.0.1
on mac or windows, run
docker run -it --rm alpine nslookup host.docker.internal
and you should see something like
Name:	host.docker.internal Address: 192.168.65.2
These changes allow the service to connect to your local redis instance from inside the docker container.
Create a directory to mount. In this example, we will use
~/livekit-egress
.
Create a config.yaml in the above directory.
redis
and
ws_url
should use the above IP instead of
localhost
insecure
should be set to true
log_level
:
debug
api_key
:
your
-
api
-
key
api_secret
:
your
-
api
-
secret
ws_url
:
ws
:
//192.168.65.2
:
7880
insecure
:
true
redis
:
address
:
192.168.65.2
:
6379
Copy
Then to run the service:
docker
run
--rm
\
--cap-add SYS_ADMIN
\
-e
EGRESS_CONFIG_FILE
=
/out/config.yaml
\
-v
~/livekit-egress:/out
\
livekit/egress
Copy
You can then use our
CLI
to submit recording requests to your server.
Helm
If you already deployed the server using our Helm chart, jump to
helm install
below.
Ensure
Helm
is installed on your machine.
Add the LiveKit repo
helm repo
add
livekit https://helm.livekit.io
Copy
Create a values.yaml for your deployment, using
egress-sample.yaml
as a template.
Each instance can record one room at a time, so be sure to either enable autoscaling, or set replicaCount >= the number of rooms you'll need to simultaneously record.
Then install the chart
helm
install
<
INSTANCE_NAME
>
livekit/egress
--namespace
<
NAMESPACE
>
--values
values.yaml
Copy
We'll publish new version of the chart with new egress releases. To fetch these updates and upgrade your installation, perform
helm repo update
helm upgrade
<
INSTANCE_NAME
>
livekit/egress
--namespace
<
NAMESPACE
>
--values
values.yaml
Copy
Ensuring availability
Room Composite egress can use anywhere between 2-6 CPUs. For this reason, it is recommended to use
pods with 4 CPUs if you will be using room composite egress.
The
livekit_egress_available
Prometheus metric is also provided to support autoscaling.
prometheus_port
must be defined in your config.
With this metric, each instance looks at its own CPU utilization and decides whether it is available to accept incoming requests.
This can be more accurate than using average CPU or memory utilization, because requests are long-running and are resource intensive.
To keep at least 3 instances available:
sum
(
livekit_egress_available
)
>
3
Copy
To keep at least 30% of your egress instances available:
sum
(
livekit_egress_available
)
/
sum
(
kube_pod_labels
{
label_project
=
~
"^.*egress.*"
}
)
>
0.3
Copy
Autoscaling with Helm
There are 3 options for autoscaling:
targetCPUUtilizationPercentage
,
targetMemoryUtilizationPercentage
, and
custom
.
autoscaling
:
enabled
:
false
minReplicas
:
1
maxReplicas
:
5
#  targetCPUUtilizationPercentage: 60
#  targetMemoryUtilizationPercentage: 60
#  custom:
#    metricName: my_metric_name
#    targetAverageValue: 70
Copy
To use
custom
, you'll need to install the prometheus adapter. You can then create a kubernetes custom metric based off the
livekit_egress_available
prometheus metric.
You can find an example on how to do this
here
.
Chrome sandboxing
By default, Room Composite and Web egresses run with Chrome sandboxing disabled. This is because the default docker security settings prevent Chrome from
switching to a different kernel namespace, which is needed by Chrome to setup its sandbox.
Chrome sandboxing within Egress can be reenabled by setting the
enable_chrome_sandbox
option to
true
in the egress configuration, and launching docker using the
provided
seccomp security profile
:
docker
run
--rm
\
-e
EGRESS_CONFIG_FILE
=
/out/config.yaml
\
-v
~/egress-test:/out
\
--security-opt
seccomp
=
chrome-sandboxing-seccomp-profile.json
\
livekit/egress
Copy
This profile is based on the
default docker seccomp security profile
and allows
the 2 extra system calls (
clone
and
unshare
) that Chrome needs to setup the sandbox.
Note that kubernetes disables seccomp entirely by default, which means that running with Chrome sandboxing enabled is possible on a kubernetes cluster with
the default security settings.
On this page
Requirements
Config
Running locally
Helm
Ensuring availability
Autoscaling with Helm
Chrome sandboxing
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/self-hosting/ingress:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Requirements
Config
Ingress Service
LiveKit Server
Health checks
Running natively on a host
Prerequisites
Configuration
Running the service
Running with Docker
Helm
Ensuring availability
Autoscaling with Helm
Requirements
If more than one Ingress worker is needed, the service must be setup behind a TCP load balancer (or HTTP reverse proxy for WHIP) to assign an incoming RTMP or WHIP request to an available instance. The load balancer is also responsible for TLS termination and performing Ingress node health checks.
Certain kinds of Ingress operations can be resource-intensive. We recommend giving each Ingress instance at least
4 CPUs
and
4 GB
of memory.
If WHIP support is enabled, the instance will also need access to UDP port 7885.
An Ingress worker may process one or more jobs at once, depending on their resource requirements. For example, a WHIP session with transcoding bypassed consumes minimal resources. For Ingress with transcoding enabled, such as RTMP or WHIP with transcoding bypass disabled, the amount of required resources depend on the video resolution and amount of video layers configured in the Ingress video settings.
Config
Ingress Service
The Ingress service takes a yaml config file:
# Required fields
api_key
:
livekit server api key. LIVEKIT_API_KEY env can be used instead
api_secret
:
livekit server api secret. LIVEKIT_API_SECRET env can be used instead
ws_url
:
livekit server websocket url. LIVEKIT_WS_URL can be used instead
redis
:
address
:
must be the same redis address used by your livekit server
username
:
redis username
password
:
redis password
db
:
redis db
# Optional fields
health_port
:
if used
,
will open an http port for health checks
prometheus_port
:
port used to collect Prometheus metrics. Used for autoscaling
rtmp_port
:
TCP port to listen for RTMP connections on (default 1935)
whip_port
:
TCP port to listen for WHIP connections on (default 8080)
http_relay_port
:
TCP port for communication between the main service process and session handler processes
,
on localhost (default 9090)
logging
:
level
:
debug
,
info
,
warn
,
or error (default info)
rtc_config
:
tcp_port
:
TCP port to use for ICE connections on (default disabled)
udp_port
:
UDP port to use for ICE connections on (default 7885)
use_external_ip
:
whether to use advertise the server public facing IP address for ICE connections
# use_external_ip should be set to true for most cloud environments where
# the host has a public IP address, but is not exposed to the process.
# LiveKit will attempt to use STUN to discover the true IP, and convenient
# that IP with its clients
cpu_cost
:
rtmp_cpu_cost
:
cpu resources to reserve when accepting RTMP sessions
,
in fraction of core count
whip_cpu_cost
:
cpu resources to reserve when accepting WHIP sessions
,
in fraction of core count
whip_bypass_transcoding_cpu_cost
:
cpu resources to reserve when accepting WHIP sessions with transcoding disabled
,
in fraction of core count
Copy
The location of the config file can be passed in the INGRESS_CONFIG_FILE env var, or its body can be passed in the INGRESS_CONFIG_BODY env var.
LiveKit Server
LiveKit Server serves as the API endpoint for the CreateIngress API calls.
Therefore, it needs to know the location of the Ingress service to provide the
Ingress URL to clients.
To achieve this, include the following in the LiveKit Server's configuration:
ingress
:
rtmp_base_url
:
'rtmps://my.domain.com/live'
whip_base_url
:
'https://my.domain.com/whip'
Copy
Health checks
The Ingress service provides HTTP endpoints for both health and availability checks. The heath check endpoint will always return a 200 status code if the Ingress service is running. The availability endpoint will only return 200 if the server load is low enough that a new request with the maximum cost, as defined in the
cpu_cost
section of the configuration file, can still be handled.
Health and availability check endpoints are exposed in 2 different ways:
A dedicated HTTP server that can be enabled by setting the
health_port
configuration entry. The health check endpoint is running at the root of the HTTP server (
/
), while the availability endpoint is available at
/availability
If enabled, the WHIP server also exposes a heath check endpoint at
/health
and an availability endpoint at
/availability
Running natively on a host
This documents how to run the Ingress service natively on a host server. This setup is convenient for testing and development, but not advised in production.
Prerequisites
The Ingress service can be run natively on any platform supported by GStreamer.
The Ingress service is built in Go. Go >= 1.18 is needed. The following
GStreamer
libraries and headers must be installed:
gstreamer
gst-plugins-base
gst-plugins-good
gst-plugins-bad
gst-plugins-ugly
gst-libav
On MacOS, these can be installed using
Homebrew
by running
mage bootstrap
.
In order to run Ingress against a local LiveKit server, a Redis server must be running on the host.
Building
Build the Ingress service by running:
mage build
Copy
Configuration
All servers must be configured to communicate over localhost. Create a file named
config.yaml
with the following content:
logging
:
level
:
debug
api_key
:
<YOUR_API_KEY
>
api_secret
:
<YOUR_API_SECRET
>
ws_url
:
ws
:
//localhost
:
7880
redis
:
address
:
localhost
:
6379
Copy
Running the service
On MacOS, if GStreamer was installed using Homebrew, the following environment must be set:
export
GST_PLUGIN_PATH
=
/opt/homebrew/Cellar/gst-plugins-base:/opt/homebrew/Cellar/gst-plugins-good:/opt/homebrew/Cellar/gst-plugins-bad:/opt/homebrew/Cellar/gst-plugins-ugly:/opt/homebrew/Cellar/gst-plugins-bad:/opt/homebrew/Cellar/gst-libav
Copy
Then to run the service:
ingress
--config
config.yaml
Copy
Running with Docker
To run against a local LiveKit server, a Redis server must be running locally. The Ingress service must be instructed to connect to LiveKit server and Redis on the host. The host network is accessible from within the container on IP:
host.docker.internal on MacOS and Windows
172.17.0.1 on linux
Create a file named
config.yaml
with the following content:
log_level
:
debug
api_key
:
<YOUR_API_KEY
>
api_secret
:
<YOUR_API_SECRET
>
ws_url
:
ws
:
//host.docker.internal
:
7880 (or ws
:
//172.17.0.1
:
7880 on linux)
redis
:
address
:
host.docker.internal
:
6379 (or 172.17.0.1
:
6379 on linux)
Copy
In order to be able to use establish WHIP sessions over UDP, the container must be run with host networking enabled.
Then to run the service:
docker
run
--rm
\
-e
INGRESS_CONFIG_BODY
=
"
`
cat
config.yaml
`
"
\
-p
1935
:1935
\
-p
8080
:8080
\
--network
host
\
livekit/ingress
Copy
Helm
If you already deployed the server using our Helm chart, jump to
helm install
below.
Ensure
Helm
is installed on your machine.
Add the LiveKit repo
helm repo
add
livekit https://helm.livekit.io
Copy
Create a values.yaml for your deployment, using
ingress-sample.yaml
as a template.
Each instance can handle a few transcoding-enabled Ingress at a time, so be sure to either enable autoscaling, or set replicaCount accordingly.
Then install the chart
helm
install
<
INSTANCE_NAME
>
livekit/ingress
--namespace
<
NAMESPACE
>
--values
values.yaml
Copy
We'll publish new version of the chart with new Ingress releases. To fetch these updates and upgrade your installation, perform
helm repo update
helm upgrade
<
INSTANCE_NAME
>
livekit/ingress
--namespace
<
NAMESPACE
>
--values
values.yaml
Copy
Ensuring availability
An Ingress with transcoding enabled can use anywhere between 2-6 CPU cores. For this reason, it is recommended to use pods with 4 CPUs if you will be transcoding incoming media.
The
livekit_ingress_available
Prometheus metric is also provided to support autoscaling.
prometheus_port
must be defined in your config.
With this metric, each instance looks at its own CPU utilization and decides whether it is available to accept incoming requests.
This can be more accurate than using average CPU or memory utilization, because requests are long-running and are resource intensive.
To keep at least 3 instances available:
sum
(
livekit_ingress_available
)
>
3
Copy
To keep at least 30% of your Ingress instances available:
sum
(
livekit_ingress_available
)
/
sum
(
kube_pod_labels
{
label_project
=
~
"^.*ingress.*"
}
)
>
0.3
Copy
Autoscaling with Helm
There are 3 options for autoscaling:
targetCPUUtilizationPercentage
,
targetMemoryUtilizationPercentage
, and
custom
.
autoscaling
:
enabled
:
false
minReplicas
:
1
maxReplicas
:
5
#  targetCPUUtilizationPercentage: 60
#  targetMemoryUtilizationPercentage: 60
#  custom:
#    metricName: my_metric_name
#    targetAverageValue: 70
Copy
To use
custom
, you'll need to install the Prometheus adapter. You can then create a Kubernetes custom metric based off the
livekit_ingress_available
Prometheus metric.
You can find an example on how to do this
here
.
On this page
Requirements
Config
Ingress Service
LiveKit Server
Health checks
Running natively on a host
Prerequisites
Configuration
Running the service
Running with Docker
Helm
Ensuring availability
Autoscaling with Helm
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/self-hosting/sip-server:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Docker compose
Running natively
1. Install SIP server
2. Create config file
3. Run SIP server:
4. Determine your SIP URI
Caution
Both SIP signaling port (
5060
) and media port range (
10000-20000
) must be accessible from the Internet.
See
Firewall configuration
for details.
Docker compose
The easiest way to run SIP Server is by using Docker Compose:
wget
https://raw.githubusercontent.com/livekit/sip/main/docker-compose.yaml
docker
compose up
Copy
This starts a local LiveKit Server and SIP Server connected to Redis.
Running natively
You may also run SIP server natively without Docker.
1. Install SIP server
Follow instructions
here
.
2. Create config file
Create a file named
config.yaml
with the following content:
api_key
:
<your
-
api
-
key
>
api_secret
:
<your
-
api
-
secret
>
ws_url
:
ws
:
//localhost
:
7880
redis
:
address
:
localhost
:
6379
sip_port
:
5060
rtp_port
:
10000
-
20000
use_external_ip
:
true
logging
:
level
:
debug
Copy
3. Run SIP server:
livekit-sip
--config
=
config.yaml
Copy
4. Determine your SIP URI
Once your SIP server is running, you would have to determine the publilc IP address of the machine.
Then your SIP URI would be:
<
public-ip-address
>
:5060
Copy
On this page
Docker compose
Running natively
1. Install SIP server
2. Create config file
3. Run SIP server:
4. Determine your SIP URI
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/quickstarts/configuring-sip-trunk:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
External provider setup
LiveKit setup
Inbound trunk setup
Create a dispatch rule
Create an outbound trunk
Next steps
Additional documentation
Overview
LiveKit is compatible with any SIP trunking provider. This guide provides general instructions for setting up a SIP trunk with an external provider and then associating it with your LiveKit project.
External provider setup
The usual steps to create a SIP trunk are as follows:
Create a SIP trunk with your provider.
Add authentication or limit trunk usage by phone numbers or IP addresses.
Purchase a phone number and associate it with your SIP trunk.
Add your
LiveKit SIP URI
to the SIP trunk.
For step-by-step instructions for Telnyx, Twilio, or Plivo, see the following quickstarts:
Twilio Setup
Step-by-step instructions for setting up a SIP trunk with Twilio.
Telnyx Setup
Step-by-step instructions for setting up a SIP trunk with Telnyx.
Plivo Setup
Step-by-step instructions for setting up a SIP trunk with Plivo.
LiveKit setup
Now you are ready to configure your LiveKit project to use the SIP trunk.
The following steps are common to all SIP trunking providers.
LiveKit CLI
These examples use the
LiveKit CLI
. For additional examples and full documentation, see the linked documentation for each component.
Inbound trunk setup
An
inbound trunk
allows you to accept incoming phone calls.
Create a temporary JSON file with your inbound trunk configuration, including the phone number you purchased from your SIP trunking provider.
inbound-trunk.json
{
"trunk"
:
{
"name"
:
"My inbound trunk"
,
"numbers"
:
[
"+15105550123"
]
}
}
Copy
Use the LiveKit CLI to register the inbound trunk to your project.
lk sip inbound create inbound-trunk.json
Copy
Create a dispatch rule
You must set up at least one
dispatch rule
to accept incoming calls into a LiveKit room.
Create a temporary JSON file with your dispatch rule configuration. This example creates a dispatch rule that puts each caller into a randomly generated unique room using the name prefix
call-
. For many applications, this is the only configuration you need.
dispatch-rule.json
{
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call-"
}
}
}
Copy
Use the LiveKit CLI to create the dispatch rule.
lk sip dispatch create dispatch-rule.json
Copy
Create an outbound trunk
Create an
outbound trunk
to make outgoing phone calls with LiveKit.
Create a temporary JSON file with your outbound trunk configuration. This example creates an outbound trunk with the phone number
+55512345678
and the trunk domain name
my-trunk-domain-name
.
outbound-trunk.json
{
"trunk"
:
{
"name"
:
"My outbound trunk"
,
"address"
:
"<my-trunk-domain-name>"
,
"numbers"
:
[
"+15105550123"
]
,
"auth_username"
:
"<username>"
,
"auth_password"
:
"<password>"
}
}
Copy
Use the LiveKit CLI to create the outbound trunk.
lk sip outbound create outbound-trunk.json
Copy
Now you are ready to
place outgoing calls
.
Next steps
See the following guides to continue building your telephony app.
Telephony agents
Building telephony-based voice AI apps with LiveKit Agents.
Make outbound calls
Detailed instructions for making outbound calls.
Additional documentation
See the following documentation for more details on the topics covered in this guide.
Inbound trunk
Detailed instructions for setting up inbound trunks.
Dispatch rule
Detailed instructions for setting up dispatch rules.
Outbound trunk
Detailed instructions for setting up outbound trunks.
On this page
Overview
External provider setup
LiveKit setup
Inbound trunk setup
Create a dispatch rule
Create an outbound trunk
Next steps
Additional documentation
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/quickstarts/configuring-twilio-trunk:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Creating a SIP trunk for inbound and outbound calls
Prerequisites
Step 1. Create a SIP trunk
Step 2: Configure your trunk
Step 3: Associate phone number and trunk
Configure a SIP trunk using the Twilio UI
Next steps
Note
If you're using LiveKit Cloud as your SIP server and you're signed in, your SIP URI is automatically
included in the code blocks where appropriate.
Use the following steps to configure inbound and outbound SIP trunks using
Twilio
.
Creating a SIP trunk for inbound and outbound calls
Create a Twilio SIP trunk for incoming or outgoing calls, or both, using the following steps.
To use the Twilio console, see
Configure a SIP trunk using the Twilio UI
.
Note
For inbound calls, you can use TwiML for Programmable Voice instead of setting up Elastic SIP Trunking. To learn more,
see
Inbound calls with Twilio Voice
.
Prerequisites
Purchase phone number
.
Install the Twilio CLI
.
Create a
Twilio profile
to use the CLI.
Step 1. Create a SIP trunk
The domain name for your SIP trunk  must end in
pstn.twilio.com
. For example to create a trunk named
My test trunk
with the domain name
my-test-trunk.pstn.twilio.com
, run the following command:
twilio api trunking v1 trunks create
\
--friendly-name
"My test trunk"
\
--domain-name
"my-test-trunk.pstn.twilio.com"
Copy
The output includes the trunk SID. Copy it for use in the following steps.
Step 2: Configure your trunk
Configure the trunk for inbound calls or outbound calls or both. To create a SIP trunk for both inbound and
outbound calls, follow the steps in both tabs:
Inbound
Outbound
For inbound trunks, configure an
origination URI
. If you're
using LiveKit Cloud and are signed in, your
<your SIP host>
is filled in below:
twilio api trunking v1 trunks origination-urls create
\
--trunk-sid
<
twilio_trunk_sid
>
\
--friendly-name
"LiveKit SIP URI"
\
--sip-url
"sip:<your SIP host>;transport=tcp"
\
--weight
1
--priority
1
--enabled
Copy
Step 3: Associate phone number and trunk
The Twilio trunk SID and phone number SID are included in the output of
previous steps. If you didn't copy the SIDs, you can list them using the following commands:
To list phone numbers:
twilio phone-numbers list
To list trunks:
twilio api trunking v1 trunks list
twilio api trunking v1 trunks phone-numbers create
\
--trunk-sid
<
twilio_trunk_sid
>
\
--phone-number-sid
<
twilio_phone_number_sid
>
Copy
Configure a SIP trunk using the Twilio UI
Sign in to the
Twilio console
.
Purchase a phone number
.
Create SIP Trunk
on Twilio:
Select
Elastic SIP Trunking
»
Manage
»
Trunks
.
Create a SIP trunk.
Tip
Using your Twilio API key, you can skip the next two steps by using
this snippet
to set your origination and termination URLs automatically.
For inbound calls:
Navigate to
Voice
»
Manage
»
Origination connection policy
, and create an
Origination Connection Policy
Select the policy you just created and set the
Origination SIP URI
to
sip:<your SIP URI>;transport=tcp
. For example, if your SIP URI is
1abcdefgh2i.sip.livekit.cloud
then enter
sip:1abcdefgh2i.sip.livekit.cloud;transport=tcp
.
For outbound calls, configure termination and authentication:
Navigate to
Elastic SIP Trunking
»
Manage
»
Trunks
.
Copy the
Termination SIP URI
to use
when you create an
outbound trunk
for LiveKit.
Configure
Authentication
:
Select
Elastic SIP Trunking
»
Manage
»
Credential lists
and create a new credential list
with a username and password of your choice.
Associate your trunk with the credential list:
Select
Elastic SIP Trunking
»
Manage
»
Trunks
and select the outbound trunk created in the
previous steps.
Select
Termination
» *
Authentication
»
Credential Lists
and select the credential list you
just created.
Next steps
Head back to the main setup documentation to finish connecting your SIP trunk to LiveKit.
SIP trunk setup
Configure your Twilio trunk in LiveKit.
On this page
Creating a SIP trunk for inbound and outbound calls
Prerequisites
Step 1. Create a SIP trunk
Step 2: Configure your trunk
Step 3: Associate phone number and trunk
Configure a SIP trunk using the Twilio UI
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/quickstarts/configuring-telnyx-trunk:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Creating a Telnyx SIP trunk using the API
Prerequisite
Step 1: Create an environment variable for API key
Step 2: Create an FQDN connection
Step 3: Associate phone number and trunk
Creating a SIP trunk using the Telnyx UI
Next steps
Note
If you're using LiveKit Cloud as your SIP server and you're signed in, your SIP URI is automatically
included in the code blocks where appropriate.
Creating a Telnyx SIP trunk using the API
You can use
curl
command to make calls to the Telnyx API V2. The commands in the steps below use the
example phone number,
+15105550100
. To use the Telnyx console, see
Creating a SIP trunk using the Telnyx UI
.
Prerequisite
Purchase a
Telnyx phone number
.
Step 1: Create an environment variable for API key
If you don't have a key a Telnyx API V2 key,
see the
Telnyx guide to create one
.
export
TELNYX_API_KEY
=
"<your_api_v2_key>"
Copy
Step 2: Create an FQDN connection
The following inbound and outbound commands include the required configuration settings if you plan on using only an inbound or
outbound trunk for your LiveKit telephony app. However, by default, an
FQDN connection
creates both an inbound and outbound trunk.
Creating an FQDN connection. Depending on your use case, select
Inbound
,
Outbound
, or
Inbound and outbound
to
accept calls, make calls, or both:
Inbound
Outbound
Inbound and Outbound
Set the caller's number format to
+E.164
for inbound calls (this identifies the caller's number with a leading
+
):
curl
-L
'https://api.telnyx.com/v2/fqdn_connections'
\
-H
'Content-Type: application/json'
\
-H
'Accept: application/json'
\
-H
"Authorization: Bearer
$TELNYX_API_KEY
"
\
-d
'{
"active": true,
"anchorsite_override": "Latency",
"connection_name": "My LiveKit trunk",
"transport_protocol": "TCP",
"inbound": {
"ani_number_format": "+E.164",
"dnis_number_format": "+e164"
}
}'
Copy
Copy the FQDN connection ID from the output:
{
"data"
:
{
"id"
:
"<connection_id>"
,
...
}
}
Copy
Create an FQDN with your SIP URI and your FQDN connection ID:
curl
-L
'https://api.telnyx.com/v2/fqdns'
\
-H
'Content-Type: application/json'
\
-H
'Accept: application/json'
\
-H
"Authorization: Bearer
$TELNYX_API_KEY
"
\
-d
'{
"connection_id": "<connection_id>",
"fqdn": "<your SIP host>",
"port": 5060,
"dns_record_type": "a"
}'
Copy
Step 3: Associate phone number and trunk
Get the phone number ID for phone number
5105550100
:
curl
-L
-g
'https://api.telnyx.com/v2/phone_numbers?filter[phone_number]=5105550100'
\
-H
'Accept: application/json'
\
-H
"Authorization: Bearer
$TELNYX_API_KEY
"
Copy
Copy the phone number ID from the output:
{
"meta"
:
{
"total_pages"
:
1
,
"total_results"
:
1
,
"page_number"
:
1
,
"page_size"
:
100
}
,
"data"
:
[
{
"id"
:
"<phone_number_id>"
,
...
}
]
}
Copy
Add the FQDN connection to the phone number:
curl
-L
-X
PATCH
'https://api.telnyx.com/v2/phone_numbers/<phone_number_id>'
\
-H
'Content-Type: application/json'
\
-H
'Accept: application/json'
\
-H
"Authorization: Bearer
$TELNYX_API_KEY
"
\
-d
'{
"id": "<phone_number_id>",
"connection_id": "<connection_id>"
}'
Copy
Creating a SIP trunk using the Telnyx UI
Sign in to the
Telnyx portal
.
Purchase a phone number
.
Navigate to
Voice
»
SIP Trunking
.
Create a SIP connection:
For inbound calls:
Select
FQDN
and save.
Select
Add FQDN
and enter <your SIP URI> from above into the
FQDN
field.
Select the
Inbound
tab. In the
Destination Number Format
field, select
+E.164
.
In the
SIP Transport Protocol
field, select either
TCP
or
UDP
. (we recommend TCP)
In the
SIP Region
field, select your region.
For outbound calls:
Select the
Outbound
tab.
In the
Outbound Voice Profile
field, select or create an outbound voice profile.
Select the
Settings
tab
Configure
FQDN Authentication
:
Select the
Settings
tab.
In the
Authentication & Routing Configuration
section, select
Outbound Calls Authentication
.
In the
Authentication Method
field, select
Credentials
and enter a username and password.
Select the
Numbers
tab and assign the purchased number to the SIP trunk.
Next steps
Head back to the main setup documentation to finish connecting your SIP trunk to LiveKit.
SIP trunk setup
Configure your Telnyx trunk in LiveKit.
On this page
Creating a Telnyx SIP trunk using the API
Prerequisite
Step 1: Create an environment variable for API key
Step 2: Create an FQDN connection
Step 3: Associate phone number and trunk
Creating a SIP trunk using the Telnyx UI
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/sip/quickstarts/configuring-plivo-trunk:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Creating a SIP trunk using the Plivo Console
Prerequisites
Create a SIP trunk
Next steps
Use the following steps to configure inbound and outbound SIP trunks using
Plivo
.
Creating a SIP trunk using the Plivo Console
Create a Plivo SIP trunk for incoming or outgoing calls, or both, using the following steps.
Prerequisites
A phone number to make/receive calls
.
Create a SIP trunk
Sign in to the
Plivo Console
.
Navigate to
Zentrunk Dashboard
.
Create a SIP connection:
Inbound
Outbound
Select
Create New Inbound Trunk
and provide a descriptive name for your trunk.
Under
Trunk Authentication
, click
Add New URI
.
Enter your LiveKit SIP URI and configure the transport protocol to TCP.
For example, if your SIP URI is
sip:1abcdefgh2i.sip.livekit.cloud
then enter
1abcdefgh2i.sip.livekit.cloud;transport=tcp
Select
Create Trunk
to complete your inbound trunk creation.
Navigate to the
Phone Numbers Dashboard
and select the number to route to your inbound trunk.
Under
Number Configuration
, set
Trunk
to your newly created inbound trunk and select
Update
to save.
Next steps
Head back to the main setup documentation to finish connecting your SIP trunk to LiveKit.
SIP trunk setup
Configure your Plivo trunk in LiveKit.
On this page
Creating a SIP trunk using the Plivo Console
Prerequisites
Create a SIP trunk
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/voice-agent/voice-pipeline:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Example agent
Model options
Modify context before LLM
Altering text before TTS
Turn detection thresholds
VAD settings
Interruption handling
Manual Interruptions
Emitted events
Events example
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Building voice agents
.
v1.0 for Node.js is coming soon.
Overview
VoicePipelineAgent is a high-level abstraction that orchestrates conversation flow using a pipeline of three main models: STT → LLM → TTS. Additional models, like VAD, are used to enhance the conversation flow.
Realtime APIs
To use a realtime model like the OpenAI Realtime API, see the
Multimodal Agent
guide instead.
Example agent
The following is an example of an AI agent created with the
VoicePipelineAgent
class:
Python
Node.js
from
livekit
.
agents
import
llm
from
livekit
.
agents
.
pipeline
import
VoicePipelineAgent
from
livekit
.
plugins
import
cartesia
,
deepgram
,
openai
,
silero
initial_ctx
=
llm
.
ChatContext
(
)
.
append
(
role
=
"system"
,
text
=
"<your prompt>"
,
)
agent
=
VoicePipelineAgent
(
vad
=
silero
.
VAD
.
load
(
)
,
# flexibility to use any models
stt
=
deepgram
.
STT
(
model
=
"nova-2-general"
)
,
llm
=
openai
.
LLM
(
)
,
tts
=
cartesia
.
TTS
(
)
,
# intial ChatContext with system prompt
chat_ctx
=
initial_ctx
,
# whether the agent can be interrupted
allow_interruptions
=
True
,
# sensitivity of when to interrupt
interrupt_speech_duration
=
0.5
,
interrupt_min_words
=
0
,
# minimal silence duration to consider end of turn
min_endpointing_delay
=
0.5
,
# callback to run before LLM is called, can be used to modify chat context
before_llm_cb
=
None
,
# callback to run before TTS is called, can be used to customize pronounciation
before_tts_cb
=
None
,
)
# start the participant for a particular room, taking audio input from a single participant
agent
.
start
(
room
,
participant
)
Copy
Model options
Options on the models can be customized when creating the plugin objects. For example, you can adjust the model and temperature of the LLM like this:
Python
Node.js
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
,
temperature
=
0.5
,
)
Copy
Modify context before LLM
The
before_llm_cb
callback allows you to modify the
ChatContext
before it is sent to the LLM model. This is useful for adding extra context or adjusting the context based on the conversation. For example, when the context becomes too long, you can truncate it to optimize the amount of tokens used in inference.
Python
Node.js
async
def
truncate_context
(
assistant
:
VoicePipelineAgent
,
chat_ctx
:
llm
.
ChatContext
)
:
if
len
(
chat_ctx
.
messages
)
>
15
:
chat_ctx
.
messages
=
chat_ctx
.
messages
[
-
15
:
]
agent
=
VoicePipelineAgent
(
.
.
.
before_llm_cb
=
truncate_context
,
)
Copy
Altering text before TTS
The
before_tts_cb
callback allows you to modify the text before it is sent to the TTS model. This is useful for customizing pronunciation or adding extra context to the text.
Python
Node.js
from
livekit
.
agents
import
tokenize
from
livekit
.
agents
.
pipeline
import
VoicePipelineAgent
def
replace_words
(
assistant
:
VoicePipelineAgent
,
text
:
str
|
AsyncIterable
[
str
]
)
:
return
tokenize
.
utils
.
replace_words
(
text
=
text
,
replacements
=
{
"livekit"
:
r"<<l|aɪ|v|k|ɪ|t|>>"
}
)
agent
=
VoicePipelineAgent
(
.
.
.
before_tts_cb
=
replace_words
,
)
Copy
Turn detection thresholds
The following setting is available for voice activity detection. In addition to VAD, you can enable
LiveKit's turn detection model to work in conjunction with VAD for a more natural conversational flow.
To learn more, see
Turn detection
.
VAD settings
min_endpointing_delay
defines the minimum silence duration to detect the end of a turn. The default
value is
0.5
(500 ms). Increasing this value allows for longer pauses before the agent assumes the
user has finished speaking.
Interruption handling
When the user interrupts, the agent stops speaking and switches to listening mode, storing the position of the speech played so far in its ChatContext.
There are three flags that control the interruption behavior:
allow_interruptions
: set to
False
to disable user interruptions.
interrupt_speech_duration
: the minimum speech duration (detected by VAD) required to consider the interruption intentional.
interrupt_min_words
: the minimum number of transcribed words needed for the interruption to be considered intentional.
Manual Interruptions
You can manually interrupt an agent using the
agent.interrupt()
method. Calling this method immediately ends any agent speech.
To stop the agent from speaking any pending speech that's currently in the pipeline, you can use the
interrupt_all
parameter
to interrupt all pending speech.
Note
This method is currently only available in Python.
# Interrupt the agent's current response whenever someone joins the room
@ctx
.
room
.
on
(
"participant_connected"
)
def
on_participant_connected
(
participant
:
rtc
.
RemoteParticipant
)
:
agent
.
interrupt
(
interrupt_all
=
True
)
Copy
Emitted events
An agent emits the following events:
Event
Description
user_started_speaking
User started speaking.
user_stopped_speaking
User stopped speaking.
agent_started_speaking
Agent started speaking.
agent_stopped_speaking
Agent stopped speaking.
user_speech_committed
User's speech was committed to the chat context.
agent_speech_committed
Agent's speech was committed to the chat context.
agent_speech_interrupted
Agent was interrupted while speaking.
function_calls_collected
The complete set of functions to be executed was received.
function_calls_finished
All function calls have been executed.
metrics_collected
Metric was collected. Metrics can include time to first token for STT, LLM, TTS, duration, and usage
metrics.
Events example
For example, when a user's speech is committed to the chat context, save it to a queue for transcription:
Python
Node.js
@agent
.
on
(
"user_speech_committed"
)
def
on_user_speech_committed
(
msg
:
llm
.
ChatMessage
)
:
# convert string lists to strings, drop images
if
isinstance
(
msg
.
content
,
list
)
:
msg
.
content
=
"\n"
.
join
(
"[image]"
if
isinstance
(
x
,
llm
.
ChatImage
)
else
x
for
x
in
msg
)
log_queue
.
put_nowait
(
f"[
{
datetime
.
now
(
)
}
] USER:\n
{
msg
.
content
}
\n\n"
)
Copy
The
full example
is available in GitHub.
On this page
Overview
Example agent
Model options
Modify context before LLM
Altering text before TTS
Turn detection thresholds
VAD settings
Interruption handling
Manual Interruptions
Emitted events
Events example
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/voice-agent/multimodal-agent:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
MultimodalAgent class
Usage
Advantages of speech-to-speech agents
Emitted events
Events example
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Building voice agents
.
v1.0 for Node.js is coming soon.
The
MultimodalAgent
class is an abstraction for building AI agents using OpenAI’s Realtime API with multimodal models. These models accept audio directly, enabling them to 'hear' your voice and capture nuances like emotion, often lost in speech-to-text conversion.
MultimodalAgent class
Unlike
VoicePipelineAgent
, the
MultimodalAgent
class uses a single primary model for the conversation flow. The model is capable of processing both audio and text inputs, generating audio responses.
MultimodalAgent
is responsible for managing the conversation state, including buffering responses from the model and sending them to the user in realtime. It also handles interruptions, indicating to OpenAI's realtime API the point at which the model had been interrupted.
Usage
Python
Node.js
from
__future__
import
annotations
import
logging
from
livekit
import
rtc
from
livekit
.
agents
import
(
AutoSubscribe
,
JobContext
,
WorkerOptions
,
cli
,
llm
,
)
from
livekit
.
agents
.
multimodal
import
MultimodalAgent
from
livekit
.
plugins
import
openai
logger
=
logging
.
getLogger
(
"myagent"
)
logger
.
setLevel
(
logging
.
INFO
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
logger
.
info
(
"starting entrypoint"
)
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
participant
=
await
ctx
.
wait_for_participant
(
)
model
=
openai
.
realtime
.
RealtimeModel
(
instructions
=
"You are a helpful assistant and you love kittens"
,
voice
=
"shimmer"
,
temperature
=
0.8
,
modalities
=
[
"audio"
,
"text"
]
,
)
assistant
=
MultimodalAgent
(
model
=
model
)
assistant
.
start
(
ctx
.
room
)
logger
.
info
(
"starting agent"
)
session
=
model
.
sessions
[
0
]
session
.
conversation
.
item
.
create
(
llm
.
ChatMessage
(
role
=
"assistant"
,
content
=
"Please begin the interaction with the user in a manner consistent with your instructions."
,
)
)
session
.
response
.
create
(
)
if
__name__
==
"__main__"
:
cli
.
run_app
(
WorkerOptions
(
entrypoint_fnc
=
entrypoint
)
)
Copy
Advantages of speech-to-speech agents
Speech-to-speech agents offer several advantages over pipeline-based agents:
Natural Interactions
: Callers can speak and hear responses with extremely low latency, mimicking human-to-human conversations.
Voice and Tone
: Speech-to-speech agents are able to dynamically change the intonation and tone of their responses based on the emotions of the caller, making interactions more engaging.
Emitted events
An agent emits the following events:
Python
Node.js
Event
Description
user_started_speaking
User started speaking.
user_stopped_speaking
User stopped speaking.
agent_started_speaking
Agent started speaking.
agent_stopped_speaking
Agent stopped speaking.
user_speech_committed
User's speech was committed to the chat context.
agent_speech_committed
Agent's speech was committed to the chat context.
agent_speech_interrupted
Agent was interrupted while speaking.
Events example
Python
Node.js
When user speech is committed to the chat context, save it to a queue:
@agent
.
on
(
"user_speech_committed"
)
def
on_user_speech_committed
(
msg
:
llm
.
ChatMessage
)
:
# convert string lists to strings, drop images
if
isinstance
(
msg
.
content
,
list
)
:
msg
.
content
=
"\n"
.
join
(
"[image]"
if
isinstance
(
x
,
llm
.
ChatImage
)
else
x
for
x
in
msg
)
log_queue
.
put_nowait
(
f"[
{
datetime
.
now
(
)
}
] USER:\n
{
msg
.
content
}
\n\n"
)
Copy
On this page
MultimodalAgent class
Usage
Advantages of speech-to-speech agents
Emitted events
Events example
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/voice-agent/telephony:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview of SIP integration
How it works
Getting started
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Integrating with telephony
.
v1.0 for Node.js is coming soon.
Using LiveKit, it's easy to integrate AI voice agents with telephony systems using SIP. By connecting your agents to phone calls, you can enable users to interact with AI-powered voice assistants over traditional telephony networks.
Overview of SIP integration
SIP is a signaling protocol used for initiating, maintaining, and terminating realtime sessions that involve voice, video, and messaging applications. In the context of telephony integration with AI voice agents, SIP allows you to bridge phone calls into LiveKit rooms where your agents can interact with callers.
How it works
SIP Trunking
: You'll require a SIP trunk with a provider like
Twilio
or
Telnyx
to obtain a phone number and handle call routing.
Call Routing
: Incoming or outgoing calls are routed through the SIP trunk into a dynamically generated LiveKit room.
Agent Participation
: Your AI voice agent joins the LiveKit room as a participant.
LiveKit Rooms
: The LiveKit room acts as a bridge between the phone call and the agent, allowing them to interact in realtime. It also provides an intuitive backend API for transcriptions, recordings, moderation, and other features.
Getting started
Agent Setup
: The best way to get started with telephony applications is by creating an agent using the
Voice pipeline agent
or a speech-to-speech agent using the
OpenAI Realtime API quickstart
guide.
Inbound Calls
: To handle inbound calls from users to your agent, follow the
Inbound Calls via SIP guide
.
Outbound Calls
: To enable your agent to make outbound calls to users, refer to the
Outbound Calls via SIP guide
.
On this page
Overview of SIP integration
How it works
Getting started
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/voice-agent/client-apps:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Voice agent components
Example
Testing agent UI
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Web and mobile frontends
.
v1.0 for Node.js is coming soon.
When building voice agents with the Agents framework, your frontend can leverage LiveKit‘s SDKs to manage media devices and transport audio and video streams.
The following UI components are specifically for voice agents, including speech visualizers and control bars.
Voice agent components
The
useVoiceAssistant
hook returns the voice assistant’s
state
and audio track. You pass them to a visualizer component to give users visual feedback about the agent's current status.
There are two additional components available:
BarVisualizer
: Visualizes audio output with vertical bars. You can optionally set the number of bars and the minimum and maximum height. The visualizer can be customized via CSS styles to fit your application's design.
Tip
When
state
is passed into the
BarVisualizer
component, it visualizes both the agent’s state (like
listening
or
thinking
) and the audio spectrum.
VoiceAssistantControlBar
: A control bar
that includes audio settings and a disconnect button.
Example
Here's a basic example showing how these components work together:
React
Android
Swift
// in next.js, "use client" is needed to indicate it shouldn't be
// server side rendered
"use client"
;
import
"@livekit/components-styles"
;
import
{
RoomContext
,
useVoiceAssistant
,
BarVisualizer
,
RoomAudioRenderer
,
VoiceAssistantControlBar
,
}
from
"@livekit/components-react"
;
export
default
function
MyVoiceAgent
(
)
{
const
[
room
]
=
useState
(
new
Room
(
)
)
;
return
(
<
RoomContext
.
Provider value
=
{
room
}
>
<
button onClick
=
{
(
)
=>
room
.
connect
(
serverUrl
,
token
)
}
>
Connect
<
/
button
>
<
SimpleVoiceAssistant
/
>
<
VoiceAssistantControlBar
/
>
<
RoomAudioRenderer
/
>
<
/
RoomContext
.
Provider
>
)
;
}
function
SimpleVoiceAssistant
(
)
{
const
{
state
,
audioTrack
}
=
useVoiceAssistant
(
)
;
return
(
<
div className
=
"h-80"
>
<
BarVisualizer state
=
{
state
}
barCount
=
{
5
}
trackRef
=
{
audioTrack
}
style
=
{
{
}
}
/
>
<
p className
=
"text-center"
>
{
state
}
<
/
p
>
<
/
div
>
)
;
}
Copy
Testing agent UI
You can try out both of these components in the
realtime playground
. These components are also included in the frontend when you create an app using
LiveKit Sandbox
.
On this page
Voice agent components
Example
Testing agent UI
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/voice-agent/function-calling:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Usage
Forwarding to the frontend
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Tool definition and use
.
v1.0 for Node.js is coming soon.
Function calling (also known as "tool calling" or "tool use") is a powerful LLM capability that allows AI models to interact with external functions and tools. This allows your agent to retrieve additional context before generating a response or take real-world actions.
For example, when a user says "What's the weather like in New York?", the LLM can intelligently detect the need to call your weather function before it responds. As another example, you could use function calling to trigger an action in the frontend, such as presenting a map to the user or querying their current location.
Both
PipelineVoiceAgent
and
MultimodalAgent
have built-in support for function calling, making it easy to create rich voice-powered applications.
Usage
Python
Node.js
import
aiohttp
from
typing
import
Annotated
from
livekit
.
agents
import
llm
from
livekit
.
agents
.
pipeline
import
VoicePipelineAgent
from
livekit
.
agents
.
multimodal
import
MultimodalAgent
# first define a class that inherits from llm.FunctionContext
class
AssistantFnc
(
llm
.
FunctionContext
)
:
# the llm.ai_callable decorator marks this function as a tool available to the LLM
# by default, it'll use the docstring as the function's description
@llm
.
ai_callable
(
)
async
def
get_weather
(
self
,
# by using the Annotated type, arg description and type are available to the LLM
location
:
Annotated
[
str
,
llm
.
TypeInfo
(
description
=
"The location to get the weather for"
)
]
,
)
:
"""Called when the user asks about the weather. This function will return the weather for the given location."""
logger
.
info
(
f"getting weather for
{
location
}
"
)
url
=
f"https://wttr.in/
{
location
}
?format=%C+%t"
async
with
aiohttp
.
ClientSession
(
)
as
session
:
async
with
session
.
get
(
url
)
as
response
:
if
response
.
status
==
200
:
weather_data
=
await
response
.
text
(
)
# response from the function call is returned to the LLM
# as a tool response. The LLM's response will include this data
return
f"The weather in
{
location
}
is
{
weather_data
}
."
else
:
raise
f"Failed to get weather data, status code:
{
response
.
status
}
"
fnc_ctx
=
AssistantFnc
(
)
# pass the function context to the agent
pipeline_agent
=
VoicePipelineAgent
(
.
.
.
fnc_ctx
=
fnc_ctx
,
)
multimodal_agent
=
MultimodalAgent
(
.
.
.
fnc_ctx
=
fnc_ctx
,
)
Copy
Forwarding to the frontend
Function calls can be forwarded to a frontend application using
RPC
. This is useful when the data needed to fulfill the function call is only available at the frontend. It can also be used to trigger actions or UI updates in a structured way.
For instance, here's a function that accesses the user's live location from their web browser:
Agent implementation
Python
Node.js
from
livekit
.
agents
import
llm
from
typing
import
Annotated
class
AssistantFnc
(
llm
.
FunctionContext
)
:
@llm
.
ai_callable
(
)
async
def
get_user_location
(
self
,
high_accuracy
:
Annotated
[
bool
,
llm
.
TypeInfo
(
description
=
"Whether to use high accuracy mode, which is slower"
)
]
=
False
)
:
"""Retrieve the user's current geolocation as lat/lng."""
try
:
return
await
ctx
.
room
.
local_participant
.
perform_rpc
(
destination_identity
=
participant
.
identity
,
method
=
"getUserLocation"
,
payload
=
json
.
dumps
(
{
"highAccuracy"
:
high_accuracy
}
)
,
response_timeout
=
10.0
if
high_accuracy
else
5.0
,
)
except
Exception
:
return
"Unable to retrieve user location"
Copy
Frontend implementation
JavaScript
import
{
RpcError
,
RpcInvocationData
}
from
'livekit-client'
;
localParticipant
.
registerRpcMethod
(
'getUserLocation'
,
async
(
data
:
RpcInvocationData
)
=>
{
try
{
let
params
=
JSON
.
parse
(
data
.
payload
)
;
const
position
:
GeolocationPosition
=
await
new
Promise
(
(
resolve
,
reject
)
=>
{
navigator
.
geolocation
.
getCurrentPosition
(
resolve
,
reject
,
{
enableHighAccuracy
:
params
.
highAccuracy
??
false
,
timeout
:
data
.
responseTimeout
,
}
)
;
}
)
;
return
JSON
.
stringify
(
{
latitude
:
position
.
coords
.
latitude
,
longitude
:
position
.
coords
.
longitude
,
}
)
;
}
catch
(
error
)
{
throw
new
RpcError
(
1
,
"Could not retrieve user location"
)
;
}
}
)
;
Copy
On this page
Usage
Forwarding to the frontend
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/voice-agent/transcriptions:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Frontend integration
Agent integration
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Text and transcriptions
.
v1.0 for Node.js is coming soon.
Overview
The Agents framework includes the ability to capture and deliver realtime transcriptions of a user's speech and LLM-generated speech or text.
Both
VoicePipelineAgent
and
MultimodalAgent
can forward transcriptions to clients automatically if you implement support for receiving them in your frontend. If you're not using either of these agent classes, you can
add transcription forwarding
to your agent code.
To learn more about creating transcriptions in the agent process, see
Recording agent sessions
.
Frontend integration
You can use a
LiveKit SDK
to receive transcription events in your frontend.
Transcriptions are delivered in segments, each associated with a particular
Participant
and
Track
. Each segment has a unique
id
. Segments might be sent as fragments as they're generated. You can monitor the
final
property to determine when a segment is complete.
JavaScript
Swift
Android
Flutter
This example uses React with TypeScript, but the principles are the same for other frameworks.
Collect
TranscriptionSegment
by listening to
RoomEvent.TranscriptionReceived
:
import
{
useEffect
,
useState
}
from
"react"
;
import
{
TranscriptionSegment
,
Participant
,
TrackPublication
,
RoomEvent
,
}
from
"livekit-client"
;
import
{
useMaybeRoomContext
}
from
"@livekit/components-react"
;
export
default
function
Transcriptions
(
)
{
const
room
=
useMaybeRoomContext
(
)
;
const
[
transcriptions
,
setTranscriptions
]
=
useState
<
{
[
id
:
string
]
:
TranscriptionSegment
}
>
(
{
}
)
;
useEffect
(
(
)
=>
{
if
(
!
room
)
{
return
;
}
const
updateTranscriptions
=
(
segments
:
TranscriptionSegment
[
]
,
participant
?
:
Participant
,
publication
?
:
TrackPublication
)
=>
{
setTranscriptions
(
(
prev
)
=>
{
const
newTranscriptions
=
{
...
prev
}
;
for
(
const
segment
of
segments
)
{
newTranscriptions
[
segment
.
id
]
=
segment
;
}
return
newTranscriptions
;
}
)
;
}
;
room
.
on
(
RoomEvent
.
TranscriptionReceived
,
updateTranscriptions
)
;
return
(
)
=>
{
room
.
off
(
RoomEvent
.
TranscriptionReceived
,
updateTranscriptions
)
;
}
;
}
,
[
room
]
)
;
return
(
<
ul
>
{
Object
.
values
(
transcriptions
)
.
sort
(
(
a
,
b
)
=>
a
.
firstReceivedTime
-
b
.
firstReceivedTime
)
.
map
(
(
segment
)
=>
(
<
li key
=
{
segment
.
id
}
>
{
segment
.
text
}
<
/
li
>
)
)
}
<
/
ul
>
)
}
Copy
Agent integration
The
STTSegmentsForwarder
class provides an interface for delivering transcriptions from your custom agent to your
frontend
in realtime. Here's a sample implementation:
from
livekit
.
agents
import
stt
,
transcription
from
livekit
.
plugins
.
deepgram
import
STT
async
def
_forward_transcription
(
stt_stream
:
stt
.
SpeechStream
,
stt_forwarder
:
transcription
.
STTSegmentsForwarder
,
)
:
"""Forward the transcription and log the transcript in the console"""
async
for
ev
in
stt_stream
:
stt_forwarder
.
update
(
ev
)
if
ev
.
type
==
stt
.
SpeechEventType
.
INTERIM_TRANSCRIPT
:
print
(
ev
.
alternatives
[
0
]
.
text
,
end
=
""
)
elif
ev
.
type
==
stt
.
SpeechEventType
.
FINAL_TRANSCRIPT
:
print
(
"\n"
)
print
(
" -> "
,
ev
.
alternatives
[
0
]
.
text
)
async
def
entrypoint
(
job
:
JobContext
)
:
stt
=
STT
(
)
tasks
=
[
]
async
def
transcribe_track
(
participant
:
rtc
.
RemoteParticipant
,
track
:
rtc
.
Track
)
:
audio_stream
=
rtc
.
AudioStream
(
track
)
stt_forwarder
=
transcription
.
STTSegmentsForwarder
(
room
=
job
.
room
,
participant
=
participant
,
track
=
track
)
stt_stream
=
stt
.
stream
(
)
stt_task
=
asyncio
.
create_task
(
_forward_transcription
(
stt_stream
,
stt_forwarder
)
)
tasks
.
append
(
stt_task
)
async
for
ev
in
audio_stream
:
stt_stream
.
push_frame
(
ev
.
frame
)
@job
.
room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
publication
:
rtc
.
TrackPublication
,
participant
:
rtc
.
RemoteParticipant
,
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_AUDIO
:
tasks
.
append
(
asyncio
.
create_task
(
transcribe_track
(
participant
,
track
)
)
)
Copy
On this page
Overview
Frontend integration
Agent integration
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/quickstarts/s2s:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Prerequisites
Steps
Setup a LiveKit account and install the CLI
Bootstrap an agent from template
Bootstrap a frontend from template
Launch your app and talk to your agent
Next steps
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Voice AI quickstart
.
v1.0 for Node.js is coming soon.
The
MultimodalAgent
class in the LiveKit Agents Framework uses the OpenAI Realtime API for speech-to-speech interactions between AI voice assistants and end users. It is implemented in both our
Python
and
Node
Agents Framework libraries.
Note
If you're not using the OpenAI Realtime API, see the
Voice agent with STT, LLM, TTS quickstart
.
Prerequisites
OpenAI API Key
Python 3.9-3.12
or
Node 20.17.0
Steps
The following steps take you through the process of creating a LiveKit account and using the LiveKit CLI to create an agent from some minimal templates. At the end of the quickstart, you'll have an agent and a frontend you can use to talk to your agent.
Setup a LiveKit account and install the CLI
Create an account or sign in to your
LiveKit Cloud account
.
(Optional)
Install the LiveKit CLI
and authenticate using
lk cloud auth
.
Note
LiveKit's CLI utility
lk
is a convenient way to setup and configure new applications, but if you'd rather do it manually, you can clone the
multimodal-agent-python
(or
node
) and
voice-assistant-frontend
repositories and follow the manual setup instructions in each.
Bootstrap an agent from template
Clone a starter template for your preferred language using the CLI:
Python
Node.js
lk app create
--template
multimodal-agent-python
Copy
Enter your
OpenAI API Key
when prompted.
Install dependencies and start your agent:
Python
Node.js
cd
<
agent_dir
>
python3
-m
venv venv
source
venv/bin/activate
python3
-m
pip
install
-r
requirements.txt
python3 agent.py dev
Copy
You can edit the
agent.py
file to customize the system prompt and other aspects of your agent.
Bootstrap a frontend from template
Clone the
Voice Assistant Frontend
Next.js app starter template using the CLI:
lk app create
--template
voice-assistant-frontend
Copy
Install dependencies and start your frontend application:
cd
<
frontend_dir
>
pnpm
install
pnpm
dev
Copy
Launch your app and talk to your agent
Visit your locally-running application (by default,
http://localhost:3000
).
Select
Connect
and start a conversation with your agent.
Next steps
Learn more in the
OpenAI Realtime API integration guide
.
Let your friends and colleagues talk to your agent by connecting it to a LiveKit
Sandbox
.
Create an
agent that accepts incoming calls
using SIP.
Create an
agent that makes outbound calls
using SIP.
On this page
Prerequisites
Steps
Setup a LiveKit account and install the CLI
Bootstrap an agent from template
Bootstrap a frontend from template
Launch your app and talk to your agent
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/quickstarts/voice-agent:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Prerequisites
Steps
Setup a LiveKit account and install the CLI
Bootstrap an agent from template
Bootstrap a frontend from template
Launch your app and talk to your agent
Customizing plugins
Next steps
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Voice AI quickstart
.
v1.0 for Node.js is coming soon.
This quickstart tutorial walks you through the steps to build a conversational AI application using Python and NextJS. It uses LiveKit's
Agents Framework
and React Components Library to create an AI-powered voice assistant that can engage in realtime conversations with users. By the end, you will have a basic voice assistant application that you can run and interact with.
Note
If you're interested in using the OpenAI Realtime API, see the
Speech-to-speech quickstart
.
Prerequisites
LiveKit Cloud Project
or
open-source LiveKit server
Deepgram API Key
Cartesia API Key
OpenAI API Key
Python 3.9-3.12
Note
By default, the example agent uses Deepgram for STT and OpenAI for TTS and LLM. However, you aren't required
to use these providers.
Steps
The following steps take you through the process of creating a voice assistant using the LiveKit CLI and some minimal templates.
Setup a LiveKit account and install the CLI
Create an account or sign in to your
LiveKit Cloud account
.
Install the LiveKit CLI
and authenticate using
lk cloud auth
.
Bootstrap an agent from template
Clone the starter template for a simple Python voice agent:
lk app create
--template
voice-pipeline-agent-python
Copy
Enter your
OpenAI API Key
and
Deepgram API Key
when prompted. If you aren't using Deepgram and OpenAI, see
Customizing plugins
.
Install dependencies and start your agent:
cd
<
agent_dir
>
python3
-m
venv venv
source
venv/bin/activate
python3
-m
pip
install
-r
requirements.txt
python3 agent.py dev
Copy
You can edit the
agent.py
file to customize the system prompt and other aspects of your agent.
Bootstrap a frontend from template
Clone the
Voice Assistant Frontend
Next.js app starter template using the CLI:
lk app create
--template
voice-assistant-frontend
Copy
Install dependencies and start your frontend application:
cd
<
frontend_dir
>
pnpm
install
pnpm
dev
Copy
Launch your app and talk to your agent
Visit your locally-running application (by default,
http://localhost:3000
).
Select
Connect
and start a conversation with your agent.
Customizing plugins
You can change the VAD, STT, TTS, and LLM plugins your agent uses by editing the
agents.py
file. By default,
the sandbox voice assistant is configured to use Silero for VAD, Deepgram for STT, and OpenAI for TTS and LLM
using the
gpt-4o-mini
model:
assistant
=
VoiceAssistant
(
vad
=
silero
.
VAD
.
load
(
)
,
stt
=
deepgram
.
STT
(
)
,
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
,
tts
=
openai
.
TTS
(
)
,
chat_ctx
=
initial_ctx
,
)
Copy
You can modify your agent to use different providers. For example, to use Cartesia for TTS, use the following
steps:
Edit file
agent.py
and update the imported plugins list to include
cartesia
:
from
livekit
.
plugins
import
cartesia
,
deepgram
,
openai
,
silero
Copy
Update the
tts
plugin for your assistant in file
agent.py
:
tts
=
cartesia
.
TTS
(
)
,
Copy
Update the
.env.local
file to include your Cartesia API key by adding a
CARTESIA_API_KEY
environment variable:
CARTESIA_API_KEY
=
"<cartesia_api_key>"
Copy
Install the plugin locally:
pip
install
"livekit-plugins-cartesia~=0.4"
Copy
Start your agent:
python3 agent.py dev
Copy
Next steps
For a list of additional plugins you can use, see
Integration guides for LiveKit Agents
.
Let your friends and colleagues talk to your agent by connecting it to a LiveKit
Sandbox
.
Create an
agent that accepts incoming calls
using SIP.
Create an
agent that makes outbound calls
using SIP.
On this page
Prerequisites
Steps
Setup a LiveKit account and install the CLI
Bootstrap an agent from template
Bootstrap a frontend from template
Launch your app and talk to your agent
Customizing plugins
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/quickstarts/inbound-calls:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Prerequisites
Step 1: Set up environment variables
Step 2: Create an AI voice agent
Step 3: Create an inbound LiveKit trunk
Add dispatch rule
Step 4: Call your agent
Next steps
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Integrating with telephony
.
v1.0 for Node.js is coming soon.
Note
To make calls, see the
Making outgoing calls
quickstart.
This guide walks you through the steps to create an AI voice agent that responds to incoming calls.
Users can call a phone number and interact directly with your AI-powered voice assistant. For example,
your agent can be a resource in the following scenarios:
Call centers
: Automate customer interactions and reduce wait times.
Customer service
: Provide immediate assistance through an AI agent.
Sales inquiries
: Allow customers to ask questions about products or services.
Prerequisites
The following are required to complete the steps in this quickstart:
A phone number purchased from your SIP trunk provider.
A SIP trunk with your provider.
LiveKit CLI
installed.
SIP server:
LiveKit Cloud
or a
self-hosted SIP server
.
Instructions for setting up a SIP trunk are available in the
Create and configure SIP trunk
quickstart.
Step 1: Set up environment variables
Tip
Log in
to see your real credentials populated in many places throughout this page
Set up the following environment variables to configure the LiveKit CLI to use your LiveKit Cloud or
self-hosted LiveKit server
instance:
export
LIVEKIT_URL
=
<
your LiveKit server URL
>
export
LIVEKIT_API_KEY
=
<
your API Key
>
export
LIVEKIT_API_SECRET
=
<
your API Secret
>
Reveal API Key and Secret
Copy
Step 2: Create an AI voice agent
The fastest way to create an agent is by using the LiveKit CLI.
Enter your API keys at the prompts. Alternatively, you can skip the prompts and manually edit the
.env.local
file with your
API keys.
Python
Node.js
lk app create
--template
voice-pipeline-agent-python
// or clone it from GitHub
git
clone https://github.com/livekit-examples/voice-pipeline-agent-python.git
Copy
Modify the agent to give it an
agent_name
. This will allow you to
dispatch the agent
explicitly when configuring SIP.
Python
Node.js
if
__name__
==
"__main__"
:
cli
.
run_app
(
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
prewarm_fnc
=
prewarm
,
# giving this agent a name of: "inbound-agent"
agent_name
=
"inbound-agent"
,
)
,
)
Copy
Then you can start the agent:
Python
Node.js
python3 agent.py dev
Copy
Step 3: Create an inbound LiveKit trunk
An inbound SIP trunk instructs LiveKit to accept calls to one or more numbers that you own. Replace the phone number
with the number purchased from your SIP trunking provider and create a file
inbound-trunk.json
with the
following content:
Twilio
Telnyx
{
"trunk"
:
{
"name"
:
"My inbound trunk"
,
"numbers"
:
[
"+15105550100"
]
}
}
Copy
Important
Twilio numbers must start with a leading
+
.
Create the LiveKit inbound SIP trunk using the CLI:
lk sip inbound create inbound-trunk.json
Copy
Add dispatch rule
Dispatch rules
route inbound SIP calls to LiveKit rooms. In this example,
the
dispatchRuleIndividual
routes each caller to their own room. Each room will
be named with a prefix of
call
.
Additionally, specify an agent to handle incoming calls. In this example,
inbound-agent
is dispatched to all callers.
Create
dispatch-rule.json
file:
{
"name"
:
"My dispatch rule"
,
"rule"
:
{
"dispatchRuleIndividual"
:
{
"roomPrefix"
:
"call"
}
}
,
"room_config"
:
{
"agents"
:
[
{
"agent_name"
:
"inbound-agent"
}
]
}
}
Copy
Apply the dispatch rule for the trunk using the CLI:
lk sip dispatch create dispatch-rule.json
Copy
Step 4: Call your agent
Calling the phone number you assigned to the trunk places you in a room with your agent.
Next steps
Using SIP participant attributes
: Modify the agent you created in Step 2
based on a caller's attributes.
To learn more, see the following topics:
Inbound trunks
: You can configure your inbound trunk to limit access to your LiveKit SIP application,
set participant metadata and attributes, and modify call properties.
Dispatch rules
: You can create dispatch rules to control how callers are dispatched to rooms,
configure authentication, and add participant attributes.
SIP participant
: You can manage SIP participants like any other LiveKit participant. You can also
use SIP specific attributes to manage callers based on the number they call or the number they're calling from, and more.
On this page
Prerequisites
Step 1: Set up environment variables
Step 2: Create an AI voice agent
Step 3: Create an inbound LiveKit trunk
Add dispatch rule
Step 4: Call your agent
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/quickstarts/outbound-calls:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Prerequisites
Outbound call flow
Step 1: Set up environment variables
Step 2: Create an outbound trunk
Step 3: Create an agent
Understanding the code
Step 4: Creating a dispatch
Next steps
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Integrating with telephony
.
v1.0 for Node.js is coming soon.
This guide walks you through the steps to create an AI voice agent that makes outgoing
calls. You can use an agent in the following scenarios:
Call center callbacks
: Automatically return customer calls.
Product check-ins
: Follow up on purchased items.
Sales calls
: Reach out to potential customers.
Appointment reminders
: Notify clients of upcoming appointments.
Surveys
: Collect feedback through automated calls.
Prerequisites
The following are required to complete the steps in this guide:
Phone number purchased from a SIP trunk provider like
Twilio
or
Telnyx
.
SIP provider trunk configured
as an outbound trunk for use with LiveKit SIP.
LiveKit Cloud
project or a
self-hosted instance
of LiveKit server
SIP server
(only required if you're self hosting the LiveKit server).
LiveKit CLI
installed (requires version 2.3.0 or later).
Outbound call flow
The suggested flow for outbound calls is as follows:
Create a dispatch for your agent.
Once your agent is connected, dial the user via CreateSIPParticipant.
The user answers the call, and starts speaking to the agent.
Step 1: Set up environment variables
Tip
Log in
to see your real credentials populated in many places throughout this page
Set up the following environment variables to configure the LiveKit CLI to use your LiveKit Cloud or
self-hosted LiveKit server
instance:
export
LIVEKIT_URL
=
<
your LiveKit server URL
>
export
LIVEKIT_API_KEY
=
<
your API Key
>
export
LIVEKIT_API_SECRET
=
<
your API Secret
>
export
OPENAI_API_KEY
=
your-openai-api-key
Reveal API Key and Secret
Copy
Step 2: Create an outbound trunk
To make outgoing calls, the provider's outbound trunk needs to be registered with LiveKit.
SIP trunking providers typically require authentication when accepting outbound SIP requests to ensure only authorized users are making calls with your number.
Note
This setup only needs to be performed once.
If you already have an
outbound trunk
for the SIP provider phone number,
you can skip to
Step 3: Create an agent
.
Create a file named
outbound-trunk.json
using your phone number, trunk domain name,
and
username
and
password
. The following example assumes your provider phone number is
+15105550100
:
Twilio
Telnyx
{
"trunk"
:
{
"name"
:
"My outbound trunk"
,
"address"
:
"<my-trunk>.pstn.twilio.com"
,
"numbers"
:
[
"+15105550100"
]
,
"auth_username"
:
"<username>"
,
"auth_password"
:
"<password>"
}
}
Copy
Create the outbound trunk using the CLI:
lk sip outbound create outbound-trunk.json
Copy
The output of the command returns the trunk ID. Copy the
<trunk-id>
for step 4:
SIPTrunkID: <trunk-id>
Copy
Step 3: Create an agent
Create an agent that makes outbound calls. In this example, create a speech-to-speech agent
using OpenAI's realtime API. The same example can also be used with
VoicePipelineAgent
.
Create a template
outbound caller agent
:
lk app create
--template
=
outbound-caller-python
Copy
Follow the instructions in the command output. Enter the outbound trunk ID from Step 2.
Understanding the code
There are a few key points of note in this example detailed in the following sections.
Set
agent_name
for explicit dispatch
if
__name__
==
"__main__"
:
cli
.
run_app
(
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
# giving this agent a name will allow us to dispatch it via API
# automatic dispatch is disabled when `agent_name` is set
agent_name
=
"outbound-caller"
,
)
)
Copy
By setting the
agent_name
field, you can use AgentDispatchService to create a
dispatch for this agent. To learn more, see
agent dispatch
.
Dialing the user
async
def
entrypoint
(
ctx
:
JobContext
)
:
global
_default_instructions
logger
.
info
(
f"connecting to room
{
ctx
.
room
.
name
}
"
)
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
user_identity
=
"phone_user"
# the phone number to dial is provided in the job metadata
phone_number
=
ctx
.
job
.
metadata
logger
.
info
(
f"dialing
{
phone_number
}
to room
{
ctx
.
room
.
name
}
"
)
# look up the user's phone number and appointment details
instructions
=
_default_instructions
+
"The customer's name is Jayden. His appointment is next Tuesday at 3pm."
# `create_sip_participant` starts dialing the user
await
ctx
.
api
.
sip
.
create_sip_participant
(
api
.
CreateSIPParticipantRequest
(
room_name
=
ctx
.
room
.
name
,
sip_trunk_id
=
outbound_trunk_id
,
sip_call_to
=
phone_number
,
participant_identity
=
user_identity
,
)
)
# a participant is created as soon as we start dialing
participant
=
await
ctx
.
wait_for_participant
(
identity
=
user_identity
)
# start either VoicePipelineAgent or MultimodalAgent
#run_voice_pipeline_agent(ctx, participant, instructions)
run_multimodal_agent
(
ctx
,
participant
,
instructions
)
Copy
Once dispatched, the agent first connects to the room and then dials the user.
The agent receives instructions on which user to call from the job metadata set during dispatch.
In this example, the user’s phone number is included in the job metadata (details below).
After determining which user to call, you can load additional information about
the user from your own database.
Finally, calling
create_sip_participant
initiates dialing the user.
Monitoring dialing status
start_time
=
perf_counter
(
)
while
perf_counter
(
)
-
start_time
<
30
:
call_status
=
participant
.
attributes
.
get
(
"sip.callStatus"
)
if
call_status
==
"active"
:
logger
.
info
(
"user has picked up"
)
return
elif
call_status
==
"automation"
:
# if DTMF is used in the `sip_call_to` number, typically used to dial
# an extension or enter a PIN.
# during DTMF dialing, the participant will be in the "automation" state
pass
elif
call_status
==
"hangup"
:
# user hung up, we'll exit the job
logger
.
info
(
"user hung up, exiting job"
)
break
await
asyncio
.
sleep
(
0.1
)
logger
.
info
(
"session timed out, exiting job"
)
ctx
.
shutdown
(
)
Copy
Once
create_sip_participant
is called, LiveKit begins dialing the user.
The method returns immediately after dialing is initiated and does not wait for the user to answer.
Optionally, you can check the
sip.callStatus
attribute to monitor dialing status.
When the user answers, this attribute updates to
active
.
Handle actions with function calling
Use
@llm.ai_callable()
to prepare functions for the LLM to use as tools.
In this example, the following actions are handled:
Detecting voicemail
Looking up availability
Confirming the appointment
Detecting intent to end the call
class
CallActions
(
llm
.
FunctionContext
)
:
def
__init__
(
self
,
*
,
api
:
api
.
LiveKitAPI
,
participant
:
rtc
.
RemoteParticipant
,
room
:
rtc
.
Room
)
:
super
(
)
.
__init__
(
)
self
.
api
=
api
self
.
participant
=
participant
self
.
room
=
room
async
def
hangup
(
self
)
:
try
:
await
self
.
api
.
room
.
remove_participant
(
api
.
RoomParticipantIdentity
(
room
=
self
.
room
.
name
,
identity
=
self
.
participant
.
identity
,
)
)
except
Exception
as
e
:
# it's possible that the user has already hung up, this error can be ignored
logger
.
info
(
f"received error while ending call:
{
e
}
"
)
@llm
.
ai_callable
(
)
async
def
end_call
(
self
)
:
"""Called when the user wants to end the call"""
logger
.
info
(
f"ending the call for
{
self
.
participant
.
identity
}
"
)
await
self
.
hangup
(
)
@llm
.
ai_callable
(
)
async
def
look_up_availability
(
self
,
date
:
Annotated
[
str
,
"The date of the appointment to check availability for"
]
,
)
:
"""Called when the user asks about alternative appointment availability"""
logger
.
info
(
f"looking up availability for
{
self
.
participant
.
identity
}
on
{
date
}
"
)
asyncio
.
sleep
(
3
)
return
json
.
dumps
(
{
"available_times"
:
[
"1pm"
,
"2pm"
,
"3pm"
]
,
}
)
@llm
.
ai_callable
(
)
async
def
confirm_appointment
(
self
,
date
:
Annotated
[
str
,
"date of the appointment"
]
,
time
:
Annotated
[
str
,
"time of the appointment"
]
,
)
:
"""Called when the user confirms their appointment on a specific date. Use this tool only when they are certain about the date and time."""
logger
.
info
(
f"confirming appointment for
{
self
.
participant
.
identity
}
on
{
date
}
at
{
time
}
"
)
return
"reservation confirmed"
@llm
.
ai_callable
(
)
async
def
detected_answering_machine
(
self
)
:
"""Called when the call reaches voicemail. Use this tool AFTER you hear the voicemail greeting"""
logger
.
info
(
f"detected answering machine for
{
self
.
participant
.
identity
}
"
)
await
self
.
hangup
(
)
.
.
.
agent
=
MultimodalAgent
(
model
=
model
,
fnc_ctx
=
CallActions
(
api
=
ctx
.
api
,
participant
=
participant
,
room
=
ctx
.
room
)
,
)
Copy
Step 4: Creating a dispatch
Once your agent is up and running, you can test it by having it make a phone call.
Use LiveKit CLI to create a dispatch. Replace
+15105550100
with the phone number you want to call.
Note
You need version 2.3.0 or later of
LiveKit CLI
to use dispatch commands.
lk dispatch create
\
--new-room
\
--agent-name outbound-caller
\
--metadata
'+15105550100'
Copy
Your phone should ring. After you pick up, you're connected to the agent.
Next steps
Create an agent that
accepts inbound calls
.
Learn more about
outbound trunks
.
This guide uses Python. For more information and Node examples, see the following topics:
Dispatching agents
VoicePipelineAgent
Creating a SIP participant
On this page
Prerequisites
Outbound call flow
Step 1: Set up environment variables
Step 2: Create an outbound trunk
Step 3: Create an agent
Understanding the code
Step 4: Creating a dispatch
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/quickstarts/vision:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Prerequisites
Step 1: Setup a LiveKit account and install the CLI
Step 2: Bootstrap an agent from template
Step 3: Add video-related imports
Step 4: Enable video subscription
Step 5: Add video frame handling
Step 6: Add the LLM Callback
Step 7: Update the system prompt
Step 8: Update the assistant configuration
Testing your agent
How it works
Next steps
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Audio and video
.
v1.0 for Node.js is coming soon.
This quickstart tutorial walks you through the steps to build an AI application using Python that has access to your camera. It uses LiveKit's
Agents Framework
to create an AI-powered voice assistant that can engage in realtime conversations with users, and analyze images captured from your camera.
Prerequisites
LiveKit Cloud Project
or
open-source LiveKit server
OpenAI API Key
Python 3.9-3.12
Note
By default, the example agent uses Deepgram for STT and OpenAI for TTS and LLM. However, you aren't required
to use these providers.
Step 1: Setup a LiveKit account and install the CLI
You can skip this step if you choose to clone the template from the GitHub repository.
Create an account or sign in to your
LiveKit Cloud account
.
Install the LiveKit CLI
and authenticate using
lk cloud auth
.
Step 2: Bootstrap an agent from template
The template provides a working voice assistant to build on. The template includes:
Basic voice interaction
Audio-only track subscription
Voice activity detection (VAD)
Speech-to-text (STT)
Language model (LLM)
Text-to-speech (TTS)
Clone the starter template for a simple Python voice agent using the CLI:
lk app create
--template
voice-pipeline-agent-python
Copy
Alternatively, clone the GitHub
repository
:
git
clone https://github.com/livekit-examples/voice-pipeline-agent-python.git
Copy
Enter your
OpenAI API Key
and
Deepgram API Key
when prompted. If you aren't using Deepgram and OpenAI, see
Customizing plugins
.
Note
If you want to use OpenAI for STT as well as TTS and LLM, you can change the
stt
plugin to
openai.STT()
.
Follow the instructions in the output of the
lk app create
command or the
setup instructions
in the
README to install dependencies and start your agent.
Step 3: Add video-related imports
Add the video-related content to our agent. At the top of your
agent.py
file, add these imports
alongside the existing ones:
from
livekit
import
rtc
from
livekit
.
agents
.
llm
import
ChatMessage
,
ChatImage
Copy
These new imports include:
rtc
: Access to LiveKit's video functionality
ChatMessage
and
ChatImage
: Classes we'll use to send images to the LLM
Step 4: Enable video subscription
Find the
ctx.connect()
line in the
entrypoint
function. Change
AutoSubscribe.AUDIO_ONLY
to
AutoSubscribe.SUBSCRIBE_ALL
:
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
SUBSCRIBE_ALL
)
Copy
This enables the assistant to receive video tracks as well as audio.
Step 5: Add video frame handling
Add these two helper functions after your imports but before the
prewarm
function:
async
def
get_video_track
(
room
:
rtc
.
Room
)
:
"""Find and return the first available remote video track in the room."""
for
participant_id
,
participant
in
room
.
remote_participants
.
items
(
)
:
for
track_id
,
track_publication
in
participant
.
track_publications
.
items
(
)
:
if
track_publication
.
track
and
isinstance
(
track_publication
.
track
,
rtc
.
RemoteVideoTrack
)
:
logger
.
info
(
f"Found video track
{
track_publication
.
track
.
sid
}
"
f"from participant
{
participant_id
}
"
)
return
track_publication
.
track
raise
ValueError
(
"No remote video track found in the room"
)
Copy
This function searches through all participants to find an available video track. It's used to locate the video feed to process.
Next, add the frame capture function:
async
def
get_latest_image
(
room
:
rtc
.
Room
)
:
"""Capture and return a single frame from the video track."""
video_stream
=
None
try
:
video_track
=
await
get_video_track
(
room
)
video_stream
=
rtc
.
VideoStream
(
video_track
)
async
for
event
in
video_stream
:
logger
.
debug
(
"Captured latest video frame"
)
return
event
.
frame
except
Exception
as
e
:
logger
.
error
(
f"Failed to get latest image:
{
e
}
"
)
return
None
finally
:
if
video_stream
:
await
video_stream
.
aclose
(
)
Copy
This function captures a single frame from the video track and ensures proper cleanup of resources. Using
aclose()
releases system resources like memory buffers and video decoder instances, which helps prevent memory leaks.
Step 6: Add the LLM Callback
Inside the
entrypoint
function, add this callback function which will inject the latest video frame just before the LLM generates a response:
async
def
before_llm_cb
(
assistant
:
VoicePipelineAgent
,
chat_ctx
:
llm
.
ChatContext
)
:
"""
Callback that runs right before the LLM generates a response.
Captures the current video frame and adds it to the conversation context.
"""
latest_image
=
await
get_latest_image
(
ctx
.
room
)
if
latest_image
:
image_content
=
[
ChatImage
(
image
=
latest_image
)
]
chat_ctx
.
messages
.
append
(
ChatMessage
(
role
=
"user"
,
content
=
image_content
)
)
logger
.
debug
(
"Added latest frame to conversation context"
)
Copy
This callback is the key to efficient context management — it only adds visual information when the assistant is about to respond. If visual information was added to every message, it would quickly fill up the context window.
Step 7: Update the system prompt
Find the
initial_ctx
creation in the
entrypoint
function and update it to include vision capabilities:
initial_ctx
=
llm
.
ChatContext
(
)
.
append
(
role
=
"system"
,
text
=
(
"You are a voice assistant created by LiveKit that can both see and hear. "
"You should use short and concise responses, avoiding unpronounceable punctuation. "
"When you see an image in our conversation, naturally incorporate what you see "
"into your response. Keep visual descriptions brief but informative."
)
,
)
Copy
Step 8: Update the assistant configuration
Find the
VoicePipelineAgent
creation in the
entrypoint
function and add the callback:
assistant
=
VoicePipelineAgent
(
vad
=
ctx
.
proc
.
userdata
[
"vad"
]
,
stt
=
deepgram
.
STT
(
)
,
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
,
tts
=
openai
.
TTS
(
)
,
chat_ctx
=
initial_ctx
,
before_llm_cb
=
before_llm_cb
)
Copy
The key change here is the
before_llm_cb
parameter, which uses the callback created earlier to inject the latest video frame into the conversation context.
Testing your agent
Start your assistant (if your agent is already running, skip this step):
python agent.py dev
Copy
Connect to the LiveKit room with a client that publishes both audio and video. The easiest way to do this is by using the
Agents Playground
.
Connect to the room, and try asking your agent some questions like:
"What do you see right now?"
"Can you describe what's happening?"
"Has anything changed in the scene?"
How it works
With these changes, your assistant now:
Connects to both audio and video streams.
Listens for user speech as before.
Just before generating each response:
Captures the current video frame.
Adds it to the conversation context.
Uses it to inform the response.
Keeps the context clean by only adding frames when needed.
Next steps
For a list of additional plugins you can use, see
Integration guides for LiveKit Agents
.
Let your friends and colleagues talk to your agent by connecting it to a LiveKit
Sandbox
.
On this page
Prerequisites
Step 1: Setup a LiveKit account and install the CLI
Step 2: Bootstrap an agent from template
Step 3: Add video-related imports
Step 4: Enable video subscription
Step 5: Add video frame handling
Step 6: Add the LLM Callback
Step 7: Update the system prompt
Step 8: Update the assistant configuration
Testing your agent
How it works
Next steps
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/build/anatomy:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Agent lifecycle
Worker options
Entrypoint
Request handler
Prewarm function
Permissions
Worker type
Starting the worker
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Worker lifecycle
.
v1.0 for Node.js is coming soon.
This guide explains the core ideas and components that make up Agents. For a step-by-step guide on building an Agent without the deep dive into its inner workings, check out the
quickstart guide
.
Agent lifecycle
The framework turns your program into an "agent" that can join a LiveKit room and interact with other participants. Here's a high-level overview of the lifecycle:
Worker registration
: When you run
myagent.py start
, it connects to LiveKit server and registers itself as a "worker" via a persistent WebSocket connection. Once registered, the app is on standby, waiting for rooms (sessions with end-users) to be created. It exchanges availability and capacity information with LiveKit server automatically, allowing for correct load balancing of incoming requests.
Agent dispatch
: When an end-user connects to a room, LiveKit server selects an available worker and sends it information about that session. The first worker to accept that request will instantiate your program and join the room. A worker can host multiple instances of your agent simultaneously, running each in its own process for isolation.
Your program
: This is where you take over. Your program can use most features of the LiveKit
Python SDK
. Agents can also leverage the
agent plugin
ecosystem to process or synthesize voice and video data.
Room close
: The room will automatically close when the last non-agent participant has left. Remaining agents will be disconnected.
Worker options
In stateful computing, a worker is similar to a web server process in traditional web architecture. The worker acts as the program's main loop, responsible for deploying and monitoring instances of the Agent on child processes. It can handle many Agent instances with minimal overhead, effectively utilizing machine resources.
When a worker experiences high load, it stops taking on new sessions and notifies LiveKit's server.
As you deploy updates to your program, the worker will gracefully drain existing sessions before shutting down, ensuring no sessions are interrupted mid-call.
The interface for creating a worker is through the
WorkerOptions
class:
Python
Node.js
opts
=
WorkerOptions
(
# entrypoint function is called when a job is assigned to this worker
# this is the only required parameter to WorkerOptions
entrypoint_fnc
,
# inspect the request and decide if the current worker should handle it.
request_fnc
,
# a function to perform any necessary initialization in a new process.
prewarm_fnc
,
# a function that reports the current system load, whether CPU or RAM, etc.
load_fnc
,
# the maximum value of load_fnc, above which new processes will not spawn
load_threshold
,
# whether the agent can subscribe to tracks, publish data, update metadata, etc.
permissions
,
# the type of worker to create, either JT_ROOM or JT_PUBLISHER
worker_type
=
WorkerType
.
ROOM
,
)
# start the worker
cli
.
run_app
(
opts
)
Copy
Note
While it is possible to supply API keys and secrets to the worker directly through
WorkerOptions
, for security reasons it is recommended to set them as environmental variables that the worker will then read in.
The full list of worker options, information about them, and their default values can be found in
the source code
.
Entrypoint
This is the main function that is called when a new job is assigned to the worker. It is the entry point for your agent's logic. The entrypoint is called
before
the agent joins the room, and is where you can set up any necessary state or configuration.
Python
Node.js
async
def
entrypoint
(
ctx
:
JobContext
)
:
# connect to the room
await
ctx
.
connect
(
)
# handle the session
.
.
.
Copy
For details about the entrypoint function, refer to the
Inside a session
section.
Request handler
The
request_fnc
function is executed each time that the server has a job for the agent. The framework expects workers to explicitly accept or reject each job request. If you accept the request, your entrypoint function will be called. If the request is rejected, it'll be sent to the next available worker.
By default, if left blank, the behavior is to auto-accept all requests dispatched to the worker.
Python
Node.js
async
def
request_fnc
(
req
:
JobRequest
)
:
# accept the job request
await
req
.
accept
(
# the agent's name (Participant.name), defaults to ""
name
=
"agent"
,
# the agent's identity (Participant.identity), defaults to "agent-<jobid>"
identity
=
"identity"
,
# attributes to set on the agent participant upon join
attributes
=
{
"myagent"
:
"rocks"
}
,
)
# or reject it
# await req.reject()
opts
=
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
request_fnc
=
request_fnc
)
Copy
Prewarm function
For isolation and performance reasons, the framework runs each agent session in its own process. Agents often need access to model files that take time to load. To address this, the prewarm function can be used to warm up the process before assigning any jobs to it. You can control the number of processes to keep warm using the
num_idle_processes
parameter.
Python
Node.js
def
prewarm_fnc
(
proc
:
JobProcess
)
:
# load silero weights and store to process userdata
proc
.
userdata
[
"vad"
]
=
silero
.
VAD
.
load
(
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
# access the loaded silero instance
vad
:
silero
.
VAD
=
ctx
.
proc
.
userdata
[
"vad"
]
opts
=
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
prewarm_fnc
=
prewarm_fnc
)
Copy
Permissions
By default, agents are allowed to both publish and subscribe from the others in the same Room. However, you can customize these permissions by setting the
permissions
parameter in
WorkerOptions
.
Python
Node.js
opts
=
WorkerOptions
(
.
.
.
permissions
=
WorkerPermissions
(
can_publish
=
True
,
can_subscribe
=
True
,
# when set to true, the agent won't be visible to others in the room.
# when hidden, it will also not be able to publish tracks to the room as it won't be visible.
hidden
=
False
,
)
,
)
Copy
Worker type
You can choose to start a new instance of the agent for each room or for each publisher in the room. This can be set when you register your worker:
Python
Node.js
opts
=
WorkerOptions
(
.
.
.
# when omitted, the default is WorkerType.ROOM
worker_type
=
WorkerType
.
ROOM
,
)
Copy
The
WorkerType
enum has two options:
ROOM
: A new instance of the agent is created for each room.
PUBLISHER
: A new instance of the agent is created for each publisher in the room.
If the agent is performing resource-intensive operations in a room that could potentially include multiple publishers (for example, processing incoming video from a set of security cameras), it may be desirable to set
worker_type
to
JT_PUBLISHER
to ensure that each publisher has its own instance of the agent.
For
PUBLISHER
jobs, the
entrypoint
function will be called once for each publisher in the room. The
JobContext.publisher
object will contain a
RemoteParticipant
representing that publisher.
Starting the worker
Finally, to spin up a worker with the configuration defined using
WorkerOptions
, call the CLI:
Python
Node.js
if
__name__
==
"__main__"
:
cli
.
run_app
(
opts
)
Copy
The Agents worker CLI provides two subcommands:
start
and
dev
. The former outputs raw JSON data to stdout, and is recommended for production.
dev
is recommended to use for development, as it outputs human-friendly colored logs, and supports hot reloading on Python.
On this page
Agent lifecycle
Worker options
Entrypoint
Request handler
Prewarm function
Permissions
Worker type
Starting the worker
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/build/dispatch/:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Automatic agent dispatch
Explicit agent dispatch
Dispatch via API
Dispatch from inbound SIP calls
Dispatch on participant connection
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Agent dispatch
.
v1.0 for Node.js is coming soon.
As part of an
agent's lifecycle
, agent workers register with the LiveKit server and remain idle until they are assigned to a room to interact with end users. The process of assigning an agent to a room is called dispatching an agent.
LiveKit’s dispatch system is optimized for concurrency and low latency. It supports hundreds of thousands of new connections per second and dispatches an agent to a room in under 150ms.
Automatic agent dispatch
By default, agents are automatically dispatched when rooms are created. If a participant connects to LiveKit and the room does not already exist, it is created automatically, and an agent is assigned to it.
Automatic dispatch is the best option if the same agent needs to be assigned to new participants consistently.
Explicit agent dispatch
For greater control over when and how agents join rooms, explicit dispatch is available. This approach leverages the same worker systems, allowing you to run agent workers in the same way.
The key difference is that when registering the agent, the
agent_name
field in
WorkerOptions
needs to be set.
Python
Node.js
opts
=
WorkerOptions
(
.
.
.
agent_name
=
"test-agent"
,
)
Copy
Important
The agent will not be automatically dispatched to any newly created rooms when the
agent_name
is set.
Dispatch via API
Agents running with an
agent_name
set can be explicitly dispatched to a room via
AgentDispatchService
.
Python
Node.js
LiveKit CLI
Go
import
asyncio
from
livekit
import
api
room_name
=
"my-room"
agent_name
=
"test-agent"
async
def
create_explicit_dispatch
(
)
:
lkapi
=
api
.
LiveKitAPI
(
)
dispatch
=
await
lkapi
.
agent_dispatch
.
create_dispatch
(
api
.
CreateAgentDispatchRequest
(
agent_name
=
agent_name
,
room
=
room_name
,
metadata
=
"my_job_metadata"
)
)
print
(
"created dispatch"
,
dispatch
)
dispatches
=
await
lkapi
.
agent_dispatch
.
list_dispatch
(
room_name
=
room_name
)
print
(
f"there are
{
len
(
dispatches
)
}
dispatches in
{
room_name
}
"
)
await
lkapi
.
aclose
(
)
asyncio
.
run
(
create_explicit_dispatch
(
)
)
Copy
If
my-room
does not exist, it is created automatically during dispatch, and an instance of
test-agent
is assigned to it.
Handling job metadata
The metadata set during dispatch is included in the Job metadata passed to the agent.
Python
Node.js
async
def
entrypoint
(
ctx
:
JobContext
)
:
logger
.
info
(
f"job metadata:
{
ctx
.
job
.
metadata
}
"
)
.
.
.
Copy
Dispatch from inbound SIP calls
Agents can be explicitly dispatched for inbound SIP calls.
SIP dispatch rules
can define one or more agents using the
room_config.agents
field.
Explicitly specifying agents with SIP inbound calls is recommended over automatic dispatch, as it allows multiple agents within a single project.
Dispatch on participant connection
A participant’s token can be configured to dispatch one or more agents immediately upon connection.
To dispatch multiple agents, include multiple
RoomAgentDispatch
entries in
RoomConfiguration
.
Python
Node.js
Go
from
livekit
.
api
import
(
AccessToken
,
RoomAgentDispatch
,
RoomConfiguration
,
VideoGrants
,
)
room_name
=
"my-room"
agent_name
=
"test-agent"
def
create_token_with_agent_dispatch
(
)
-
>
str
:
token
=
(
AccessToken
(
)
.
with_identity
(
"my_participant"
)
.
with_grants
(
VideoGrants
(
room_join
=
True
,
room
=
room_name
)
)
.
with_room_config
(
RoomConfiguration
(
agents
=
[
RoomAgentDispatch
(
agent_name
=
"test-agent"
,
metadata
=
"my_metadata"
)
]
,
)
,
)
.
to_jwt
(
)
)
return
token
Copy
On this page
Automatic agent dispatch
Explicit agent dispatch
Dispatch via API
Dispatch from inbound SIP calls
Dispatch on participant connection
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/build/session:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Entrypoint
Customizing for participant
Ending the session
Disconnecting the agent
Disconnecting everyone
Post-processing and cleanup
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Job lifecycle
.
v1.0 for Node.js is coming soon.
Once the worker accepts the job, the framework starts a new process to run your entrypoint function with the context of that specific job.
Each session runs in a separate process to ensure that agents are isolated from each other. This isolation means that if an agent instance crashes, it will not affect other agents running on the same worker.
Entrypoint
The entrypoint function is called as soon as the job is assigned to the worker. From there, you have full control over the session. The session will continue to run until all human participants have left the room, or when the job is explicitly shut down.
Python
Node.js
async
def
do_something
(
track
:
rtc
.
RemoteAudioTrack
)
:
audio_stream
=
rtc
.
AudioStream
(
track
)
async
for
event
in
audio_stream
:
# Do something here to process event.frame
pass
await
audio_stream
.
aclose
(
)
async
def
entrypoint
(
ctx
:
JobContext
)
:
# an rtc.Room instance from the LiveKit Python SDK
room
=
ctx
.
room
# set up listeners on the room before connecting
@room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
*
_
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_AUDIO
:
asyncio
.
create_task
(
do_something
(
track
)
)
# connect to room
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
# when connected, room.local_participant represents the agent
await
room
.
local_participant
.
publish_data
(
"hello world"
)
# iterate through currently connected remote participants
for
rp
in
room
.
remote_participants
.
values
(
)
:
print
(
rp
.
identity
)
Copy
Working examples of LiveKit Agents for Python are available in the
repository
. More on publishing and receiving tracks will be expanded in a later section.
Customizing for participant
The agent can be customized to behave differently based on the connected participant, enabling a personalized experience.
LiveKit provides several ways to identify participants:
ctx.room.name
: the name that the participant is connected to
participant.identity
: the identity of the participant
participant.attributes
:
custom attributes
set on the participant
Here's an example:
Python
Node.js
async
def
entrypoint
(
ctx
:
JobContext
)
:
# connect to the room
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
# wait for the first participant to arrive
participant
=
await
ctx
.
wait_for_participant
(
)
# customize behavior based on the participant
print
(
f"connected to room
{
ctx
.
room
.
name
}
with participant
{
participant
.
identity
}
"
)
# inspect the current value of the attribute
language
=
participant
.
attributes
.
get
(
"user.language"
)
# listen to when the attribute is changed
@ctx
.
room
.
on
(
"participant_attributes_changed"
)
def
on_participant_attributes_changed
(
changed_attrs
:
dict
[
str
,
str
]
,
p
:
rtc
.
Participant
)
:
if
p
==
participant
:
language
=
p
.
attributes
.
get
(
"user.language"
)
print
(
f"participant
{
p
.
identity
}
changed language to
{
language
}
"
)
Copy
Ending the session
Disconnecting the agent
When the agent should no longer be in the room, you could disconnect it from the room. This will allow the other participants in the session to continue. After disconnection, your shutdown hooks will also be called.
Python
Node.js
async
def
entrypoint
(
ctx
:
JobContext
)
:
# do some work
.
.
.
# disconnect from the room
ctx
.
shutdown
(
reason
=
"Session ended"
)
Copy
Disconnecting everyone
If the session should end for everyone, use the server API
deleteRoom
to end the session.
The
Disconnected
room event
will be sent, and the room will be removed from the server.
Python
Node.js
from
livekit
import
api
async
def
entrypoint
(
ctx
:
JobContext
)
:
# do some work
.
.
.
api_client
=
api
.
LiveKitAPI
(
os
.
getenv
(
"LIVEKIT_URL"
)
,
os
.
getenv
(
"LIVEKIT_API_KEY"
)
,
os
.
getenv
(
"LIVEKIT_API_SECRET"
)
,
)
await
api_client
.
room
.
delete_room
(
api
.
DeleteRoomRequest
(
room
=
ctx
.
job
.
room
.
name
,
)
)
Copy
Post-processing and cleanup
After the session has ended, you may want to perform post-processing or cleanup tasks. This could include saving user state in a database. Agents framework supports shutdown hooks that are called when the session ends.
Python
Node.js
async
def
entrypoint
(
ctx
:
JobContext
)
:
async
def
my_shutdown_hook
(
)
:
# save user state
.
.
.
ctx
.
add_shutdown_callback
(
my_shutdown_hook
)
Copy
Note
Shutdown hooks are expected to complete within a short amount of time. By default, the framework waits 60 seconds before forcefully terminating the agent process. You can adjust this timeout using the
shutdown_process_timeout
parameter in
WorkerOptions
.
On this page
Entrypoint
Customizing for participant
Ending the session
Disconnecting the agent
Disconnecting everyone
Post-processing and cleanup
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/build/tracks:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Receiving
Working with video
Publishing
Publishing audio
Publishing video
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Audio and video
.
v1.0 for Node.js is coming soon.
Receiving
Reading WebRTC tracks via LiveKit is done through streams, which are exposed in Python as AsyncIterators. The LiveKit SDKs provide utilities for working with both audio and video tracks:
Python
Node.js
async
def
do_something
(
track
:
rtc
.
Track
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_AUDIO
:
audio_stream
=
rtc
.
AudioStream
(
track
)
async
for
event
in
audio_stream
:
# Do something here to process event.frame
pass
await
audio_stream
.
aclose
(
)
elif
track
.
kind
==
rtc
.
TrackKind
.
KIND_VIDEO
:
video_stream
=
rtc
.
VideoStream
(
track
)
async
for
event
in
video_stream
:
# Do something here to process event.frame
pass
await
video_stream
.
aclose
(
)
@ctx
.
room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
publication
:
rtc
.
TrackPublication
,
participant
:
rtc
.
RemoteParticipant
,
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_AUDIO
:
asyncio
.
create_task
(
do_something
(
track
)
)
elif
track
.
kind
==
rtc
.
TrackKind
.
KIND_VIDEO
:
asyncio
.
create_task
(
do_something
(
track
)
)
Copy
As in every LiveKit SDK, the
TrackSubscribed
event
is triggered when a track is subscribed to. The agent can be configured to automatically subscribe to tracks when you accept the job. To manage subscriptions manually, set
auto_subscribe
to
AutoSubscribe.SUBSCRIBE_NONE
:
Python
Node.js
async
def
entrypoint_fnc
(
ctx
:
JobContext
)
:
await
ctx
.
connect
(
# valid values are SUBSCRIBE_ALL, SUBSCRIBE_NONE, VIDEO_ONLY, AUDIO_ONLY
# when omitted, it defaults to SUBSCRIBE_ALL
auto_subscribe
=
AutoSubscribe
.
SUBSCRIBE_NONE
,
)
Copy
Note
All subscribed tracks will be streamed to the machine. To ensure efficient use of resources, subscribe only to the tracks that your agent needs.
Working with video
Because different applications work with different video buffer encodings, LiveKit supports many and translates between them automatically.
VideoFrame
provides the current video buffer type and a method to convert it to any of the other encodings:
Python
Node.js
async
def
handle_video
(
track
:
rtc
.
Track
)
:
video_stream
=
rtc
.
VideoStream
(
track
)
async
for
event
in
video_stream
:
video_frame
=
event
.
frame
current_type
=
video_frame
.
type
frame_as_bgra
=
video_frame
.
convert
(
rtc
.
VideoBufferType
.
BGRA
)
# [...]
await
video_stream
.
aclose
(
)
@ctx
.
room
.
on
(
"track_subscribed"
)
def
on_track_subscribed
(
track
:
rtc
.
Track
,
publication
:
rtc
.
TrackPublication
,
participant
:
rtc
.
RemoteParticipant
,
)
:
if
track
.
kind
==
rtc
.
TrackKind
.
KIND_VIDEO
:
asyncio
.
create_task
(
handle_video
(
track
)
)
Copy
Publishing
Similarly, publishing data to the agent’s track is done by transmitting a continuous live feed. Audio streams carry raw PCM data at a specified sample rate and channel count, while video streams can transmit data in any of 11 buffer encodings.
Publishing audio
Publishing audio involves splitting the stream into audio frames of a configurable length. An internal buffer holds 50ms of queued audio to be sent to the realtime stack. The
capture_frame
method, used to send new frames, is blocking and will not return control until the buffer has taken in the entire frame. This allows for easier interruption handling.
In order to publish an audio track, the sample rate and number of channels need to be determined beforehand, as well as the length (number of samples) of each frame. The following example transmits a constant 16-bit sine wave at 48kHz in 10ms long frames:
Python
Node.js
SAMPLE_RATE
=
48000
NUM_CHANNELS
=
1
# mono audio
AMPLITUDE
=
2
**
8
-
1
SAMPLES_PER_CHANNEL
=
480
# 10ms at 48kHz
async
def
entrypoint
(
ctx
:
JobContext
)
:
await
ctx
.
connect
(
)
source
=
rtc
.
AudioSource
(
SAMPLE_RATE
,
NUM_CHANNELS
)
track
=
rtc
.
LocalAudioTrack
.
create_audio_track
(
"example-track"
,
source
)
# since the agent is a participant, our audio I/O is its "microphone"
options
=
rtc
.
TrackPublishOptions
(
source
=
rtc
.
TrackSource
.
SOURCE_MICROPHONE
)
# ctx.agent is an alias for ctx.room.local_participant
publication
=
await
ctx
.
agent
.
publish_track
(
track
,
options
)
frequency
=
440
async
def
_sinewave
(
)
:
audio_frame
=
rtc
.
AudioFrame
.
create
(
SAMPLE_RATE
,
NUM_CHANNELS
,
SAMPLES_PER_CHANNEL
)
audio_data
=
np
.
frombuffer
(
audio_frame
.
data
,
dtype
=
np
.
int16
)
time
=
np
.
arange
(
SAMPLES_PER_CHANNEL
)
/
SAMPLE_RATE
total_samples
=
0
while
True
:
time
=
(
total_samples
+
np
.
arange
(
SAMPLES_PER_CHANNEL
)
)
/
SAMPLE_RATE
sinewave
=
(
AMPLITUDE
*
np
.
sin
(
2
*
np
.
pi
*
frequency
*
time
)
)
.
astype
(
np
.
int16
)
np
.
copyto
(
audio_data
,
sinewave
)
# send this frame to the track
await
source
.
capture_frame
(
frame
)
total_samples
+=
samples_per_channel
Copy
Warning
When streaming finite audio (e.g. from a file), make sure the frame length isn't longer than the amount of samples left to stream, otherwise the end of the buffer will consist of noise.
Publishing video
When publishing video tracks, the frame rate and buffer encoding of the video need to be established beforehand. In this example, the Agent connects to the room and starts publishing a solid color frame at 10 frames per second:
Python
Node.js
WIDTH
=
640
HEIGHT
=
480
async
def
entrypoint
(
ctx
:
JobContext
)
:
await
ctx
.
connect
(
)
source
=
rtc
.
VideoSource
(
WIDTH
,
HEIGHT
)
track
=
rtc
.
LocalVideoTrack
.
create_video_track
(
"example-track"
,
source
)
options
=
rtc
.
TrackPublishOptions
(
# since the agent is a participant, our video I/O is its "camera"
source
=
rtc
.
TrackSource
.
SOURCE_CAMERA
,
simulcast
=
True
,
# when modifying encoding options, max_framerate and max_bitrate must both be set
video_encoding
=
rtc
.
VideoEncoding
(
max_framerate
=
30
,
max_bitrate
=
3_000_000
,
)
,
audio_encoding
=
rtc
.
AudioEncoding
(
max_bitrate
=
48000
)
,
video_codec
=
rtc
.
VideoCodec
.
H264
,
)
publication
=
await
ctx
.
agent
.
publish_track
(
track
,
options
)
# this color is encoded as ARGB. when passed to VideoFrame it gets re-encoded.
COLOR
=
[
255
,
255
,
0
,
0
]
;
# FFFF0000 RED
async
def
_draw_color
(
)
:
argb_frame
=
bytearray
(
WIDTH
*
HEIGHT
*
4
)
while
True
:
await
asyncio
.
sleep
(
0.1
)
# 10 fps
argb_frame
[
:
]
=
COLOR
*
WIDTH
*
HEIGHT
frame
=
rtc
.
VideoFrame
(
WIDTH
,
HEIGHT
,
rtc
.
VideoBufferType
.
RGBA
,
argb_frame
)
# send this frame to the track
source
.
capture_frame
(
frame
)
asyncio
.
create_task
(
_draw_color
(
)
)
Copy
Note
Although the frame being published is static, it is still necessary to stream it continuously, for the benefit of participants joining the room after the initial frame had been sent.
Note
Unlike audio, video
capture_frame
does not keep an internal buffer.
On this page
Receiving
Working with video
Publishing
Publishing audio
Publishing video
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/build/metrics:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Logging events
Aggregating metrics
Metrics reference
Speech-to-text (STT)
LLM
Text-to-speech (TTS)
End-of-utterance (EOU)
Measuring conversation latency
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Logging and metrics
.
v1.0 for Node.js is coming soon.
Overview
For increased observability into the performance and model usage by your agent, you can enable and log detailed metrics that are provided by LiveKit Agents. These metrics provide detailed insights into the duration, latency, and usage across the stages of a session and are provided for both
VoicePipelineAgent
and
MultimodalAgent
.
Logging events
Agent metrics events are fired by LiveKit Agents whenever there is a new metrics object available during an active session.
In order to capture newly available metrics objects in your agent, import the
metrics
module from LiveKit Agents and subscribe to the
metrics_collected
event. When your agent receives this event, log the available metrics in your agent.
The
metrics
module includes a simple helper function which formats logging output based upon the type of metrics received in the event. Call the
log_metrics
helper function when you receive a new
metrics_collected
event in order to utilize this formatting for your logs.
Python
Node.js
# The metrics module is required to capture agent metrics
from
livekit
.
agents
import
metrics
# Subscribe to metrics collection events and process accordingly
@agent
.
on
(
"metrics_collected"
)
def
_on_metrics_collected
(
mtrcs
:
metrics
.
AgentMetrics
)
:
# Use this helper to format and log based on metrics type
metrics
.
log_metrics
(
mtrcs
)
Copy
Aggregating metrics
The
metrics
module also includes a helper class that can be used to aggregate usage metrics over the course of a session and generate a summary once the session is complete. Use the
UsageCollector
class
Python
Node.js
# Use the usage collector to aggregate agent usage metrics
usage_collector
=
metrics
.
UsageCollector
(
)
# Add metrics to usage collector as they are received
@agent
.
on
(
"metrics_collected"
)
def
_on_metrics_collected
(
mtrcs
:
metrics
.
AgentMetrics
)
:
# Pass the latest usage metrics to the usage collector for aggregation
usage_collector
.
collect
(
mtrcs
)
# Log aggregated summary of usage metrics generated by usage collector
async
def
log_usage
(
)
:
summary
=
usage_collector
.
get_summary
(
)
logger
.
info
(
f"Usage: $
{
summary
}
"
)
# At shutdown, generate and log the summary from the usage collector
ctx
.
add_shutdown_callback
(
log_usage
)
Copy
Metrics reference
Note
The following metric types are available for
VoicePipelineAgent
.
Speech-to-text (STT)
STT metrics events are reported by the
metrics
module when the STT model being used by the agent has generated output.
Python
Node.js
Metric
Description
audio_duration
The duration (seconds) of the audio input received by the STT model.
duration
The total amount of time (seconds) that the connection has been open with the STT provider.
LLM
LLM metrics events are reported by the
metrics
module when the LLM being used by the agent has generated a completion.
Python
Node.js
Metric
Description
ttft
Time to first token. The amount of time (seconds) that it took for the LLM to generate the first token of the completion.
input_tokens
The number of tokens provided in the prompt sent to the LLM.
output_tokens
The number of tokens generated by the LLM in the completion.
tokens_per_second
The rate of token generation (tokens/second) by the LLM to generate the completion.
Text-to-speech (TTS)
TTS metrics events are reported by the
metrics
module when the TTS model being used by the agent has generated output.
Python
Node.js
Metric
Description
ttfb
Time to first byte. The amount of time (seconds) that it took for the TTS model to generate the first byte of its audio output.
audio_duration
The duration (seconds) of the audio output generated by the TTS model.
End-of-utterance (EOU)
EOU metrics events are reported by the
metrics
module when the agent is about to play speech back to the user.
Python
Node.js
Metric
Description
end_of_utterance_delay
Total amount of time (seconds) between when VAD detected end of speech and when LLM inference was performed.
transcription_delay
The amount of time (seconds) for the STT model to generate a final transcript.
Measuring conversation latency
Total conversation latency is defined as the time it takes for the agent to respond to a user's utterance. Given the metrics above, it can be computed as follows:
Python
Node.js
total_latency
=
eou
.
end_of_utterance_delay
+
llm
.
ttft
+
tts
.
ttfb
Copy
On this page
Overview
Logging events
Aggregating metrics
Metrics reference
Speech-to-text (STT)
LLM
Text-to-speech (TTS)
End-of-utterance (EOU)
Measuring conversation latency
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/build/record:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Video or audio recording
Example
Transcriptions
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Session recording and transcripts
.
v1.0 for Node.js is coming soon.
There are multiple reasons you might want to record AI agent conversations with users. These reasons might include monitoring agent
performance, evaluating customer satisfaction, or for regulatory compliance. LiveKit allows you to record
video and audio of AI agent conversations or save conversations as text transcripts.
Video or audio recording
LiveKit's
Egress feature
provides flexible options for recording audio and/or video. Start a
room composite recorder
in your agent's entrypoint to record a session. Room recording starts when the agent enters a room with a user and captures all the participants and interactions in a LiveKit room. Recording ends when all participants leave and the room is closed.
LiveKit egress requires access to a cloud storage provider to upload the recording files. The following example uses
Google Cloud Storage, but you can also save files to any Amazon S3-compatible storage provider or Azure Blob Storage.
Example
This example uses the
VoicePipelineAgent template
as a starting point.
Clone the
repo
or run the following LiveKit CLI command:
lk app create
--template
=
voice-pipeline-agent-python my-recording-app
Copy
After you run the command, follow the instructions in the command output to finish setup.
Update the
agent.py
file to import
livekit.api
:
from
livekit
import
api
Copy
Update the
entrypoint
function to add room recording.
This example uses Google Cloud Storage. The
credentials.json
file includes authentication credentials for accessing
the bucket. For additional egress examples using Amazon S3 and Azure, see the
Egress examples
. To learn more about
credentials.json
, see
Cloud storage configurations
.
Replace
<my-bucket>
and update the entrypoint function with the following:
Note
To record only audio, update the
audio_only
parameter to
True
and remove the
preset
parameter.
async
def
entrypoint
(
ctx
:
JobContext
)
:
# Get GCP credentials from credentials.json file.
file_contents
=
""
with
open
(
"/path/to/credentials.json"
,
"r"
)
as
f
:
file_contents
=
f
.
read
(
)
# Set up recording
req
=
api
.
RoomCompositeEgressRequest
(
room_name
=
"my-room"
,
layout
=
"speaker"
,
preset
=
api
.
EncodingOptionsPreset
.
H264_720P_30
,
audio_only
=
False
,
segment_outputs
=
[
api
.
SegmentedFileOutput
(
filename_prefix
=
"my-output"
,
playlist_name
=
"my-playlist.m3u8"
,
live_playlist_name
=
"my-live-playlist.m3u8"
,
segment_duration
=
5
,
gcp
=
api
.
GCPUpload
(
credentials
=
file_contents
,
bucket
=
"<my-bucket>"
,
)
,
)
]
,
)
lkapi
=
api
.
LiveKitAPI
(
)
res
=
await
lkapi
.
egress
.
start_room_composite_egress
(
req
)
initial_ctx
=
llm
.
ChatContext
(
)
.
append
(
role
=
"system"
,
text
=
(
"You are a voice assistant created by LiveKit. Your interface with users will be voice. "
"You should use short and concise responses, and avoiding usage of unpronouncable punctuation. "
"You were created as a demo to showcase the capabilities of LiveKit's agents framework."
)
,
)
logger
.
info
(
f"connecting to room
{
ctx
.
room
.
name
}
"
)
await
ctx
.
connect
(
auto_subscribe
=
AutoSubscribe
.
AUDIO_ONLY
)
# Wait for the first participant to connect
participant
=
await
ctx
.
wait_for_participant
(
)
logger
.
info
(
f"starting voice assistant for participant
{
participant
.
identity
}
"
)
# This project is configured to use Deepgram STT, OpenAI LLM and TTS plugins
# Other great providers exist like Cartesia and ElevenLabs
# Learn more and pick the best one for your app:
# https://docs.livekit.io/agents/v0/integrations
agent
=
VoicePipelineAgent
(
vad
=
ctx
.
proc
.
userdata
[
"vad"
]
,
stt
=
deepgram
.
STT
(
)
,
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
,
tts
=
openai
.
TTS
(
)
,
chat_ctx
=
initial_ctx
,
)
agent
.
start
(
ctx
.
room
,
participant
)
# The agent should be polite and greet the user when it joins :)
await
agent
.
say
(
"Hey, how can I help you today?"
,
allow_interruptions
=
True
)
await
lkapi
.
aclose
(
)
Copy
Start the agent:
python3 agent
.
py dev
Copy
Recording starts when a participant joins a room and the agent is dispatched to that room. After the participant
leaves the room, the recording stops. Files are uploaded to storage as they're recorded.
Transcriptions
This section describes creating a text log of a conversation by the agent process (that is, server side). For transcriptions for your frontend applications, see
Transcriptions
.
You can save the text of a conversation with an AI voice agent by listening for agent events and logging user and agent speech to a text file. For example, log messages when user speech is committed (
user_speech_committed
) and when the agent stops speaking (
agent_stopped_speaking
).
For a list of events emitted by agents, see the following topics:
VoicePipelineAgent events
MultimodalAgent events
For example code in Python, see this example of a
MultimodalAgent that saves conversation to a text file
.
On this page
Video or audio recording
Example
Transcriptions
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/plugins:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
STT
Voice activity detector (VAD) and StreamAdapter
TTS
Building your own
LiveKit plugins
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Integration guides
.
v1.0 for Node.js is coming soon.
The Agents framework includes a set of prebuilt plugins that make it easier to build an AI agent. These plugins cover common tasks like speech-to-text (STT), text-to-speech (TTS), running inference on a generative AI model, and more.
The API for plugins is standardized to make it easy to switch between different providers. Having a consistent interface also makes it simpler for anyone to extend the framework and build new plugins for other providers.
STT
STT converts audio frames to a stream of text. The following example uses Deepgram STT to process an audio stream:
from
livekit
import
agents
,
rtc
from
livekit
.
plugins
import
deepgram
from
livekit
.
agents
.
stt
import
SpeechEventType
,
SpeechEvent
from
typing
import
AsyncIterable
async
def
process_track
(
ctx
:
agents
.
JobContext
,
track
:
rtc
.
Track
)
:
stt
=
deepgram
.
STT
(
)
stt_stream
=
stt
.
stream
(
)
audio_stream
=
rtc
.
AudioStream
(
track
)
ctx
.
create_task
(
process_text_from_speech
(
stt_stream
)
)
async
for
audio_event
in
audio_stream
:
stt_stream
.
push_frame
(
audio_event
.
frame
)
stt_stream
.
end_input
(
)
async
def
process_text_from_speech
(
self
,
stream
:
AsyncIterable
[
SpeechEvent
]
)
:
async
for
event
in
stream
:
if
event
.
type
==
SpeechEventType
.
FINAL_TRANSCRIPT
:
text
=
event
.
alternatives
[
0
]
.
text
# Do something with text
elif
event
.
type
==
SpeechEventType
.
INTERIM_TRANSCRIPT
:
pass
elif
event
.
type
==
SpeechEventType
.
START_OF_SPEECH
:
pass
elif
event
.
type
==
SpeechEventType
.
END_OF_SPEECH
:
pass
await
stream
.
aclose
(
)
Copy
Voice activity detector (VAD) and StreamAdapter
Some providers or models, such as Whisper, do not support streaming input. In these cases, the application must determine when a chunk of audio represents a
complete segment of speech. This can be accomplished using a VAD together with the
StreamAdapter
class.
The following example modifies the example above to use VAD and StreamAdapter:
from
livekit
import
agents
,
rtc
from
livekit
.
plugins
import
openai
,
silero
async
def
process_track
(
ctx
:
agents
.
JobContext
,
track
:
rtc
.
Track
)
:
whisper_stt
=
openai
.
STT
(
)
vad
=
silero
.
VAD
.
load
(
min_speech_duration
=
0.1
,
min_silence_duration
=
0.5
,
)
vad_stream
=
vad
.
stream
(
)
# StreamAdapter will buffer audio until VAD emits END_SPEAKING event
stt
=
agents
.
stt
.
StreamAdapter
(
whisper_stt
,
vad_stream
)
stt_stream
=
stt
.
stream
(
)
.
.
.
Copy
TTS
TTS synthesizes text into audio frames. The following example uses ElevenLabs TTS to convert text input into audio and
plays the audio stream:
from
livekit
import
agents
,
rtc
from
livekit
.
agents
.
tts
import
SynthesizedAudio
from
livekit
.
plugins
import
elevenlabs
from
typing
import
AsyncIterable
ctx
:
agents
.
JobContext
=
.
.
.
text_stream
:
AsyncIterable
[
str
]
=
.
.
.
audio_source
=
rtc
.
AudioSource
(
44100
,
1
)
track
=
rtc
.
LocalAudioTrack
.
create_audio_track
(
"agent-audio"
,
audio_source
)
await
ctx
.
room
.
local_participant
.
publish_track
(
track
)
tts
=
elevenlabs
.
TTS
(
model_id
=
"eleven_turbo_v2"
)
tts_stream
=
tts
.
stream
(
)
# create a task to consume and publish audio frames
ctx
.
create_task
(
send_audio
(
tts_stream
)
)
# push text into the stream, TTS stream will emit audio frames along with events
# indicating sentence (or segment) boundaries.
async
for
text
in
text_stream
:
tts_stream
.
push_text
(
text
)
tts_stream
.
end_input
(
)
async
def
send_audio
(
audio_stream
:
AsyncIterable
[
SynthesizedAudio
]
)
:
async
for
a
in
audio_stream
:
await
audio_source
.
capture_frame
(
e
.
audio
.
frame
)
Copy
Building your own
The plugin framework is designed to be extensible, allowing anyone to build their own plugin. Your plugin can integrate with various providers or directly load models for local inference.
By adopting the standard STT or TTS interfaces, you can abstract away implementation specifics and simplify switching between different providers in your agent code.
Code contributions to plugins are always welcome. To learn more, see the guidelines for contributions to
the
Python repository
or
the
Node.js repository
.
LiveKit plugins
The following plugins provide utilities for LiveKit agents. For a list of plugins for providers of LLM, STT, and TTS,
see
Integration guides for LiveKit Agents
.
Plugin
SDK
Feature
livekit-plugins-browser
Python
Chrome browser.
livekit-plugins-llama-index
Python
Support for LlamaIndex
query engine
and
chat engine
. Query engine is used primarily for RAG.
Chat engine can be used as an LLM in a pipeline agent.
livekit-plugins-nltk
Python
Utilities for working with text using
NLTK
.
livekit-plugins-rag
Python
Vector retrieval with
Annoy
.
livekit-plugins-silero
Python
,
Node.js
Silero VAD
.
livekit-plugins-turn-detector
Python
LiveKit
turn detector
.
On this page
STT
Voice activity detector (VAD) and StreamAdapter
TTS
Building your own
LiveKit plugins
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/openai-compatible-llms:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Customize the LLM for your voice agent
Example voice agent
Supported LLMs
Method name
Syntax
Parameters
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
LLM integration guides
.
v1.0 for Node.js is coming soon.
Customize the LLM for your voice agent
The OpenAI plugin provides methods that allow you to use OpenAI API compatible LLMs. In most cases, you can call the method with no parameter values by setting the required environment variables and accepting default values. The minimal syntax for each method assumes the environment variables for required values (for example, API keys) are set.
Support for additional LLMs is available through other
LiveKit plugins
.
Example voice agent
Use the
VoicePipelineAgent
class and the OpenAI plugin to specify the LLM. In this example, use Groq as the LLM:
Set the
GROQ_API_KEY
environment variable:
export
GROQ_API_KEY
=
<
your_groq_api_key
>
Copy
Create an agent:
Python
Node.js
agent
=
VoicePipelineAgent
(
vad
=
ctx
.
proc
.
userdata
[
"vad"
]
,
stt
=
deepgram
.
STT
(
)
,
llm
=
openai
.
LLM
.
with_groq
(
)
tts
=
cartesia
.
TTS
(
)
,
chat_ctx
=
initial_ctx
,
)
Copy
Supported LLMs
The OpenAI plugin offers support for the following LLMs:
Azure
Cerebras
Deepseek
Fireworks
Groq
Octo
Ollama
Perplexity
Telnyx
Together
xAI
Select an LLM in the dropdown menu to view parameters and syntax:
Azure
Cerebras
DeepSeek
Fireworks
Groq
Octo
Ollama
Perplexity
Telnyx
Together
xAI
Method name
with_azure
Syntax
The minimal syntax for this method assumes the environment variables for required values (for example, API keys) are set.
Python
Node.js
agent
=
VoicePipelineAgent
(
vad
=
ctx
.
proc
.
userdata
[
"vad"
]
,
stt
=
deepgram
.
STT
(
)
,
llm
=
openai
.
LLM
.
with_azure
(
)
tts
=
cartesia
.
TTS
(
)
,
chat_ctx
=
initial_ctx
,
)
Copy
Parameters
The
with_azure
method accepts the following parameters:
Python
Node.js
To learn more, see the
plugin documentation
.
Parameter
Data type
Default value /
Environment variable
model
String
gpt-4o
azure_endpoint
String
AZURE_OPENAI_ENDPOINT
azure_deployment
String
api_version
String
OPENAI_API_VERSION
api_key
String
AZURE_OPENAI_API_KEY
azure_ad_token
String
AZURE_OPENAI_AD_TOKEN
azure_ad_token_provider
AsyncAzureADTokenProvider
organization
String
OPENAI_ORG_ID
project
String
OPENAI_PROJECT_ID
base_url
String
user
String
temperature
Float
On this page
Customize the LLM for your voice agent
Example voice agent
Supported LLMs
Method name
Syntax
Parameters
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/azure:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Azure OpenAI LLM
Azure OpenAI TTS
Azure Speech STT
Azure Speech TTS
Azure Realtime API
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Azure integration
.
v1.0 for Node.js is coming soon.
Overview
LiveKit's Azure integration provides support for multiple Azure AI Services. These include
Azure OpenAI
,
Speech service
for STT and TTS, and
Realtime API
. Azure OpenAI allows you to run OpenAI models using the security capabilities of Microsoft Azure. The Speech service's STT and TTS allow you to transcribe speech-to-text with high accuracy and produce natural-sounding text-to-speech voices. Azure's Realtime API processes user input and responds immediately, allowing you to create agents that sound naturally responsive.
LiveKit provides multiple integration paths for using Azure AI Services for building agents:
OpenAI plugin support for Azure OpenAI
LLM
and
TTS
.
Azure plugin for Speech service
STT
and
TTS
.
OpenAI plugin support for Azure AI Services
Realtime API
.
You can use Azure STT, TTS, and LLM to create agents using the
VoicePipelineAgent
class. To use the Realtime API, you can create an agent using the
MultimodalAgent
class.
Quick reference
The following sections provide a quick reference for integrating Azure AI Services with LiveKit. For the complete
reference, see the links provided in each section.
Azure OpenAI LLM
LiveKit's Azure integration provides an
OpenAI compatible
LLM interface.
This can be used as the LLM for an agent created using the
VoicePipelineAgent
class.
Azure's OpenAI compatible API needs to be configured to connect to OpenAI. You can set the environment variables listed
in the usage section or pass in these values when you create the LLM instance.
LLM.with_azure usage
Use the
with_azure
method to create an instance of an Azure OpenAI LLM:
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
LLM
azure_llm
=
LLM
.
with_azure
(
model
=
"gpt-4o"
,
temperature
=
0.8
,
)
Copy
LLM.with_azure parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
model
string
Optional
Default:
gpt-4o
#
ID of the model to use for inference. To learn more, see
supported models
.
azure_endpoint
string
Optional
Env:
AZURE_OPENAI_ENDPOINT
#
Azure OpenAI endpoint in the following format:
https://{your-resource-name}.openai.azure.com
.
azure_deployment
string
Optional
#
Name of your model deployment.
api_version
string
Optional
Env:
OPENAI_API_VERSION
#
OpenAI REST API version used for the request.
api_key
string
Optional
Env:
AZURE_OPENAI_API_KEY
#
Azure OpenAI API key.
azure_ad_token
string
Optional
Env:
AZURE_OPENAI_AD_TOKEN
#
Azure Active Directory token.
azure_ad_token_provider
string
Optional
#
Function that returns an Azure Active Directory token.
organization
string
Optional
Env:
OPENAI_ORG_ID
#
OpenAI organization ID.
project
string
Optional
Env:
OPENAI_PROJECT_ID
#
OpenAI project ID.
temperature
float
Optional
Default:
1.0
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more,
see
chat completions
.
Azure OpenAI TTS
LiveKit's Azure integration provides an
OpenAI compatible
text-to-speech (TTS) interface.
This can be used for speech generation for an agent created with the
VoicePipelineAgent
class.
Azure's OpenAI compatible API needs to be configured to connect to OpenAI. You can set the environment variables listed
in the usage section or pass in these values when you create the TTS instance.
TTS.create_azure_client usage
Use the
TTS.create_azure_client
method to create an instance of an Azure OpenAI TTS:
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
tts
azure_tts
=
tts
.
TTS
.
create_azure_client
(
model
=
"tts-1"
,
voice
=
"alloy"
,
)
Copy
TTS.create_azure_client parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
model
string
Optional
Default:
tts-1
#
ID of the model to use for TTS. To learn more, see
supported models
.
voice
string
Optional
#
OpenAI text-to-speech voice. To learn more, see
Voice options
.
azure_endpoint
string
Optional
Env:
AZURE_OPENAI_ENDPOINT
#
Azure OpenAI endpoint in the following format:
https://{your-resource-name}.openai.azure.com
.
azure_deployment
string
Optional
#
Name of your model deployment.
api_version
string
Optional
Env:
OPENAI_API_VERSION
#
OpenAI REST API version used for the request.
api_key
string
Optional
Env:
AZURE_OPENAI_API_KEY
#
Azure OpenAI API key.
azure_ad_token
string
Optional
Env:
AZURE_OPENAI_AD_TOKEN
#
Azure Active Directory token.
organization
string
Optional
Env:
OPENAI_ORG_ID
#
OpenAI organization ID.
project
string
Optional
Env:
OPENAI_PROJECT_ID
#
OpenAI project ID.
Azure Speech STT
LiveKit's Azure plugin provides support for Speech service STT. To connect to Azure's Speech service, set the environment
variables listed in the usage section, or pass these values in when you create an STT instance.
Note
The Azure plugin is currently only available for the Python Agents framework.
Azure Speech STT usage
.env.local
Python
AZURE_SPEECH_KEY
=
<
azure-speech-key
>
AZURE_SPEECH_REGION
=
<
azure-speech-region
>
AZURE_SPEECH_HOST
=
<
azure-speech-host
>
LIVEKIT_API_KEY
=
<
livekit-api-key
>
LIVEKIT_API_SECRET
=
<
livekit-api-secret
>
LIVEKIT_URL
=
<
livekit-url
>
Copy
Azure Speech STT parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
Note
To create an instance of
azure.STT
, one of the following options must be met:
speech_host
must be set,
or
speech_key
and
speech_region
must both be set,
or
speech_auth_token
and
speech_region
must both be set
speech_key
string
Optional
Env:
AZURE_SPEECH_KEY
#
Azure Speech speech-to-text key. To learn more, see
Azure Speech prerequisites
.
speech_region
string
Optional
Env:
AZURE_SPEECH_REGION
#
Azure Speech speech-to-text region. To learn more, see
Azure Speech prerequisites
.
speech_host
string
Optional
Env:
AZURE_SPEECH_HOST
#
Azure Speech endpoint.
speech_auth_token
string
Optional
#
Azure Speech authentication token.
languages
list[string]
Optional
#
List of potential source languages. To learn more, see
Standard locale names
.
Azure Speech TTS
LiveKit's Azure plugin provides support for Speech service TTS. To connect to Azure's Speech service, set the environment
variables listed in the usage section, or pass these values in when you create a TTS instance.
Note
The Azure plugin is currently only available for the Python Agents framework.
Python
.env.local
from
livekit
.
plugins
import
azure
azure_stt
=
azure
.
TTS
(
speech_key
=
"<speech_service_key>"
,
speech_region
=
"<speech_service_region>"
,
)
Copy
Azure Speech TTS parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
Note
To create an instance of
azure.TTS
, one of the following options must be met:
speech_host
must be set,
or
speech_key
and
speech_region
must both be set,
or
speech_auth_token
and
speech_region
must both be set
voice
string
Optional
#
Voice for text-to-speech. To learn more, see
Select synthesis language and voice
.
language
string
Optional
#
Language of the input text. To learn more, see
Select synthesis language and voice
.
prosody
ProsodyConfig
Optional
#
Specify changes to pitch, rate, and volume for the speech output. To learn more,
see
Adjust prosody
.
speech_key
string
Optional
Env:
AZURE_SPEECH_KEY
#
Azure Speech speech-to-text key. To learn more, see
Azure Speech prerequisites
.
speech_region
string
Optional
Env:
AZURE_SPEECH_REGION
#
Azure Speech speech-to-text region. To learn more, see
Azure Speech prerequisites
.
speech_host
string
Optional
Env:
AZURE_SPEECH_HOST
#
Azure Speech endpoint.
speech_auth_token
string
Optional
#
Azure Speech authentication token.
Azure Realtime API
LiveKit's OpenAI plugin provides support for Azure AI Services Realtime API when you create an agent
with the
MultimodalAgent
class.
To use the Realtime API, use the
RealtimeModel.with_azure
method.
RealtimeModel.with_azure usage
Create an instance of
MultimodalAgent
using Azure's Realtime API:
agent
=
multimodal
.
MultimodalAgent
(
model
=
openai
.
realtime
.
RealtimeModel
.
with_azure
(
azure_deployment
=
"<model-deployment>"
,
azure_endpoint
=
"wss://<endpoint>.openai.azure.com/"
,
# or AZURE_OPENAI_ENDPOINT
api_key
=
"<api-key>"
,
# or AZURE_OPENAI_API_KEY
api_version
=
"2024-10-01-preview"
,
# or OPENAI_API_VERSION
voice
=
"alloy"
,
temperature
=
0.8
,
instructions
=
"You are a helpful assistant"
,
turn_detection
=
openai
.
realtime
.
ServerVadOptions
(
threshold
=
0.6
,
prefix_padding_ms
=
200
,
silence_duration_ms
=
500
)
,
)
,
fnc_ctx
=
fnc_ctx
,
)
Copy
RealtimeModel.with_azure parameters
This section describes some of the parameters for the
RealtimeModel.with_azure
method.
For a full list of parameters, see the
plugin documentation
.
azure_deployment
string
Optional
#
Name of your model deployment.
azure_endpoint
string
Optional
Env:
AZURE_OPENAI_ENDPOINT
#
Azure OpenAI endpoint in the following format:
https://{your-resource-name}.openai.azure.com
.
api_version
string
Optional
Env:
OPENAI_API_VERSION
#
OpenAI REST API version used for the request.
api_key
string
Optional
Env:
AZURE_OPENAI_API_KEY
#
Azure OpenAI API key.
entra_token
string
Optional
#
Microsoft Entra authentication token. Required if not using API key authentication.
To learn more see Azure's
Authentication
documentation.
voice
string
Optional
Default:
alloy
#
Voice to use for speech. To learn more, see
Voice options
.
temperature
float
Optional
Default:
1.0
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more, see
chat completions
.
instructions
string
Optional
#
Initial system instructions.
modalities
list[api_proto.Modality]
Optional
Default:
["text", "audio"]
#
Modalities to use, such as ["text", "audio"].
turn_detection
ServerVadOptions
Optional
#
Server-side VAD settings.
To learn more, see
Turn detection
and
ServerVadOptions class
.
On this page
Overview
Quick reference
Azure OpenAI LLM
Azure OpenAI TTS
Azure Speech STT
Azure Speech TTS
Azure Realtime API
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/cartesia:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Environment variables
TTS
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Cartesia integration
.
v1.0 for Node.js is coming soon.
Try the playground
Chat with a voice assistant built with LiveKit and Cartesia TTS
Overview
Cartesia
provides customizable speech synthesis (TTS) across a number of different languages and produces natural-sounding speech with low latency. With LiveKit's Cartesia integration and the Agents framework, you can build AI voice applications that sound realistic. For a demonstration of what you can build, try out the
LiveKit voice assistant with Cartesia
.
Note
If you're looking to build an AI voice assistant with Cartesia, check out our
Voice Agent Quickstart
guide and use the Cartesia TTS module as demonstrated below.
Quick reference
Environment variables
.env.local
CARTESIA_API_KEY
=
<
your-cartesia-api-key
>
Copy
TTS
LiveKit's Cartesia integration provides a text-to-speech (TTS) interface. This can be used in a
VoicePipelineAgent
or as a standalone speech generator. For a complete reference of all available parameters, see the
plugin reference
.
Usage
main.py
.env.local
from
livekit
.
plugins
.
cartesia
import
tts
cartesia_tts
=
tts
.
TTS
(
model
=
"sonic-english"
,
voice
=
"c2ac25f9-ecc4-4f56-9095-651354df60c0"
,
speed
=
0.8
,
emotion
=
[
"curiosity:highest"
,
"positivity:high"
]
)
Copy
Parameters
model
string
Optional
Default:
sonic
#
ID of the model to use for generation. See
supported models
.
voice
string | list[float]
Optional
Default:
c2ac25f9-ecc4-4f56-9095-651354df60c0
#
ID of the voice to use for generation, or an embedding array. See
official documentation
.
speed
string | float
Optional
Default:
1.0
#
Speed of generated speech. Either a float in range [-1.0, 1.0], or one of
"fastest"
,
"fast"
,
"normal"
,
"slow"
,
"slowest"
. See
speed options
.
emotion
list[string]
Optional
Default:
neutral
#
Emotion of generated speech. See
emotion options
.
language
string
Optional
Default:
en
#
Language of input text in
ISO-639-1
format. For a list of languages support by model, see
supported models
.
On this page
Overview
Quick reference
Environment variables
TTS
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/cerebras:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Environment variables
LLM
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Cerebras integration
.
v1.0 for Node.js is coming soon.
Try the playground
Chat with a voice assistant built with LiveKit and Cerebras LLM
Overview
Cerebras
provides high-throughput, low-latency AI inference for Meta's Llama models. With LiveKit's Cerebras integration and the Agents framework, you can build responsive AI voice applications. For a demonstration of what you can build, check out the
voice assistant built with Cerebras and LiveKit
.
Note
If you're looking to build an AI voice assistant with Cerebras, check out our
Voice Agent Quickstart
guide and use the Cerebras LLM module as demonstrated below.
Quick reference
Environment variables
.env.local
CEREBRAS_API_KEY
=
<
your-cerebras-api-key
>
Copy
LLM
LiveKit's Cerebras integration provides an
OpenAI compatible
LLM interface. This can be used in a
VoicePipelineAgent
. For a complete reference of all available parameters, see the
plugin reference
.
Usage
main.py
.env.local
from
livekit
.
plugins
.
openai
import
llm
cerebras_llm
=
llm
.
LLM
.
with_cerebras
(
model
=
"llama3.1-8b"
,
temperature
=
0.8
,
)
Copy
Parameters
model
string
Optional
Default:
llama3.1-8b
#
ID of the model to use for inference. See
supported models
.
temperature
float
Optional
Default:
1.0
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more, see the
Cerebras documentation
.
On this page
Overview
Quick reference
Environment variables
LLM
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/deepgram:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Environment variables
STT
TTS
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Deepgram integration
.
v1.0 for Node.js is coming soon.
Overview
Deepgram
provides advanced speech recognition technology and AI-driven audio processing solutions. Customizable speech models allow you to fine tune transcription performance for your specific use case. With LiveKit's Deepgram integration and the Agents framework, you can build AI agents that provide high-accuracy transcriptions.
Note
If you're looking to build an AI voice assistant with Deepgram, check out our
Voice Agent Quickstart
guide and use the Deepgram STT and/or TTS module as demonstrated below.
Quick reference
Environment variables
.env.local
DEEPGRAM_API_KEY
=
<
your-deepgram-api-key
>
Copy
STT
LiveKit's Deepgram integration provides a speech-to-text (STT) interface that can be used as the first stage in a
VoicePipelineAgent
or as a standalone transcription service. For a complete reference of all available parameters, see the plugin reference for
Python
or
Node
.
Usage
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
deepgram
import
stt
deepgram_stt
=
deepgram
.
stt
.
STT
(
model
=
"nova-2-general"
,
interim_results
=
True
,
smart_format
=
True
,
punctuate
=
True
,
filler_words
=
True
,
profanity_filter
=
False
,
keywords
=
[
(
"LiveKit"
,
1.5
)
]
,
language
=
"en-US"
,
)
Copy
Parameters
model
string
Optional
Default:
nova-2-general
#
ID of the model to use for inference. To learn more, see
supported models
.
interim_results
bool
Optional
Default:
true
#
Enable preliminary results before the final transcription is available.
smart_format
bool
Optional
Default:
true
#
Enable smart formatting to improve the readability of transcriptions.
punctuate
bool
Optional
Default:
true
#
Enable punctuation in transcriptions.
filler_words
bool
Optional
Default:
true
#
Enable filler words to improve turn detection.
profanity_filter
bool
Optional
Default:
false
#
Replace recognized profanity with asterisks in transcriptions.
keywords
list[tuple[string, float]]
Optional
Default:
[]
#
A list of keywords and intensifiers to boost or suppress in transcriptions. Positive values boost; negative values suppress.
language
string
Optional
Default:
en
#
Language of input audio in
ISO-639-1
format.
TTS
LiveKit's Deepgram integration also provides a text-to-speech (TTS) interface. This can be used in a
VoicePipelineAgent
or as a standalone speech generator. For a complete reference of all available parameters, see the
plugin reference
.
Usage
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
deepgram
import
tts
deepgram_tts
=
tts
.
TTS
(
model
=
"aura-asteria-en"
,
)
Copy
Parameters
model
string
Optional
Default:
aura-asteria-en
#
ID of the model to use for generation. To learn more, see
supported models
.
On this page
Overview
Quick reference
Environment variables
STT
TTS
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/elevenlabs:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Environment variables
TTS
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
ElevenLabs integration
.
v1.0 for Node.js is coming soon.
Overview
ElevenLabs
provides an AI text-to-speech (TTS) service with thousands of human-like voices
across a number of different languages. With LiveKit's ElevenLabs integration and the Agents framework, you can build
AI voice applications that sound realistic.
Quick reference
Environment variables
.env.local
ELEVEN_API_KEY
=
<
your-elevenlabs-api-key
>
Copy
TTS
LiveKit's ElevenLabs integration provides a text-to-speech (TTS) interface. This can be used in a
VoicePipelineAgent
or as a standalone speech generator. For a complete reference of all available parameters, see the
plugin reference
.
Usage
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
elevenlabs
import
tts
eleven_tts
=
elevenlabs
.
tts
.
TTS
(
model
=
"eleven_turbo_v2_5"
,
voice
=
elevenlabs
.
tts
.
Voice
(
id
=
"EXAVITQu4vr4xnSDxMaL"
,
name
=
"Bella"
,
category
=
"premade"
,
settings
=
elevenlabs
.
tts
.
VoiceSettings
(
stability
=
0.71
,
similarity_boost
=
0.5
,
style
=
0.0
,
use_speaker_boost
=
True
)
,
)
,
language
=
"en"
,
streaming_latency
=
3
,
enable_ssml_parsing
=
False
,
chunk_length_schedule
=
[
80
,
120
,
200
,
260
]
,
)
Copy
Parameters
model
string
Optional
Default:
eleven_turbo_v2_5
#
ID of the model to use for generation. To learn more, see the
ElevenLabs documentation
.
voice
Voice
Optional
Default:
DEFAULT_VOICE
#
Voice configuration. To learn more, see the
ElevenLabs documentation
.
id
string
Required
#
ID of the voice to use for generation. To learn more, see the
ElevenLabs documentation
.
name
string
Required
#
category
string
Required
#
settings
VoiceSettings
Optional
#
See the
ElevenLabs documentation
.
stability
float
Required
#
similarity_boost
float
Required
#
style
float
Optional
#
use_speaker_boost
bool
Optional
#
language
string
Optional
Default:
en
#
Language of output audio in
ISO-639-1
format. To learn more,
see the
ElevenLabs documentation
.
streaming_latency
int
Optional
Default:
3
#
Latency in seconds for streaming.
enable_ssml_parsing
bool
Optional
Default:
false
#
Enable Speech Synthesis Markup Language (SSML) parsing for input text.
chunk_length_schedule
list[int]
Optional
Default:
[80, 120, 200, 260]
#
Schedule for chunk lengths. Valid values range from
50
to
500
.
On this page
Overview
Quick reference
Environment variables
TTS
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/google:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Gemini LLM
google.LLM usage
google.LLM parameters
Google Cloud STT and TTS
google.STT usage
google.STT parameters
google.TTS usage
google.TTS parameters
Gemini Live API
RealtimeModel usage
RealtimeModel parameters
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Google integration
.
v1.0 for Node.js is coming soon.
Try the playground
Chat with a voice assistant built with LiveKit and Google's Gemini Live API
Overview
LiveKit's Google integration provides support for Google Gemini LLM, Google Cloud STT and TTS, and Gemini Live API:
Google plugin support for
Gemini LLM
, and Google Cloud
STT and TTS
.
Support for Google's
Gemini Live API
using the
RealtimeModel
class.
The following sections provide a quick reference for integrating Google AI services with LiveKit. For the complete
reference, see the links provided in each section.
Gemini LLM
LiveKit's Google plugin provides support for Gemini models across both Google AI and Vertex AI platforms. Use LiveKit's
Google integration with the LiveKit Agents framework and create AI agents with advanced reasoning and contextual
understanding.
google.LLM usage
Create a new instance of Gemini LLM to use in a
VoicePipelineAgent
:
agent.py
.env.local
from
livekit
.
plugins
import
google
google_llm
=
google
.
LLM
(
model
=
"gemini-2.0-flash-exp"
,
temperature
=
"0.8"
,
)
Copy
google.LLM parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
Note
Google application credentials must be provided using one of the following options:
For Vertex AI, the
GOOGLE_APPLICATION_CREDENTIALS
environment variable must be set to the path of
the service account key file.
The Google Cloud project and location can be set via
project
and
location
arguments or the environment variables
GOOGLE_CLOUD_PROJECT
and
GOOGLE_CLOUD_LOCATION
. By default, the project is inferred from the service account
key file and the location defaults to "us-central1".
For Google AI, set the
api_key
argument or the
GOOGLE_API_KEY
environment variable.
model
ChatModels | str
Optional
Default:
gemini-2.0-flash-exp
#
ID of the model to use. For a full list, see
Gemini models
.
api_key
str
Optional
Env:
GOOGLE_API_KEY
#
API key for Google Gemini.
vertexai
bool
Optional
Default:
false
#
True to use
Vertex AI
; false to use
Google AI
.
project
str
Optional
#
Google Cloud project to use (only if using Vertex AI).
temperature
float
Optional
Default:
0.8
#
The temperature controls the degree of randomness in token selection. A lower temperature results in more deterministic
output.
To learn more, see
Model parameters
.
max_output_tokens
int
Optional
#
Maximum number of tokens that can be generated in the response.
To learn more, see
Model parameters
.
Google Cloud STT and TTS
LiveKit's Google integration includes a
Google plugin
with STT and TTS support.
Google Cloud STT
supports over 125 languages and can use
chirp
, a foundational model with improved recognition and transcription for spoken languages and accents.
Google Cloud TTS
provides a wide voice selection and generates speech with humanlike intonation.
Instances of Google STT and TTS can be used as part of the pipeline for an agent created using the
VoicePipelineAgent
class or as part of a standalone transcription service.
Note
LiveKit's Google plugin is currently only available in Python.
google.STT usage
Use the
google.STT
method to create an instance of an STT:
main.py
.env.local
from
livekit
.
plugins
import
google
google_stt
=
google
.
STT
(
model
=
"chirp"
,
spoken_punctuation
=
True
,
)
Copy
google.STT parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
Note
Google Cloud credentials must be provided by one of the following methods:
Passed in the
credentials_info
dictionary.
Saved in the
credentials_file
JSON file (
GOOGLE_APPLICATION_CREDENTIALS
environment variable).
Application Default Credentials. To learn more,
see
How Application Default Credentials works
languages
LanguageCode
Optional
Default:
en-US
#
Specify input languages. For a full list of supported languages,
see
Speech-to-text supported languages
.
spoken_punctuation
boolean
Optional
Default:
True
#
Replace spoken punctuation with punctuation characters in text.
model
SpeechModels | string
Optional
Default:
long
#
Model to use for speech-to-text. To learn more, see
Select a transcription model
.
credentials_info
array
Optional
#
Key-value pairs of authentication credential information.
credentials_file
string
Optional
#
Name of the JSON file that contains authentication credentials for Google Cloud.
google.TTS usage
Use the
google.TTS
method to create an instance of a TTS:
main.py
.env.local
from
livekit
.
plugins
import
google
google_stt
=
google
.
TTS
(
gender
=
"female"
,
voice_name
=
"en-US-Standard-H"
,
)
Copy
google.TTS parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
Note
Google Cloud credentials must be provided by one of the following methods:
Passed in the
credentials_info
dictionary.
Saved in the
credentials_file
JSON file (
GOOGLE_APPLICATION_CREDENTIALS
environment variable).
Application Default Credentials. To learn more,
see
How Application Default Credentials works
language
SpeechLanguages | string
Optional
Default:
en-US
#
Specify output language. For a full list of languages,
see
Supported voices and languages
.
gender
Gender | string
Optional
Default:
neutral
#
Voice gender. Valid values are
male
,
female
, and
neutral
.
voice_name
string
Optional
#
Name of the voice to use for speech. For a full list of voices,
see
Supported voices and languages
.
credentials_info
array
Optional
#
Key-value pairs of authentication credential information.
credentials_file
string
Optional
#
Name of the JSON file that contains authentication credentials for Google Cloud.
Gemini Live API
LiveKit's Google plugin includes a
RealtimeModel
class that allows you to use Google's
Gemini Live API
.
The Gemini Live API enables low-latency, two-way interactions that use text, audio, and video input,
with audio and text output. Use LiveKit's Google integration with the Agents framework to create agents with
natural, human-like voice conversations.
RealtimeModel usage
Create a model using the Gemini Live API for use in a
MultimodalAgent
:
from
livekit
.
plugins
import
google
model
=
google
.
beta
.
realtime
.
RealtimeModel
(
voice
=
"Puck"
,
temperature
=
0.8
,
instructions
=
"You are a helpful assistant"
,
)
,
Copy
For a full agent example, see the
Gemini example
in the LiveKit Agents GitHub repository.
RealtimeModel parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
instructions
string
Optional
#
System instructions to better control the model's output and specify tone and sentiment of responses. To learn more,
see
System instructions
.
model
LiveAPIModels | string
Required
Default:
gemini-2.0-flash-exp
#
Live API model to use.
api_key
string
Required
Env:
GOOGLE_API_KEY
#
Google Gemini API key.
voice
Voice | string
Required
Default:
Puck
#
Name of the Gemini Live API voice. For a full list, see
Voices
.
modalities
list[Modality]
Optional
Default:
["AUDIO"]
#
List of modalities to use, such as ["TEXT", "AUDIO"].
vertexai
boolean
Required
Default:
False
#
If set to true, use Vertex AI.
project
string
Optional
Env:
GOOGLE_CLOUD_PROJECT
#
Google Cloud project ID to use for the API (if
vertextai=True
). By default, the project is inferred from the service
account key file (set using the
GOOGLE_APPLICATION_CREDENTIALS
environment variable).
location
string
Optional
Env:
GOOGLE_CLOUD_LOCATION
#
Google Cloud location to use for the API (if
vertextai=True
). By default, the project is inferred from the service
account key file and the location defaults to
us-central1
.
temperature
float
Optional
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more,
see
Temperature
.
On this page
Overview
Gemini LLM
google.LLM usage
google.LLM parameters
Google Cloud STT and TTS
google.STT usage
google.STT parameters
google.TTS usage
google.TTS parameters
Gemini Live API
RealtimeModel usage
RealtimeModel parameters
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/groq:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Prerequisites
Instructions
Setup a LiveKit account and install the CLI
Bootstrap an agent from template
Create a minimal frontend with Next.js
Launch your app and talk to your agent
Quick reference
Environment variables
STT
LLM
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Groq integration
.
v1.0 for Node.js is coming soon.
Try out Groq STT live
See Groq's STT in action with our realtime transcription playground
Overview
Groq
provides low-latency AI inference with deterministic results and automatic speech recognition. LiveKit's Groq integration provides both STT and LLM functionality through an OpenAI-compatible interface. Use Groq and the Agents framework to build AI voice assistants that are realistic and predictable with accurate transcriptions.
This guide walks you through the steps to build a live transcription application that uses LiveKit's
Agents Framework
and the Groq STT service. For a demonstration of the following application, see the
LiveKit and Groq transcription app
.
Note
If you're looking to build an AI voice assistant with Groq, check out our
Voice Agent Quickstart
guide and use the Groq integration as your STT and/or LLM provider.
Prerequisites
Groq API Key
Python 3.9-3.12
or
Node 20.17.0
or higher
Instructions
Setup a LiveKit account and install the CLI
Create an account or sign in to your
LiveKit Cloud account
.
Install the LiveKit CLI
and authenticate using
lk cloud auth
— (
Optional
).
Tip
The LiveKit CLI utility
lk
is a convenient way to setup and configure applications and manage your LiveKit services,
but installing it isn't required.
Bootstrap an agent from template
Clone a starter template for your preferred language using the CLI:
Terminal
Python
Node.js
lk app create
\
--template-url https://github.com/livekit-examples/transcription-groq-python
Copy
If you aren't using the LiveKit CLI, clone the repository yourself:
Terminal
Python
Node.js
git
clone https://github.com/livekit-examples/transcription-groq-python
Copy
Enter your
Groq API Key
when prompted or manually add your environment variables:
GROQ_API_KEY
=
<
your-groq-api-key
>
LIVEKIT_API_KEY
=
<
your-livekit-api-key
>
LIVEKIT_API_SECRET
=
<
your-livekit-api-secret
>
LIVEKIT_URL
=
<
your-livekit-url
>
Copy
Install dependencies and start your agent:
Terminal
Python
Node.js
cd
<
agent_dir
>
python3
-m
venv venv
source
venv/bin/activate
python3
-m
pip
install
-r
requirements.txt
python3 main.py dev
Copy
Note
For more details on using the STT module to perform transcription outside of the context of a voice pipeline or multimodal agent, see the
transcriptions
documentation.
Create a minimal frontend with Next.js
Clone the
Transcription Frontend
Next.js app starter template using the CLI:
lk app create
--template
transcription-frontend
Copy
If you aren't using the LiveKit CLI, clone the repository yourself:
git
clone https://github.com/livekit-examples/transcription-frontend
Copy
Enter your environment variables:
LIVEKIT_API_KEY
=
<
your-livekit-api-key
>
LIVEKIT_API_SECRET
=
<
your-livekit-api-secret
>
NEXT_PUBLIC_LIVEKIT_URL
=
<
your-livekit-url
>
Copy
Install dependencies and start your frontend application:
cd
<
frontend_dir
>
pnpm
install
pnpm
dev
Copy
Launch your app and talk to your agent
Visit your locally-running application (by default,
http://localhost:3000
).
Select
Start voice transcription
and begin speaking.
Quick reference
Environment variables
.env.local
GROQ_API_KEY
=
<
your-groq-api-key
>
Copy
STT
LiveKit's Groq integration provides an
OpenAI compatible
speech-to-text (STT) interface. This can be used as the first stage in a
VoicePipelineAgent
or as a standalone transcription service as documented above. For a complete reference of all available parameters, see the
plugin reference
.
Usage
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
stt
groq_stt
=
stt
.
STT
.
with_groq
(
model
=
"whisper-large-v3-turbo"
,
language
=
"en"
,
)
Copy
Parameters
model
string
Optional
Default:
whisper-large-v3-turbo
#
ID of the model to use for inference. See
supported models
.
language
string
Optional
Default:
en
#
Language of input audio in
ISO-639-1
format.
detect_language
bool
Optional
Default:
false
#
Whether or not language should be detected from the audio stream. Not every
model
supports language detection. See
supported models
.
LLM
LiveKit's Groq integration also provides an
OpenAI compatible
LLM interface. This can be used in a
VoicePipelineAgent
. For a complete reference of all available parameters, see the
plugin reference
.
Usage
main.py
.env.local
Python
Node.js
from
livekit
.
plugins
.
openai
import
llm
groq_llm
=
llm
.
LLM
.
with_groq
(
model
=
"llama3-8b-8192"
,
temperature
=
0.8
,
)
Copy
Parameters
model
string
Optional
Default:
llama3-8b-8192
#
ID of the model to use for inference. For a complete list, see
supported models
.
temperature
float
Optional
Default:
1.0
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more, see
chat completions
.
On this page
Overview
Prerequisites
Instructions
Setup a LiveKit account and install the CLI
Bootstrap an agent from template
Create a minimal frontend with Next.js
Launch your app and talk to your agent
Quick reference
Environment variables
STT
LLM
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/playai:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
TTS
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
PlayHT integration guide
.
v1.0 for Node.js is coming soon.
Overview
PlayHT
provides realistic TTS voice generation. With LiveKit's PlayHT integration and the Agents
framework, you can build AI voice applications with fluent and conversational voices.
Note
If you're looking to build an AI voice assistant with PlayHT, check out our
Voice Agent Quickstart
guide and use the PlayHT TTS module as demonstrated below.
Quick reference
The following sections provide a quick reference for integrating PlayHT TTS with LiveKit.
For a complete reference of all available parameters, see the
plugin reference
.
TTS
LiveKit's PlayHT integration provides a text-to-speech (TTS) interface. This can be used in a
VoicePipelineAgent
or as a standalone speech generator.
Usage
Use the
playai.tts.TTS
class to create an instance of a PlayHT TTS:
main.py
.env.local
from
livekit
.
plugins
.
playai
import
tts
playht_tts
=
tts
.
TTS
(
voice
=
"s3://voice-cloning-zero-shot/a59cb96d-bba8-4e24-81f2-e60b888a0275/charlottenarrativesaad/manifest.json"
,
language
=
"SPANISH"
,
)
Copy
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters,
see the
plugin reference
.
api_key
string
Required
Env:
PLAYHT_API_KEY
#
PlayAI API key. Generate a user ID and API key in the
PlayHT dashboard
.
user_id
string
Required
Env:
PLAYHT_USER_ID
#
PlayAI user ID. Generate a user ID and API key in the
PlayHT dashboard
.
voice
string
Required
Default:
s3://voice-cloning-zero-shot/d9ff78ba-d016-47f6-b0ef-dd630f59414e/female-cs/manifest.json
#
URL of the voice manifest file. For a full list, see
List of pre-built voices
.
model
TTSModel | string
Required
Default:
Play3.0-mini
#
Name of the TTS model. For a full list, see
Models
.
language
string
Required
Default:
ENGLISH
#
Language of the text to be spoken. For language support by model, see
Models
.
On this page
Overview
Quick reference
TTS
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/rime:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
TTS
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Rime integration guide
.
v1.0 for Node.js is coming soon.
Overview
Rime
provides text-to-speech synthesis (TTS) optimized for speed and quality. With LiveKit's Rime integration and the Agents framework, you can build AI voice applications that are responsive and sound realistic.
Note
If you're looking to build an AI voice assistant with Rime, check out our
Voice Agent quickstart
guide and use the Rime TTS module as demonstrated below.
Quick reference
The following sections provide a quick reference for integrating Rime TTS with LiveKit.
For the complete reference, see the links provided in each section.
TTS
LiveKit's Rime integration provides a text-to-speech (TTS) interface. This can be used in a
VoicePipelineAgent
or as a standalone speech generator.
Usage
Create an instance of Rime TTS:
agent.py
.env.local
from
livekit
.
plugins
.
rime
import
TTS
rime_tts
=
TTS
(
model
=
"mist"
,
speaker
=
"rainforest"
,
speed_alpha
=
0.9
,
reduce_latency
=
True
,
)
Copy
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters,
see the
plugin reference
.
model
string
Optional
Default:
mist
#
ID of the model to use. To learn more, see
Models
.
speaker
string
Optional
Default:
lagoon
#
ID of the voice to use for speech generation. To learn more, see
Voices
.
audio_format
TTSEncoding
Optional
Default:
pcm
#
Audio format to use. Valid values are:
pcm
and
mp3
.
sample_rate
integer
Optional
Default:
16000
#
Sample rate of the generated audio. Set this rate to best match your application needs.
To learn more, see
Recommendations for reducing response time
.
speed_alpha
float
Optional
Default:
1.0
#
Adjusts the speed of speech. Lower than
1.0
results in faster speech; higher than
1.0
results in slower speech.
reduce_latency
boolean
Optional
Default:
false
#
When set to
true
, turns off text normalization to reduce the amount of time spent preparing input text for TTS inference. This
might result in the mispronunciation of digits and abbreviations.
To learn more, see
Recommendations for reducing response time
.
phonemize_between_brackets
boolean
Optional
Default:
false
#
When set to
true
, allows the use of custom pronunciation strings in text. To learn more, see
Custom pronunciation
.
api_key
string
Optional
Env:
RIME_API_KEY
#
Rime API Key.
On this page
Overview
Quick reference
TTS
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/speechmatics:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Environment variables
STT
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Speechmatics integration guide
.
v1.0 for Node.js is coming soon.
Overview
Speechmatics provides AI speech recognition technology. Their advanced speech models deliver highly accurate transcriptions across diverse languages, dialects, and accents. With LiveKit’s Speechmatics integration and the Agents framework, you can build voice AI agents that provide reliable, real-time transcriptions.
Note
If you're looking to build an AI voice assistant with Speechmatics, check out our
Voice Agent Quickstart
guide and use the Speechmatics STT module as demonstrated below.
Quick reference
Environment variables
.env.local
SPEECHMATICS_API_KEY
=
<
your-speechmatics-api-key
>
Copy
STT
LiveKit's Speechmatics integration provides a speech-to-text (STT) interface that can be used as the first stage in a
VoicePipelineAgent
or as a standalone transcription service. For a complete reference of all available parameters, see the plugin reference for
Python
.
Note
The Speechmatics STT plugin is currently only supported for Python.
Usage
main.py
.env.local
from
livekit
.
plugins
import
speechmatics
from
livekit
.
plugins
.
speechmatics
.
types
import
TranscriptionConfig
,
AudioSettings
speechmatics_stt
=
speechmatics
.
STT
(
transcription_config
=
TranscriptionConfig
(
operating_point
=
"enhanced"
,
enable_partials
=
True
,
language
=
"en"
,
output_locale
=
"en-US"
,
diarization
=
"speaker"
,
enable_entities
=
True
,
additional_vocab
=
[
{
"content"
:
"financial crisis"
}
,
{
"content"
:
"gnocchi"
,
"sounds_like"
:
[
"nyohki"
,
"nokey"
,
"nochi"
]
}
,
{
"content"
:
"CEO"
,
"sounds_like"
:
[
"C.E.O."
]
}
]
,
max_delay
=
0.7
,
max_delay_mode
=
"flexible"
)
,
audio_settings
=
AudioSettings
(
encoding
=
"pcm_s16le"
,
sample_rate
=
16000
,
)
,
)
Copy
Parameters
operating_point
string
Optional
Default:
enhanced
#
Operating point to use for the transcription per required accuracy & complexity. To learn more, see
Accuracy Reference
.
enable_partials
bool
Optional
Default:
True
#
Partial transcripts allow you to receive preliminary transcriptions and update as more context is available until the higher-accuracy
final transcript
is returned. Partials are returned faster but without any post-processing such as formatting.
language
string
Optional
Default:
en
#
ISO 639-1 language code. All languages are global and can understand different dialects/accents. To see the list of all supported languages, see
Supported Languages
.
output_locale
string
Optional
Default:
en-US
#
RFC-5646 language code for transcription output. For supported locales, see
Output Locale
.
diarization
string
Optional
Default:
NULL
#
Setting this to
speaker
enables accurate labeling of different speakers detected with the attributed transcribed output e.g. S1, S2. For more information, visit
Speaker Diarization
.
additional_vocab
list[dict{“content”:str, ”sounds_like”:str}]
Optional
Default:
NULL
#
Add custom words for each transcription job. To learn more, see
Custom Dictionary
.
enable_entities
bool
Optional
Default:
False
#
Allows the written form of various entities such as phone numbers, emails, currency, etc to be output in the transcript. To learn more about the supported entities, see
Entities
.
max_delay
number
Optional
Default:
0.7
#
The delay in seconds between the end of a spoken word and returning the final transcript results.
max_delay_mode
string
Optional
Default:
flexible
#
If set to
flexible
, the final transcript is delayed until proper numeral formatting is complete. To learn more, see
Numeral Formatting
.
On this page
Overview
Quick reference
Environment variables
STT
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/turns/turn-detector:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Requirements
Installation
Download model weights
Usage
Parameters
Supported languages
Realtime model usage
Benchmarks
Runtime performance
Detection accuracy
Additional resources
Overview
The LiveKit turn detector plugin is a custom, open-weights language model that adds conversational context as an additional signal to voice activity detection (VAD) to improve end of turn detection in voice AI apps.
Traditional VAD models are effective at determining the presence or absence of speech, but without language understanding they can provide a poor user experience. For instance, a user might say "I need to think about that for a moment" and then take a long pause. The user has more to say but a VAD-only system interrupts them anyways. A context-aware model can predict that they have more to say and wait for them to finish before responding.
The LiveKit turn detector plugin is free to use with the Agents SDK with both English-only and multilingual models.
Turn detector demo
A video showcasing the improvements provided by the LiveKit turn detector.
Quick reference
The following sections provide a quick overview of the turn detector plugin. For more information, see
Additional resources
.
Requirements
The LiveKit turn detector is designed for use inside an
AgentSession
and also requires an
STT plugin
be provided. If you're using a realtime LLM you must include a separate STT plugin to use the LiveKit turn detector plugin.
LiveKit recommends also using the
Silero VAD plugin
for maximum performance, but you can rely on your STT plugin's endpointing instead if you prefer.
The model runs locally on the CPU and requires <500 MB of RAM even with multiple concurrent jobs with a shared inference server.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[turn-detector]~=1.0"
Copy
Download model weights
You must download the model weights before running your agent for the first time:
python main.py download-files
Copy
Usage
Initialize your
AgentSession
with the turn detector and initialize your STT plugin with matching language settings. These examples use the Deepgram STT plugin, but more than 10 other STT plugins
are available
.
English-only model
Use the
EnglishModel
and ensure your STT plugin configuration matches:
from
livekit
.
plugins
.
turn_detector
.
english
import
EnglishModel
from
livekit
.
plugins
import
deepgram
session
=
AgentSession
(
turn_detection
=
EnglishModel
(
)
,
stt
=
deepgram
.
STT
(
model
=
"nova-3"
,
language
=
"en"
)
,
# ... vad, stt, tts, llm, etc.
)
Copy
Multilingual model
Use the
MultilingualModel
and ensure your STT plugin configuration matches. In this example, Deepgram performs automatic language detection and passes that value to the turn detector.
from
livekit
.
plugins
.
turn_detector
.
multilingual
import
MultilingualModel
from
livekit
.
plugins
import
deepgram
session
=
AgentSession
(
turn_detection
=
MultilingualModel
(
)
,
stt
=
deepgram
.
STT
(
model
=
"nova-3"
,
language
=
"multi"
)
,
# ... vad, stt, tts, llm, etc.
)
Copy
Parameters
The turn detector itself has no configuration, but the
AgentSession
that uses it supports the following related parameters:
min_endpointing_delay
float
Optional
Default:
0.5
#
The number of seconds to wait before considering the turn complete. The session uses this delay when no turn detector model is present, or when the model indicates a likely turn boundary.
max_endpointing_delay
float
Optional
Default:
6.0
#
The maximum time to wait for the user to speak after the turn detector model indicates the user is likely to continue speaking. This parameter has no effect without the turn detector model.
Supported languages
The
MultilingualModel
supports English and 13 other languages. The model relies on your
STT plugin
to report the language of the user's speech. To set the language to a fixed value, configure the STT plugin with a specific language. For example, to force the model to use Spanish:
session
=
AgentSession
(
turn_detection
=
MultilingualModel
(
)
,
stt
=
deepgram
.
STT
(
model
=
"nova-2"
,
language
=
"es"
)
,
# ... vad, stt, tts, llm, etc.
)
Copy
The model currently supports English, Spanish, French, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Indonesian, Turkish, and Russian.
Realtime model usage
Realtime models like the OpenAI Realtime API produce user transcripts after the end of the turn, rather than incrementally while the user speaks. The turn detector model requires live STT results to operate, so you must provide an STT plugin to the
AgentSession
to use it with a realtime model. This incurs extra cost for the STT model.
Benchmarks
The following data shows the expected performance of the turn detector model.
Runtime performance
The size on disk and typical CPU inference time for the turn detector models is as follows:
Model
Base Model
Size on Disk
Per Turn Latency
English-only
SmolLM2-135M
66 MB
~15-45 ms
Multilingual
Qwen2.5-0.5B
281 MB
~50-160 ms
Detection accuracy
The following tables show accuracy metrics for the turn detector models in each supported language.
True positive
means the model correctly identifies the user has finished speaking.
True negative
means the model correctly identifies the user will continue speaking.
English-only model
Accuracy metrics for the English-only model:
Language
True Positive Rate
True Negative Rate
English
98.8%
87.5%
Multilingual model
Accuracy metrics for the multilingual model, when configured with the correct language:
Language
True Positive Rate
True Negative Rate
French
98.8%
97.3%
Indonesian
98.8%
97.3%
Russian
98.8%
97.3%
Turkish
98.8%
97.2%
Dutch
98.8%
97.1%
Portuguese
98.8%
97.1%
Spanish
98.8%
96.7%
German
98.8%
96.6%
Italian
98.8%
96.5%
Korean
98.8%
89.7%
English
98.8%
89.5%
Japanese
98.8%
83.6%
Chinese
98.8%
75.7%
Additional resources
The following resources provide more information about using the LiveKit turn detector plugin.
Python package
The
livekit-plugins-turn-detector
package on PyPI.
Plugin reference
Reference for the LiveKit turn detector plugin.
GitHub repo
View the source or contribute to the LiveKit turn detector plugin.
LiveKit Model License
LiveKit Model License used for the turn detector model.
On this page
Overview
Quick reference
Requirements
Installation
Download model weights
Usage
Parameters
Supported languages
Realtime model usage
Benchmarks
Runtime performance
Detection accuracy
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/build/turns/vad:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Requirements
Installation
Download model weights
Usage
Prewarm
Configuration
Additional resources
Overview
The Silero VAD plugin provides voice activity detection (VAD) that contributes to accurate
turn detection
in voice AI applications.
VAD is a crucial component for voice AI applications as it helps determine when a user is speaking versus when they are silent. This enables natural turn-taking in conversations and helps optimize resource usage by only performing speech-to-text while the user speaks.
LiveKit recommends using the Silero VAD plugin in combination with the custom
turn detector model
for the best performance.
Quick reference
The following sections provide a quick overview of the Silero VAD plugin. For more information, see
Additional resources
.
Requirements
The model runs locally on the CPU and requires minimal system resources.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[silero]~=1.0"
Copy
Download model weights
You must download the model weights before running your agent for the first time:
python main.py download-files
Copy
Usage
Initialize your
AgentSession
with the Silero VAD plugin:
from
livekit
.
plugins
import
silero
session
=
AgentSession
(
vad
=
silero
.
VAD
.
load
(
)
,
# ... stt, tts, llm, etc.
)
Copy
Prewarm
You can
prewarm
the plugin to improve load times for new jobs:
async
def
entrypoint
(
ctx
:
agents
.
JobContext
)
:
await
ctx
.
connect
(
)
session
=
AgentSession
(
vad
=
ctx
.
proc
.
userdata
[
"vad"
]
,
# ... stt, tts, llm, etc.
)
# ... session.start etc ...
def
prewarm
(
proc
:
agents
.
JobProcess
)
:
proc
.
userdata
[
"vad"
]
=
silero
.
VAD
.
load
(
)
if
__name__
==
"__main__"
:
agents
.
cli
.
run_app
(
agents
.
WorkerOptions
(
entrypoint_fnc
=
entrypoint
,
prewarm_fnc
=
prewarm
)
)
Copy
Configuration
The following parameters are available on the
load
method:
min_speech_duration
float
Optional
Default:
0.05
#
Minimum duration of speech required to start a new speech chunk.
min_silence_duration
float
Optional
Default:
0.55
#
Duration of silence to wait after speech ends to determine if the user has finished speaking.
prefix_padding_duration
float
Optional
Default:
0.5
#
Duration of padding to add to the beginning of each speech chunk.
max_buffered_speech
float
Optional
Default:
60.0
#
Maximum duration of speech to keep in the buffer (in seconds).
activation_threshold
float
Optional
Default:
0.5
#
Threshold to consider a frame as speech. A higher threshold results in more conservative detection but might potentially miss soft speech. A lower threshold results in more sensitive detection, but might identify noise as speech.
sample_rate
Literal[8000, 16000]
Optional
Default:
16000
#
Sample rate for the inference (only 8KHz and 16KHz are supported).
force_cpu
bool
Optional
Default:
True
#
Force the use of CPU for inference.
Additional resources
The following resources provide more information about using the LiveKit Silero VAD plugin.
Python package
The
livekit-plugins-silero
package on PyPI.
Plugin reference
Reference for the LiveKit Silero VAD plugin.
GitHub repo
View the source or contribute to the LiveKit Silero VAD plugin.
Silero VAD project
The open source VAD model that powers the LiveKit Silero VAD plugin.
Transcriber
An example using standalone VAD and STT outside of an
AgentSession
.
On this page
Overview
Quick reference
Requirements
Installation
Download model weights
Usage
Prewarm
Configuration
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/realtime/openai:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Server VAD
Semantic VAD
Additional resources
OpenAI Playground
Experiment with OpenAI's Realtime API in the playground with personalities like
the
Snarky Teenager
or
Opera Singer
.
Overview
OpenAI's Realtime API enables low-latency, multimodal interactions with realtime audio and text processing. Use
LiveKit's OpenAI plugin to create an agent that uses the Realtime API.
Note
Using Azure OpenAI? See our
Azure OpenAI Realtime API guide
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the OpenAI plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Authentication
The OpenAI plugin requires an
OpenAI API key
.
Set
OPENAI_API_KEY
in your
.env
file.
Usage
Use the OpenAI Realtime API within an
AgentSession
. For example,
you can use it in the
Voice AI quickstart
.
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
realtime
.
RealtimeModel
(
)
,
)
Copy
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
model
str
Optional
Default:
'gpt-4o-realtime-preview'
#
ID of the Realtime model to use. For a list of available models, see the
Models
.
voice
str
Optional
Default:
'alloy'
#
Voice to use for speech generation. For a list of available voices, see
Voice options
.
temperature
float
Optional
Default:
0.8
#
Valid values are between
0.6
and
1.2
. To learn more, see
temperature
.
turn_detection
TurnDetection | None
Optional
#
Configuration for turn detection, see the section on
Turn detection
for more information.
Turn detection
OpenAI's Realtime API includes
voice activity detection (VAD)
to automatically detect when a user has started or stopped speaking. This feature is enabled by default.
There are two modes for VAD:
Server VAD
(default): Uses periods of silence to automatically chunk the audio.
Semantic VAD
: Uses a semantic classifier to detect when the user has finished speaking based on their words.
Server VAD
Server VAD is the default mode and can be configured with the following properties:
from
livekit
.
plugins
.
openai
import
realtime
from
openai
.
types
.
beta
.
realtime
.
session
import
TurnDetection
session
=
AgentSession
(
llm
=
realtime
.
RealtimeModel
(
turn_detection
=
TurnDetection
(
type
=
"server_vad"
,
threshold
=
0.5
,
prefix_padding_ms
=
300
,
silence_duration_ms
=
500
,
create_response
=
True
,
interrupt_response
=
True
,
)
)
,
)
Copy
threshold
: Higher values require louder audio to activate, better for noisy environments.
prefix_padding_ms
: Amount of audio to include before detected speech.
silence_duration_ms
: Duration of silence to detect speech stop (shorter = faster turn detection).
Semantic VAD
Semantic VAD uses a classifier to determine when the user is done speaking based on their words. This mode is less likely to interrupt users mid-sentence or chunk transcripts prematurely.
from
livekit
.
plugins
.
openai
import
realtime
from
openai
.
types
.
beta
.
realtime
.
session
import
TurnDetection
session
=
AgentSession
(
llm
=
realtime
.
RealtimeModel
(
turn_detection
=
TurnDetection
(
type
=
"semantic_vad"
,
eagerness
=
"auto"
,
create_response
=
True
,
interrupt_response
=
True
,
)
)
,
)
Copy
The
eagerness
property controls how quickly the model responds:
auto
(default) - Equivalent to
medium
.
low
- Lets users take their time speaking.
high
- Chunks audio as soon as possible.
medium
- Balanced approach.
For more information about turn detection in general, see the
Turn detection guide
.
Additional resources
The following resources provide more information about using OpenAI with LiveKit Agents.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the OpenAI Realtime API plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Voice AI quickstart
Build a simple realtime model voice assistant using the OpenAI Realtime API in less than 10 minutes.
OpenAI docs
OpenAI Realtime API documentation.
OpenAI ecosystem overview
Overview of the entire OpenAI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Server VAD
Semantic VAD
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/openai:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
OpenAI
provides powerful language models like
gpt-4o
and
o1
. With LiveKit's OpenAI integration and the Agents framework, you can build sophisticated voice AI applications using their industry-leading models.
Using Azure OpenAI?
See our
Azure OpenAI LLM guide
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Authentication
The OpenAI plugin requires an
OpenAI API key
.
Set
OPENAI_API_KEY
in your
.env
file.
Usage
Use OpenAI within an
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
(
model
=
"gpt-4o-mini"
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
gpt-4o-mini
#
The model to use for the LLM. For more information, see the
OpenAI documentation
.
temperature
float
Optional
Default:
0.8
#
A measure of randomness in output. A lower value results in more predictable output, while a higher value results in
more creative output.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Specifies whether to use tools during response generation.
Additional resources
The following resources provide more information about using OpenAI with LiveKit Agents.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
OpenAI docs
OpenAI platform documentation.
Voice AI quickstart
Get started with LiveKit Agents and OpenAI.
OpenAI ecosystem overview
Overview of the entire OpenAI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/tts/openai:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Try LiveKit.fm
Chat with OpenAI's latest `gpt-4o-mini-tts` model in a LiveKit demo inspired by OpenAI.fm
Overview
OpenAI TTS
provides lifelike spoken audio through their latest model
gpt-4o-mini-tts
model or their well-tested
tts-1
and
tts-1-hd
models. With LiveKit's OpenAI TTS integration and the Agents framework, you can build voice AI applications that sound realistic and natural.
To learn more about TTS and generating agent speech, see
Agent speech
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Authentication
The OpenAI plugin requires an
OpenAI API key
.
Set
OPENAI_API_KEY
in your
.env
file.
Usage
Use OpenAI TTS in an
AgentSession
or as a standalone speech generator. For example, you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
tts
=
openai
.
TTS
(
model
=
"gpt-4o-mini-tts"
,
voice
=
"ash"
,
instructions
=
"Speak in a friendly and conversational tone."
,
)
,
# ... llm, stt, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
TTSModels | string
Optional
Default:
gpt-4o-mini-tts
#
ID of the model to use for speech generation. To learn more, see
TTS models
.
voice
TTSVoice | string
Optional
Default:
ash
#
ID of the voice used for speech generation. To learn more, see
TTS voice options
.
instructions
string
Optional
#
Instructions to control tone, style, and other characteristics of the speech. Does not work with
tts-1
or
tts-1-hd
models.
Additional resources
The following resources provide more information about using OpenAI with LiveKit Agents.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the OpenAI TTS plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI TTS plugin.
OpenAI docs
OpenAI TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and OpenAI TTS.
OpenAI ecosystem guide
Overview of the entire OpenAI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/stt/openai:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
OpenAI
provides STT support via the latest
gpt-4o-transcribe
model as well as
whisper-1
. You can use the open source OpenAI plugin for LiveKit agents to build voice AI applications with fast, accurate transcription.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Authentication
The OpenAI plugin requires an
OpenAI API key
.
Set
OPENAI_API_KEY
in your
.env
file.
Usage
Use OpenAI STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
stt
=
openai
.
STT
(
model
=
"gpt-4o-transcribe"
,
)
,
# ... llm, tts, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
WhisperModels | string
Optional
Default:
gpt-4o-transcribe
#
Model to use for transcription. See OpenAI's documentation for a list of
supported models
.
language
string
Optional
Default:
en
#
Language of input audio in
ISO-639-1
format. See OpenAI's documentation for a list of
supported languages
.
Additional resources
The following resources provide more information about using OpenAI with LiveKit Agents.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the OpenAI STT plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI STT plugin.
OpenAI docs
OpenAI STT docs.
Voice AI quickstart
Get started with LiveKit Agents and OpenAI STT.
OpenAI ecosystem guide
Overview of the entire OpenAI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/realtime/gemini:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Additional resources
Try the playground
Chat with a voice assistant built with LiveKit and the Gemini Live API
Overview
Google's
Gemini Live API
enables low-latency, two-way interactions that use text, audio, and video input, with audio and text output. LiveKit's Google plugin includes a
RealtimeModel
class that allows you to use this API to create agents with natural, human-like voice conversations.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the Google plugin from PyPI:
pip
install
"livekit-agents[google]~=1.0"
Copy
Authentication
The Google plugin requires authentication based on your chosen service:
For Vertex AI, you must set the
GOOGLE_APPLICATION_CREDENTIALS
environment variable to the path of the service account key file.
For Google Gemini API, set the
GOOGLE_API_KEY
environment variable.
Usage
Use the Gemini Live API within an
AgentSession
. For example,
you can use it in the
Voice AI quickstart
.
from
livekit
.
plugins
import
google
model
=
google
.
beta
.
realtime
.
RealtimeModel
(
model
=
"gemini-2.0-flash-exp"
,
voice
=
"Puck"
,
temperature
=
0.8
,
instructions
=
"You are a helpful assistant"
,
)
,
Copy
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
instructions
string
Optional
#
System instructions to better control the model's output and specify tone and sentiment of responses. To learn more,
see
System instructions
.
model
LiveAPIModels | string
Required
Default:
gemini-2.0-flash-exp
#
Live API model to use.
api_key
string
Required
Env:
GOOGLE_API_KEY
#
Google Gemini API key.
voice
Voice | string
Required
Default:
Puck
#
Name of the Gemini Live API voice. For a full list, see
Voices
.
modalities
list[Modality]
Optional
Default:
["AUDIO"]
#
List of response modalities to use, such as
["TEXT", "AUDIO"]
.
vertexai
boolean
Required
Default:
false
#
If set to true, use Vertex AI.
project
string
Optional
Env:
GOOGLE_CLOUD_PROJECT
#
Google Cloud project ID to use for the API (if
vertextai=True
). By default, it uses the project in the service
account key file (set using the
GOOGLE_APPLICATION_CREDENTIALS
environment variable).
location
string
Optional
Env:
GOOGLE_CLOUD_LOCATION
#
Google Cloud location to use for the API (if
vertextai=True
). By default, it uses the location from the service
account key file or
us-central1
.
temperature
float
Optional
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more,
see
Temperature
.
Turn detection
The Gemini Live API includes built-in VAD-based turn detection, which is currently the only supported turn detection method.
Additional resources
The following resources provide more information about using Gemini with LiveKit Agents.
Python package
The
livekit-plugins-google
package on PyPI.
Plugin reference
Reference for the Gemini Live API plugin.
GitHub repo
View the source or contribute to the LiveKit Google plugin.
Gemini docs
Gemini Live API documentation.
Voice AI quickstart
Get started with LiveKit Agents and Gemini Live API.
Google AI ecosystem guide
Overview of the entire Google AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/gemini:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
Google Gemini
provides powerful language models with advanced reasoning and multimodal capabilities. With LiveKit's Google plugin and the Agents framework, you can build sophisticated voice AI applications using Vertex AI or Google Gemini API.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[google]~=1.0"
Copy
Authentication
The Google plugin requires authentication based on your chosen service:
For Vertex AI, you must set the
GOOGLE_APPLICATION_CREDENTIALS
environment variable to the path of the service account key file.
For Google Gemini API, set the
GOOGLE_API_KEY
environment variable.
Usage
Use Gemini within an
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
google
session
=
AgentSession
(
llm
=
google
.
LLM
(
model
=
"gemini-2.0-flash-exp"
,
temperature
=
0.8
,
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
model
ChatModels | str
Optional
Default:
gemini-2.0-flash-001
#
ID of the model to use. For a full list, see
Gemini models
.
api_key
str
Optional
Env:
GOOGLE_API_KEY
#
API key for Google Gemini API.
vertexai
bool
Optional
Default:
false
#
True to use
Vertex AI
; false to use
Google AI
.
project
str
Optional
Env:
GOOGLE_CLOUD_PROJECT
#
Google Cloud project to use (only if using Vertex AI). Required if using Vertex AI and the environment variable isn't set.
location
str
Optional
Env:
GOOGLE_CLOUD_LOCATION
#
Google Cloud location to use (only if using Vertex AI). Required if using Vertex AI and the environment variable isn't set.
temperature
float
Optional
Default:
0.8
#
A measure of randomness in output. A lower value results in more predictable output, while a higher value results in
more creative output.
To learn more, see
Model parameters
.
max_output_tokens
int
Optional
#
Maximum number of tokens that can be generated in the response.
To learn more, see
Model parameters
.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Specifies whether to use tools during response generation.
Additional resources
The following resources provide more information about using Google Gemini with LiveKit Agents.
Python package
The
livekit-plugins-google
package on PyPI.
Plugin reference
Reference for the Google Gemini LLM plugin.
GitHub repo
View the source or contribute to the LiveKit Google Gemini LLM plugin.
Gemini docs
Google Gemini documentation.
Voice AI quickstart
Get started with LiveKit Agents and Google Gemini.
Google AI ecosystem guide
Overview of the entire Google AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/tts/google:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing speech
Additional resources
Overview
Google Cloud TTS
provides a wide voice selection and generates speech with humanlike intonation. With LiveKit's Google Cloud TTS integration and the Agents framework, you can build voice AI applications that sound realistic.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[google]~=1.0"
Copy
Authentication
Google Cloud credentials must be provided by one of the following methods:
Passed in the
credentials_info
dictionary.
Saved in the
credentials_file
JSON file (
GOOGLE_APPLICATION_CREDENTIALS
environment variable).
Application Default Credentials. To learn more,
see
How Application Default Credentials works
Usage
Use a Google Cloud TTS in an
AgentSession
or as a standalone speech generator. For example, you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
google
session
=
AgentSession
(
tts
=
google
.
TTS
(
gender
=
"female"
,
voice_name
=
"en-US-Standard-H"
,
)
,
# ... llm, stt, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
language
SpeechLanguages | string
Optional
Default:
en-US
#
Specify output language. For a full list of languages,
see
Supported voices and languages
.
gender
Gender | string
Optional
Default:
neutral
#
Voice gender. Valid values are
male
,
female
, and
neutral
.
voice_name
string
Optional
#
Name of the voice to use for speech. For a full list of voices,
see
Supported voices and languages
.
credentials_info
array
Optional
#
Key-value pairs of authentication credential information.
credentials_file
string
Optional
#
Name of the JSON file that contains authentication credentials for Google Cloud.
Customizing speech
Google Cloud TTS supports Speech Synthesis Markup Language (SSML) to customize pronunciation and speech.
To learn more, see the
SSML reference
.
Additional resources
The following resources provide more information about using Google Cloud with LiveKit Agents.
Python package
The
livekit-plugins-google
package on PyPI.
Plugin reference
Reference for the Google Cloud TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Google Cloud TTS plugin.
Google Cloud docs
Google Cloud TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and Google Cloud TTS.
Google ecosystem guide
Overview of the entire Google AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing speech
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/stt/google:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
Google Cloud
provides a streaming
STT service
with support for over 125 languages and access to the foundational model
chirp
, which provides improved recognition and transcription for spoken languages and accents. You can use the open source Google Cloud plugin for LiveKit Agents to build voice AI with fast, accurate transcription.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[google]~=1.0"
Copy
Authentication
Google Cloud credentials must be provided by one of the following methods:
Passed in the
credentials_info
dictionary.
Saved in the
credentials_file
JSON file (
GOOGLE_APPLICATION_CREDENTIALS
environment variable).
Application Default Credentials. To learn more,
see
How Application Default Credentials works
Usage
Use a Google Cloud STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
google
session
=
AgentSession
(
stt
=
google
.
STT
(
model
=
"chirp"
,
spoken_punctuation
=
False
,
)
,
# ... llm, tts, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
languages
LanguageCode
Optional
Default:
en-US
#
Specify input languages. For a full list of supported languages,
see
Speech-to-text supported languages
.
spoken_punctuation
boolean
Optional
Default:
True
#
Replace spoken punctuation with punctuation characters in text.
model
SpeechModels | string
Optional
Default:
long
#
Model to use for speech-to-text. To learn more, see
Select a transcription model
.
credentials_info
array
Optional
#
Key-value pairs of authentication credential information.
credentials_file
string
Optional
#
Name of the JSON file that contains authentication credentials for Google Cloud.
Additional resources
The following resources provide more information about using Google Cloud with LiveKit Agents.
Python package
The
livekit-plugins-google
package on PyPI.
Plugin reference
Reference for the Google Cloud STT plugin.
GitHub repo
View the source or contribute to the LiveKit Google Cloud STT plugin.
Google Cloud docs
Google Cloud STT docs.
Voice AI quickstart
Get started with LiveKit Agents and Google Cloud STT.
Google ecosystem guide
Overview of the entire Google AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/tts/azure:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Controlling speech and pronunciation
Additional resources
Overview
Azure Speech
provides a
streaming TTS service
with high accuracy, realtime transcription. You can use the open source Azure Speech plugin for LiveKit Agents to build voice AI with fast, accurate transcription.
Quick reference
This section provides a brief overview of the Azure Speech TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[azure]~=1.0"
Copy
Authentication
The Azure Speech plugin requires an
Azure Speech key
.
Set the following environment variables in your
.env
file:
AZURE_SPEECH_KEY
=
<
azure-speech-key
>
AZURE_SPEECH_REGION
=
<
azure-speech-region
>
AZURE_SPEECH_HOST
=
<
azure-speech-host
>
Copy
Usage
Use an Azure Speech TTS within an
AgentSession
or as a standalone speech generator. For example, you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
azure
session
=
AgentSession
(
tts
=
azure
.
TTS
(
speech_key
=
"<speech_service_key>"
,
speech_region
=
"<speech_service_region>"
,
)
,
# ... llm, stt, etc.
)
Copy
Note
To create an instance of
azure.TTS
, one of the following options must be met:
speech_host
must be set,
or
speech_key
and
speech_region
must both be set,
or
speech_auth_token
and
speech_region
must both be set.
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
voice
string
Optional
#
Voice for text-to-speech. To learn more, see
Select synthesis language and voice
.
language
string
Optional
#
Language of the input text. To learn more, see
Select synthesis language and voice
.
prosody
ProsodyConfig
Optional
#
Specify changes to pitch, rate, and volume for the speech output. To learn more,
see
Adjust prosody
.
speech_key
string
Optional
Env:
AZURE_SPEECH_KEY
#
Azure Speech speech-to-text key. To learn more, see
Azure Speech prerequisites
.
speech_region
string
Optional
Env:
AZURE_SPEECH_REGION
#
Azure Speech speech-to-text region. To learn more, see
Azure Speech prerequisites
.
speech_host
string
Optional
Env:
AZURE_SPEECH_HOST
#
Azure Speech endpoint.
speech_auth_token
string
Optional
#
Azure Speech authentication token.
Controlling speech and pronunciation
Azure Speech TTS supports Speech Synthesis Markup Language (SSML) for customizing generated speech. To learn
more, see
SSML overview
.
Additional resources
The following resources provide more information about using Azure Speech with LiveKit Agents.
Python package
The
livekit-plugins-azure
package on PyPI.
Plugin reference
Reference for the Azure Speech TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Azure Speech TTS plugin.
Azure Speech docs
Azure Speech's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Azure Speech.
Azure ecosystem guide
Overview of the entire Azure AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Controlling speech and pronunciation
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/stt/azure:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
Azure Speech
provides a streaming
STT service
with high accuracy, realtime transcription. You can use the open source Azure Speech plugin for LiveKit Agents to build voice AI with fast, accurate transcription.
Quick reference
This section provides a brief overview of the Azure Speech STT plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[azure]~=1.0"
Copy
Authentication
The Azure Speech plugin requires an
Azure Speech key
.
Set the following environment variables in your
.env
file:
AZURE_SPEECH_KEY
=
<
azure-speech-key
>
AZURE_SPEECH_REGION
=
<
azure-speech-region
>
AZURE_SPEECH_HOST
=
<
azure-speech-host
>
Copy
Usage
Use Azure Speech STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
azure
azure_stt
=
stt
.
STT
(
speech_key
=
"<speech_service_key>"
,
speech_region
=
"<speech_service_region>"
,
)
Copy
Note
To create an instance of
azure.STT
, one of the following options must be met:
speech_host
must be set,
or
speech_key
and
speech_region
must both be set,
or
speech_auth_token
and
speech_region
must both be set
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
speech_key
string
Optional
Env:
AZURE_SPEECH_KEY
#
Azure Speech speech-to-text key. To learn more, see
Azure Speech prerequisites
.
speech_region
string
Optional
Env:
AZURE_SPEECH_REGION
#
Azure Speech speech-to-text region. To learn more, see
Azure Speech prerequisites
.
speech_host
string
Optional
Env:
AZURE_SPEECH_HOST
#
Azure Speech endpoint.
speech_auth_token
string
Optional
#
Azure Speech authentication token.
languages
list[string]
Optional
#
List of potential source languages. To learn more, see
Standard locale names
.
Additional resources
The following resources provide more information about using Azure Speech with LiveKit Agents.
Python package
The
livekit-plugins-azure
package on PyPI.
Plugin reference
Reference for the Azure Speech STT plugin.
GitHub repo
View the source or contribute to the LiveKit Azure Speech STT plugin.
Azure Speech docs
Azure Speech's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Azure Speech.
Azure ecosystem guide
Overview of the entire Azure AI and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/realtime/azure-openai:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Server VAD
Additional resources
Overview
Azure OpenAI
provides an implementation of OpenAI's Realtime API that enables low-latency, multimodal interactions with realtime audio and text processing through Azure's managed service. Use LiveKit's Azure OpenAI plugin to create an agent that uses the Realtime API.
Note
Using the OpenAI platform instead of Azure? See our
OpenAI Realtime API guide
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the OpenAI plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Authentication
The Azure OpenAI plugin requires an
Azure OpenAI API key
and your Azure OpenAI endpoint.
Set the following environment variables in your
.env
file:
AZURE_OPENAI_API_KEY
=
<
your-azure-openai-api-key
>
AZURE_OPENAI_ENDPOINT
=
<
your-azure-openai-endpoint
>
OPENAI_API_VERSION
=
2024
-10-01-preview
Copy
Usage
Use the Azure OpenAI Realtime API within an
AgentSession
:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
realtime
.
RealtimeModel
.
with_azure
(
azure_deployment
=
"<model-deployment>"
,
azure_endpoint
=
"wss://<endpoint>.openai.azure.com/"
,
api_key
=
"<api-key>"
,
api_version
=
"2024-10-01-preview"
,
)
,
)
Copy
For a more comprehensive agent example, see the
Voice AI quickstart
.
Parameters
This section describes the Azure-specific parameters. For a complete list of all available parameters, see the
plugin documentation
.
azure_deployment
string
Required
#
Name of your model deployment.
entra_token
string
Optional
#
Microsoft Entra ID authentication token. Required if not using API key authentication.
To learn more see Azure's
Authentication
documentation.
voice
string
Optional
Default:
alloy
#
Voice to use for speech. To learn more, see
Voice options
.
temperature
float
Optional
Default:
1.0
#
A measure of randomness of completions. A lower temperature is more deterministic. To learn more, see
chat completions
.
instructions
string
Optional
#
Initial system instructions.
modalities
list[api_proto.Modality]
Optional
Default:
["text", "audio"]
#
Modalities to use, such as ["text", "audio"].
turn_detection
TurnDetection | None
Optional
#
Configuration for turn detection, see the section on
Turn detection
for more information.
Turn detection
The Azure OpenAI Realtime API includes
voice activity detection (VAD)
to automatically detect when a user has started or stopped speaking. This feature is enabled by default
There is one supported mode for VAD:
Server VAD
(default) - Uses periods of silence to automatically chunk the audio
Server VAD
Server VAD is the default mode and can be configured with the following properties:
from
livekit
.
plugins
.
openai
import
realtime
from
openai
.
types
.
beta
.
realtime
.
session
import
TurnDetection
session
=
AgentSession
(
llm
=
realtime
.
RealtimeModel
(
turn_detection
=
TurnDetection
(
type
=
"server_vad"
,
threshold
=
0.5
,
prefix_padding_ms
=
300
,
silence_duration_ms
=
500
,
create_response
=
True
,
interrupt_response
=
True
,
)
)
,
)
Copy
threshold
: Higher values require louder audio to activate, better for noisy environments.
prefix_padding_ms
: Amount of audio to include before detected speech.
silence_duration_ms
: Duration of silence to detect speech stop (shorter = faster turn detection).
Additional resources
The following resources provide more information about using Azure OpenAI with LiveKit Agents.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the Azure OpenAI Realtime plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI Realtime plugin.
Azure OpenAI docs
Azure OpenAI service documentation.
Voice AI quickstart
Get started with LiveKit Agents and Azure OpenAI.
Azure ecosystem overview
Overview of the entire Azure AI ecosystem and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Turn detection
Server VAD
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/azure-openai:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
Azure OpenAI
provides access to OpenAI's powerful language models like
gpt-4o
and
o1
through Azure's managed service. With LiveKit's Azure OpenAI integration and the Agents framework, you can build sophisticated voice AI applications using their industry-leading models.
Note
Using the OpenAI platform instead of Azure? See our
OpenAI LLM integration guide
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Authentication
The Azure OpenAI plugin requires either an
Azure OpenAI API key
or a Microsoft Entra ID token.
Set the following environment variables in your
.env
file:
AZURE_OPENAI_API_KEY
or
AZURE_OPENAI_ENTRA_TOKEN
AZURE_OPENAI_ENDPOINT
OPENAI_API_VERSION
Usage
Use Azure OpenAI within an
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_azure
(
azure_deployment
=
"<model-deployment>"
,
azure_endpoint
=
"https://<endpoint>.openai.azure.com/"
,
# or AZURE_OPENAI_ENDPOINT
api_key
=
"<api-key>"
,
# or AZURE_OPENAI_API_KEY
api_version
=
"2024-10-01-preview"
,
# or OPENAI_API_VERSION
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Copy
Parameters
This section describes the Azure-specific parameters. For a complete list of all available parameters, see the
plugin documentation
.
azure_deployment
string
Required
#
Name of your model deployment.
entra_token
string
Optional
#
Microsoft Entra ID authentication token. Required if not using API key authentication.
To learn more see Azure's
Authentication
documentation.
Additional resources
The following links provide more information about the Azure OpenAI LLM plugin.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the Azure OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Azure OpenAI docs
Azure OpenAI service documentation.
Voice AI quickstart
Get started with LiveKit Agents and Azure OpenAI.
Azure ecosystem overview
Overview of the entire Azure AI ecosystem and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/aws:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
Amazon Bedrock
is a fully managed service that provides a wide range of pre-trained models. With LiveKit's open source Bedrock integration and the Agents framework, you can build sophisticated voice AI applications using models from a wide variety of providers.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[aws]~=1.0"
Copy
Authentication
The AWS plugin requires AWS credentials. Set the following environment variables in your
.env
file:
AWS_ACCESS_KEY_ID
=
<
your-aws-access-key-id
>
AWS_SECRET_ACCESS_KEY
=
<
your-aws-secret-access-key
>
Copy
Usage
Use Bedrock within an
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
aws
session
=
AgentSession
(
llm
=
aws
.
LLM
(
model
=
"anthropic.claude-3-5-sonnet-20240620-v1:0"
,
temperature
=
0.8
,
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
plugin reference
.
model
string | TEXT_MODEL
Optional
Default:
anthropic.claude-3-5-sonnet-20240620-v1:0
#
The model to use for the LLM. For more information, see the documentation for the
modelId
parameter in the
Amazon Bedrock API reference
.
region
string
Optional
Default:
us-east-1
#
The region to use for AWS API requests.
temperature
float
Optional
#
A measure of randomness in output. A lower value results in more predictable output, while a higher value results in
more creative output.
Default values vary depending on the model you select. To learn more, see
Inference request parameters and response fields for foundation models
.
tool_choice
[ToolChoice | Literal['auto', 'required', 'none']]
Optional
Default:
auto
#
Specifies whether to use tools during response generation.
Additional resources
The following links provide more information about the Amazon Bedrock LLM plugin.
Python package
The
livekit-plugins-aws
package on PyPI.
Plugin reference
Reference for the Amazon Bedrock LLM plugin.
GitHub repo
View the source or contribute to the LiveKit Amazon Bedrock LLM plugin.
Bedrock docs
Amazon Bedrock docs.
Voice AI quickstart
Get started with LiveKit Agents and Amazon Bedrock.
AWS ecosystem guide
Overview of the entire AWS and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/tts/aws:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Controlling speech and pronunciation
Additional resources
Overview
Amazon Polly
is an AI voice generator that provides high-quality, natural-sounding
human voices in multiple languages. With LiveKit's Amazon Polly integration and the Agents framework, you can build
voice AI applications that sound realistic.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[aws]~=1.0"
Copy
Authentication
The Amazon Polly plugin requires an
AWS API key
.
Set the following environment variables in your
.env
file:
AWS_ACCESS_KEY_ID
=
<
aws-access-key-id
>
AWS_SECRET_ACCESS_KEY
=
<
aws-secret-access-key
>
AWS_DEFAULT_REGION
=
<
aws-deployment-region
>
Copy
Usage
Use an Amazon Polly TTS within an
AgentSession
or as a standalone speech generator. For example, you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
aws
session
=
AgentSession
(
tts
=
aws
.
TTS
(
voice
=
"Ruth"
,
speech_engine
=
"generative"
,
language
=
"en-US"
,
)
,
# ... llm, stt, etc.
)
Copy
Parameters
This section describes some of the parameters. See the
plugin reference
for a complete list of all available parameters.
voice
TTSModels
Optional
Default:
Ruth
#
Voice to use for the synthesis. For a full list, see
Available voices
.
language
TTS_LANGUAGE | string
Optional
#
Language code for the Synthesize Speech request. This is only necessary if using a bilingual voice, such as Aditi,
which can be used for either Indian English (en-IN) or Hindi (hi-IN). To learn more,
see
Languages in Amazon Polly
.
speech_engine
TTS_SPEECH_ENGINE
Optional
Default:
generative
#
The voice engine to use for the synthesis. Valid values are
standard
,
neural
,
long-form
, and
generative
.
To learn more, see
Amazon Polly voice engines
.
Controlling speech and pronunciation
Amazon Polly supports Speech Synthesis Markup Language (SSML) for customizing generated speech. To learn more, see
Generating speech from SSML docs
and
Supported SSML tags
.
Additional resources
The following resources provide more information about using Amazon Polly with LiveKit Agents.
Python package
The
livekit-plugins-aws
package on PyPI.
Plugin reference
Reference for the Amazon Polly TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Amazon Polly TTS plugin.
AWS docs
Amazon Polly's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Amazon Polly.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Controlling speech and pronunciation
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/stt/aws:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Amazon Transcribe
provides a streaming STT service with high accuracy, realtime transcription. You can use the open source Amazon Transcribe plugin for LiveKit Agents to build voice AI with fast, accurate transcription.
Quick reference
This section provides a brief overview of the Amazon Transcribe STT plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[aws]~=1.0"
Copy
Authentication
The Amazon Transcribe plugin requires an
AWS API key
.
Set the following environment variables in your
.env
file:
AWS_ACCESS_KEY_ID
=
<
aws-access-key-id
>
AWS_SECRET_ACCESS_KEY
=
<
aws-secret-access-key
>
AWS_DEFAULT_REGION
=
<
aws-deployment-region
>
Copy
Usage
Use Amazon Transcribe STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
aws
session
=
AgentSession
(
stt
=
aws
.
STT
(
session_id
=
"my-session-id"
,
language
=
"en-US"
,
vocabulary_name
=
"my-vocabulary"
,
vocab_filter_name
=
"my-vocab-filter"
,
vocab_filter_method
=
"mask"
,
)
,
# ... llm, tts, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
speech_region
string
Optional
Default:
us-east-1
Env:
AWS_DEFAULT_REGION
#
The region of the AWS deployment. Required if the environment variable isn't set.
language
string
Optional
Default:
en-US
#
The language of the audio. For a full list of supported languages, see the
Supported languages
page.
vocabulary_name
string
Optional
Default:
None
#
Name of the custom vocabulary you want to use when processing your transcription. To learn more, see
Custom vocabularies
.
session_id
string
Optional
#
Name for your transcription session. If left empty, Amazon Transcribe generates an ID and returns it in the response.
vocab_filter_name
string
Optional
Default:
None
#
Name of the custom vocabulary filter that you want to use when processing your transcription. To learn more, see
Using custom vocabulary filters to delete, mask, or flag words
.
vocab_filter_method
string
Optional
Default:
None
#
Display method for the vocabulary filter. To learn more, see
Using custom vocabulary filters to delete, mask, or flag words
.
Additional resources
The following resources provide more information about using Amazon Transcribe with LiveKit Agents.
Python package
The
livekit-plugins-aws
package on PyPI.
Plugin reference
Reference for the Amazon Transcribe STT plugin.
GitHub repo
View the source or contribute to the LiveKit Amazon Transcribe STT plugin.
AWS docs
Amazon Transcribe's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Amazon Transcribe.
On this page
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/groq:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
Groq
provides fast LLM inference using open models from Llama, DeepSeek, and more. With LiveKit's Groq integration and the Agents framework, you can build low-latency voice AI applications.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[groq]~=1.0"
Copy
Authentication
The Groq plugin requires a
Groq API key
.
Set
GROQ_API_KEY
in your
.env
file.
Usage
Use a Groq LLM in your
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
groq
session
=
AgentSession
(
llm
=
groq
.
LLM
(
model
=
"llama3-8b-8192"
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters,
see the
plugin reference
.
model
string
Optional
Default:
llama-3.3-70b-versatile
#
Name of the LLM model to use. For all options, see the
Groq model list
.
temperature
float
Optional
Default:
1.0
#
A measure of randomness in output. A lower value results in more predictable output, while a higher value results in
more creative output.
parallel_tool_calls
bool
Optional
#
Set to true to parallelize tool calls.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Specifies whether to use tools during response generation.
Additional resources
The following resources provide more information about using Groq with LiveKit Agents.
Python package
The
livekit-plugins-groq
package on PyPI.
Plugin reference
Reference for the Groq LLM plugin.
GitHub repo
View the source or contribute to the LiveKit Groq LLM plugin.
Groq docs
Groq docs.
Voice AI quickstart
Get started with LiveKit Agents and Groq.
Groq ecosystem overview
Overview of the entire Groq and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/tts/groq:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
Groq
provides fast TTS using models from PlayAI. With LiveKit's Groq integration and the Agents
framework, you can build voice AI applications with fluent and conversational voices. Groq TTS supports English and Arabic speech.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[groq]~=1.0"
Copy
Authentication
The Groq plugin requires a
Groq API key
.
Set
GROQ_API_KEY
in your
.env
file.
Usage
Use Groq TTS in your
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
groq
session
=
AgentSession
(
tts
=
groq
.
TTS
(
model
=
"playai-tts"
,
voice
=
"Arista-PlayAI"
,
)
,
# ... stt, llm, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
TTSModel | string
Optional
Default:
playai-tts
#
Name of the TTS model. For a full list, see
Models
.
voice
string
Optional
Default:
Arista-PlayAI
#
Name of the voice. For a full list, see
English
and
Arabic
voices.
Additional resources
The following resources provide more information about using Groq with LiveKit Agents.
Python package
The
livekit-plugins-groq
package on PyPI.
Plugin reference
Reference for the Groq TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Groq TTS plugin.
Groq docs
Groq TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and Groq TTS.
Groq ecosystem guide
Overview of the entire Groq and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/stt/groq:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Try Groq transcription
Experience Groq's fast STT in a LiveKit-powered playground
Overview
Groq
provides fast STT using fine-tuned and distilled models based on Whisper V3 Large. With LiveKit's Groq integration and the Agents framework, you can build AI voice applications with fluent and conversational voices.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[groq]~=1.0"
Copy
Authentication
The Groq plugin requires a
Groq API key
.
Set
GROQ_API_KEY
in your
.env
file.
Usage
Use Groq STT in your
AgentSession
or as a standalone transcription service. For example,
you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
groq
session
=
AgentSession
(
stt
=
groq
.
STT
(
model
=
"whisper-large-v3-turbo"
,
language
=
"en"
,
)
,
# ... tts, llm, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
whisper-large-v3-turbo
#
Name of the STT model to use. For help with model selection, see the
Groq STT documentation
.
language
string
Optional
Default:
en
#
Language of the input audio in
ISO-639-1
format.
prompt
string
Optional
#
Prompt to guide the model's style or specify how to spell unfamiliar words. 224 tokens max.
Additional resources
The following resources provide more information about using Groq with LiveKit Agents.
Python package
The
livekit-plugins-groq
package on PyPI.
Plugin reference
Reference for the Groq STT plugin.
GitHub repo
View the source or contribute to the LiveKit Groq STT plugin.
Groq docs
Groq STT docs.
Voice AI quickstart
Get started with LiveKit Agents and Groq STT.
Groq ecosystem guide
Overview of the entire Groq and LiveKit Agents integration.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/anthropic:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
Anthropic
provides Claude, an advanced AI assistant with capabilities including advanced reasoning, vision analysis, code generation, and multilingual processing. With LiveKit's Anthropic integration and the Agents framework, you can build sophisticated voice AI applications.
You can also use Claude with
Amazon Bedrock
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[anthropic]~=1.0"
Copy
Authentication
The Anthropic plugin requires an
Anthropic API key
.
Set
ANTHROPIC_API_KEY
in your
.env
file.
Usage
Use Claude within an
AgentSession
or as a standalone LLM service. For example,
you can use this LLM in the
Voice AI quickstart
.
from
livekit
.
plugins
import
anthropic
session
=
AgentSession
(
llm
=
anthropic
.
LLM
(
model
=
"claude-3-5-sonnet-20241022"
,
temperature
=
0.8
,
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
str | ChatModels
Optional
Default:
claude-3-5-sonnet-20241022
#
Model to use. For a full list of available models, see the
Model options
.
max_tokens
int
Optional
#
The maximum number of tokens to generate before stopping. To learn more, see the
Anthropic API reference
.
temperature
float
Optional
Default:
1
#
A measure of randomness in output. A lower value results in more predictable output, while a higher value results in
more creative output.
Valid values are between
0
and
1
. To learn more, see the
Anthropic API reference
.
parallel_tool_calls
bool
Optional
#
Set to true to parallelize tool calls.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Specifies whether to use tools during response generation.
Additional resources
The following links provide more information about the Anthropic LLM plugin.
Python package
The
livekit-plugins-anthropic
package on PyPI.
Plugin reference
Reference for the Anthropic LLM plugin.
GitHub repo
View the source or contribute to the LiveKit Anthropic LLM plugin.
Anthropic docs
Anthropic Claude docs.
Voice AI quickstart
Get started with LiveKit Agents and Anthropic.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/cerebras:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Usage
Parameters
Links
Overview
Cerebras
provides access to Llama 3.1 and 3.3 models through their inference API. These models are multilingual and text-only, making them suitable for a variety of agent applications.
Usage
Install the OpenAI plugin to add Cerebras support:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Set the following environment variable in your
.env
file:
CEREBRAS_API_KEY
=
<
your-cerebras-api-key
>
Copy
Create a Cerebras LLM using the
with_cerebras
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_cerebras
(
model
=
"llama3.1-8b"
,
temperature
=
0.7
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
str | CerebrasChatModels
Optional
Default:
llama3.1-8b
#
Model to use for inference. To learn more, see
supported models
.
temperature
float
Optional
Default:
1.0
#
A measure of randomness in output. A lower value results in more predictable output, while a higher value results in
more creative output.
Valid values are between
0
and
1.5
. To learn more, see the
Cerebras documentation
.
parallel_tool_calls
bool
Optional
#
Set to true to parallelize tool calls.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Specifies whether to use tools during response generation.
Links
The following links provide more information about the Cerebras LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_cerebras
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Cerebras docs
Cerebras inference docs.
Voice AI quickstart
Get started with LiveKit Agents and Cerebras.
On this page
Overview
Usage
Parameters
Links
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/deepseek:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Usage
Parameters
Links
Overview
DeepSeek
provides access to their latest models through their OpenAI-compatible API. These models are multilingual and text-only, making them suitable for a variety of agent applications.
Additional providers
DeepSeek models are also available through a number of other providers, such as
Cerebras
and
Groq
.
Usage
Use the OpenAI plugin's
with_deepseek
method to set the default agent session LLM to DeepSeek:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Set the following environment variable in your
.env
file:
DEEPSEEK_API_KEY
=
<
your-deepseek-api-key
>
Copy
from
livekit
.
plugins
import
openai
deepseek_llm
=
openai
.
LLM
.
with_deepseek
(
model
=
"deepseek-chat"
,
# this is DeepSeek-V3
temperature
=
0.7
)
Copy
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
method reference
.
model
str | DeepSeekChatModels
Optional
Default:
deepseek-chat
#
DeepSeek model to use. See
models and pricing
for a complete list.
temperature
float
Optional
Default:
1.0
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Valid values are between
0
and
2
.
parallel_tool_calls
bool
Optional
#
Controls whether the model can make multiple tool calls in parallel. When enabled, the model can make multiple tool calls simultaneously, which can improve performance for complex tasks.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Controls how the model uses tools. Set to 'auto' to let the model decide, 'required' to force tool usage, or 'none' to disable tool usage.
Links
The following links provide more information about the DeepSeek LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_deepseek
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
DeepSeek docs
DeepSeek API documentation.
Voice AI quickstart
Get started with LiveKit Agents and DeepSeek.
On this page
Overview
Usage
Parameters
Links
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/fireworks:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Usage
Parameters
Links
Overview
Fireworks AI
provides access to Llama 3.1 instruction-tuned models through their inference API. These models are multilingual and text-only, making them suitable for a variety of agent applications.
Usage
Install the OpenAI plugin to add Fireworks AI support:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Set the following environment variable in your
.env
file:
FIREWORKS_API_KEY
=
<
your-fireworks-api-key
>
Copy
Create a Fireworks AI LLM using the
with_fireworks
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_fireworks
(
model
=
"accounts/fireworks/models/llama-v3p3-70b-instruct"
,
temperature
=
0.7
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
method reference
.
model
str
Optional
Default:
accounts/fireworks/models/llama-v3p3-70b-instruct
#
Model to use for inference. To learn more, see
supported models
.
temperature
float
Optional
Default:
1.0
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Valid values are between
0
and
1.5
.
parallel_tool_calls
bool
Optional
#
Controls whether the model can make multiple tool calls in parallel. When enabled, the model can make multiple tool calls simultaneously, which can improve performance for complex tasks.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Controls how the model uses tools. Set to 'auto' to let the model decide, 'required' to force tool usage, or 'none' to disable tool usage.
Links
The following links provide more information about the Fireworks AI LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_fireworks
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Fireworks AI docs
Fireworks AI API documentation.
Voice AI quickstart
Get started with LiveKit Agents and Fireworks AI.
On this page
Overview
Usage
Parameters
Links
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/ollama:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Usage
Parameters
Links
Overview
Ollama
is an open source LLM engine that you can use to run models locally with an OpenAI-compatible API. Ollama support is available using the OpenAI plugin for LiveKit Agents.
Usage
Install the OpenAI plugin to add Ollama support:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Create an Ollama LLM using the
with_ollama
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_ollama
(
model
=
"llama3.1"
,
base_url
=
"http://localhost:11434/v1"
,
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
llama3.1
#
Ollama model to use. For a list of available models, see
Ollama models
.
base_url
string
Optional
Default:
http://localhost:11434/v1
#
Base URL for the Ollama API.
temperature
float
Optional
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Links
The following links provide more information about the Ollama integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_ollama
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Ollama docs
Ollama site and documentation.
Voice AI quickstart
Get started with LiveKit Agents and Ollama.
On this page
Overview
Usage
Parameters
Links
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/perplexity:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Usage
Parameters
Links
Overview
Perplexity
provides access to their Sonar models, which are based on Llama 3.1 but fine-tuned for search, through their inference API. These models are multilingual and text-only, making them suitable for a variety of agent applications.
Usage
Install the OpenAI plugin to add Perplexity support:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Set the following environment variable in your
.env
file:
PERPLEXITY_API_KEY
=
<
your-perplexity-api-key
>
Copy
Create a Perplexity LLM using the
with_perplexity
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_perplexity
(
model
=
"llama-3.1-sonar-small-128k-chat"
,
temperature
=
0.7
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
method reference
.
model
str | PerplexityChatModels
Optional
Default:
llama-3.1-sonar-small-128k-chat
#
Model to use for inference. To learn more, see
supported models
.
temperature
float
Optional
Default:
1.0
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Valid values are between
0
and
2
.
parallel_tool_calls
bool
Optional
#
Controls whether the model can make multiple tool calls in parallel. When enabled, the model can make multiple tool calls simultaneously, which can improve performance for complex tasks.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Controls how the model uses tools. Set to 'auto' to let the model decide, 'required' to force tool usage, or 'none' to disable tool usage.
Links
The following links provide more information about the Perplexity LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_perplexity
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Perplexity docs
Perplexity API documentation.
Voice AI quickstart
Get started with LiveKit Agents and Perplexity.
On this page
Overview
Usage
Parameters
Links
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/telnyx:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Usage
Parameters
Links
Overview
Telnyx
provides access to Llama 3.1 and other models through their inference API. These models are multilingual and text-only, making them suitable for a variety of agent applications.
Usage
Install the OpenAI plugin to add Telnyx support:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Set the following environment variable in your
.env
file:
TELNYX_API_KEY
=
<
your-telnyx-api-key
>
Copy
Create a Telnyx LLM using the
with_telnyx
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_telnyx
(
model
=
"meta-llama/Meta-Llama-3.1-70B-Instruct"
,
temperature
=
0.7
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
method reference
.
model
str | TelnyxChatModels
Optional
Default:
meta-llama/Meta-Llama-3.1-70B-Instruct
#
Model to use for inference. To learn more, see
supported models
.
temperature
float
Optional
Default:
0.1
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Valid values are between
0
and
2
.
parallel_tool_calls
bool
Optional
#
Controls whether the model can make multiple tool calls in parallel. When enabled, the model can make multiple tool calls simultaneously, which can improve performance for complex tasks.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Controls how the model uses tools. Set to 'auto' to let the model decide, 'required' to force tool usage, or 'none' to disable tool usage.
Links
The following links provide more information about the Telnyx LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_telnyx
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Telnyx docs
Telnyx API documentation.
Voice AI quickstart
Get started with LiveKit Agents and Telnyx.
On this page
Overview
Usage
Parameters
Links
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/together:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Usage
Parameters
Links
Overview
Together AI
provides access to Llama 2 and Llama 3 models including instruction-tuned models through their inference API. These models are multilingual and text-only, making them suitable for a variety of agent applications.
Usage
Install the OpenAI plugin to add Together AI support:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Set the following environment variable in your
.env
file:
TOGETHER_API_KEY
=
<
your-together-api-key
>
Copy
Create a Together AI LLM using the
with_together
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_together
(
model
=
"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
,
temperature
=
0.7
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
method reference
.
model
str | TogetherChatModels
Optional
Default:
meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
#
Model to use for inference. To learn more, see
supported models
.
temperature
float
Optional
Default:
1.0
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Valid values are between
0
and
1
.
parallel_tool_calls
bool
Optional
#
Controls whether the model can make multiple tool calls in parallel. When enabled, the model can make multiple tool calls simultaneously, which can improve performance for complex tasks.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Controls how the model uses tools. Set to 'auto' to let the model decide, 'required' to force tool usage, or 'none' to disable tool usage.
Links
The following links provide more information about the Together AI LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_together
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
Together AI docs
Together AI API documentation.
Voice AI quickstart
Get started with LiveKit Agents and Together AI.
On this page
Overview
Usage
Parameters
Links
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/llm/xai:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Usage
Parameters
Links
Overview
xAI
provides access to Grok models through their OpenAI-compatible API. These models are multilingual and support multimodal capabilities, making them suitable for a variety of agent applications.
Usage
Install the OpenAI plugin to add xAI support:
pip
install
"livekit-agents[openai]~=1.0"
Copy
Set the following environment variable in your
.env
file:
XAI_API_KEY
=
<
your-xai-api-key
>
Copy
Create a Grok LLM using the
with_x_ai
method:
from
livekit
.
plugins
import
openai
session
=
AgentSession
(
llm
=
openai
.
LLM
.
with_x_ai
(
model
=
"grok-2-public"
,
temperature
=
1.0
,
)
,
# ... tts, stt, vad, turn_detection, etc.
)
Copy
Parameters
This section describes some of the available parameters. For a complete reference of all available parameters, see the
method reference
.
model
str | XAIChatModels
Optional
Default:
grok-2-public
#
Grok model to use. To learn more, see the
xAI Grok models
page.
temperature
float
Optional
Default:
1.0
#
Controls the randomness of the model's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic.
Valid values are between
0
and
2
. To learn more, see the optional parameters for
Chat completions
parallel_tool_calls
bool
Optional
#
Controls whether the model can make multiple tool calls in parallel. When enabled, the model can make multiple tool calls simultaneously, which can improve performance for complex tasks.
tool_choice
ToolChoice | Literal['auto', 'required', 'none']
Optional
Default:
auto
#
Controls how the model uses tools. Set to 'auto' to let the model decide, 'required' to force tool usage, or 'none' to disable tool usage.
Links
The following links provide more information about the xAI Grok LLM integration.
Python package
The
livekit-plugins-openai
package on PyPI.
Plugin reference
Reference for the
with_x_ai
method of the OpenAI LLM plugin.
GitHub repo
View the source or contribute to the LiveKit OpenAI LLM plugin.
xAI docs
xAI Grok documentation.
Voice AI quickstart
Get started with LiveKit Agents and xAI Grok.
On this page
Overview
Usage
Parameters
Links
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/stt/assemblyai:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
AssemblyAI
provides a streaming STT service with high accuracy, realtime transcription. You can use the open source AssemblyAI plugin for LiveKit Agents to build voice AI with fast, accurate transcription.
Quick reference
This section provides a brief overview of the AssemblyAI STT plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[assemblyai]~=1.0"
Copy
Authentication
The AssemblyAI plugin requires an
AssemblyAI API key
.
Set
ASSEMBLYAI_API_KEY
in your
.env
file.
Usage
Use AssemblyAI STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
assemblyai
session
=
AgentSession
(
stt
=
assemblyai
.
STT
(
word_boost
=
[
"LiveKit"
]
,
)
,
# ... llm, tts, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
word_boost
list[string]
Optional
#
List of custom vocabulary. Max 2500 characters. To learn more, see
Add custom vocabulary
.
disable_partial_transcripts
bool
Optional
Default:
False
#
Disable partial transcripts. To learn more, see
Disable partial transcripts
.
end_utterance_silence_threshold
int
Optional
Default:
500
#
Duration of silence to wait after the last utterance in milliseconds. To learn more, see
Configure the threshold for automatic utterance detection
.
Additional resources
The following resources provide more information about using AssemblyAI with LiveKit Agents.
Python package
The
livekit-plugins-assemblyai
package on PyPI.
Plugin reference
Reference for the AssemblyAI STT plugin.
GitHub repo
View the source or contribute to the LiveKit AssemblyAI STT plugin.
AssemblyAI docs
AssemblyAI's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and AssemblyAI.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/stt/clova:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
CLOVA Speech Recognition
is the NAVER Cloud Platform's service to convert human voice into text. You can use the open source CLOVA plugin for LiveKit Agents to build voice AI with fast, accurate transcription.
Quick reference
This section provides a brief overview of the CLOVA STT plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[clova]~=1.0"
Copy
Authentication
The CLOVA plugin requires the following keys, which may set as environment variables or passed to the constructor.
CLOVA_STT_SECRET_KEY
=
<
your-api-key
>
CLOVA_STT_INVOKE_URL
=
<
your-invoke-url
>
Copy
Usage
Create a CLOVA STT to use within an
AgentSession
or as a standalone transcription service.
For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
clova
session
=
AgentSession
(
stt
=
clova
.
STT
(
word_boost
=
[
"LiveKit"
]
,
)
,
# ... llm, tts, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
language
ClovaSttLanguages
Optional
Default:
en-US
#
Speech recognition language. Clova supports English, Korean, Japanese, and Chinese. Valid values are
ko-KR
,
en-US
,
enko
,
ja
,
zh-cn
,
zh-tw
.
Additional resources
The following resources provide more information about using CLOVA with LiveKit Agents.
Python package
The
livekit-plugins-clova
package on PyPI.
Plugin reference
Reference for the CLOVA STT plugin.
GitHub repo
View the source or contribute to the LiveKit CLOVA STT plugin.
CLOVA docs
CLOVA's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and CLOVA.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/stt/deepgram:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
Deepgram
provides advanced speech recognition technology and AI-driven audio processing solutions. Customizable speech models allow you to fine tune transcription performance for your specific use case. With LiveKit's Deepgram integration and the Agents framework, you can build AI agents that provide high-accuracy transcriptions.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[deepgram]~=1.0"
Copy
Authentication
The Deepgram plugin requires a
Deepgram API key
.
Set
DEEPGRAM_API_KEY
in your
.env
file.
Usage
Use Deepgram STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
deepgram
session
=
AgentSession
(
stt
=
deepgram
.
STT
(
model
=
"nova-2-general"
,
interim_results
=
True
,
smart_format
=
True
,
punctuate
=
True
,
filler_words
=
True
,
profanity_filter
=
False
,
keywords
=
[
(
"LiveKit"
,
1.5
)
]
,
language
=
"en-US"
,
)
,
# ... llm, tts, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
nova-2-general
#
ID of the model to use for inference. To learn more, see
supported models
.
interim_results
bool
Optional
Default:
true
#
Enable preliminary results before the final transcription is available.
smart_format
bool
Optional
Default:
true
#
Enable smart formatting to improve the readability of transcriptions.
punctuate
bool
Optional
Default:
true
#
Enable punctuation in transcriptions.
filler_words
bool
Optional
Default:
true
#
Enable filler words to improve turn detection.
profanity_filter
bool
Optional
Default:
false
#
Replace recognized profanity with asterisks in transcriptions.
keywords
list[tuple[string, float]]
Optional
Default:
[]
#
A list of keywords and intensifiers to boost or suppress in transcriptions. Positive values boost; negative values suppress.
language
string
Optional
Default:
en
#
Language of input audio in
ISO-639-1
format.
Additional resources
The following resources provide more information about using Deepgram with LiveKit Agents.
Python package
The
livekit-plugins-deepgram
package on PyPI.
Plugin reference
Reference for the Deepgram STT plugin.
GitHub repo
View the source or contribute to the LiveKit Deepgram STT plugin.
Deepgram docs
Deepgram's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Deepgram.
Deepgram TTS
Guide to the Deepgram TTS integration with LiveKit Agents.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/stt/fal:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
fal
provides a hosted inference platform for a wide variety of model types, including
Wizper
, a speech-to-text model based on Whisper v3 Large. With LiveKit's fal integration and the Agents framework, you can build AI agents that integrate Wizper for fast and accurate transcription.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[fal]~=1.0"
Copy
Authentication
The fal plugin requires a
fal API key
.
Set
FAL_KEY
in your
.env
file.
Usage
Use fal STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
fal
session
=
AgentSession
(
stt
=
fal
.
STT
(
language
=
"de"
,
task
=
"translate"
,
)
,
# ... llm, tts, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
language
str
Optional
Default:
en
#
Speech recognition language.
task
str
Optional
Default:
transcribe
#
Task to perform with audio file. Valid values are
transcribe
and
translate
.
Additional resources
The following resources provide more information about using fal with LiveKit Agents.
Python package
The
livekit-plugins-fal
package on PyPI.
Plugin reference
Reference for the fal STT plugin.
GitHub repo
View the source or contribute to the LiveKit fal STT plugin.
fal docs
fal's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and fal.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/stt/gladia:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Initialization
Realtime translation
Updating options
Parameters
Additional resources
Overview
Gladia
provides accurate speech recognition optimized for enterprise use cases. You can use the open source Gladia integration for LiveKit Agents to build voice AI with fast, accurate transcription and optional translation features.
Quick reference
This section provides a brief overview of the Gladia STT plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[gladia]~=1.0"
Copy
Authentication
The Gladia plugin requires a
Gladia API key
.
Set
GLADIA_API_KEY
in your
.env
file.
Initialization
Use Gladia STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
gladia
session
=
AgentSession
(
stt
=
gladia
.
STT
(
)
,
# ... llm, tts, etc.
)
Copy
Realtime translation
To use realtime translation, set
translation_enabled
to
True
and specify the expected audio languages in
languages
and the desired target language in
translation_target_languages
.
For example, to transcribe and translate a mixed English and French audio stream into English, set the following options:
gladia
.
STT
(
translation_enabled
=
True
,
languages
=
[
"en"
,
"fr"
]
,
translation_target_languages
=
[
"en"
]
)
Copy
Note that if you specify more than one target language, the plugin emits a separate transcription event for each. When used in an
AgentSession
, this adds each transcription to the conversation history, in order, which might confuse the LLM.
Updating options
Use the
update_options
method to configure the STT on the fly:
gladia_stt
=
gladia
.
STT
(
)
gladia_stt
.
update_options
(
languages
=
[
"ja"
,
"en"
]
,
translation_enabled
=
True
,
translation_target_languages
=
[
"fr"
]
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
languages
list[string]
Optional
Default:
[]
#
List of languages to use for transcription. If empty, Gladia will auto-detect the language.
code_switching
bool
Optional
Default:
false
#
Enable switching between languages during recognition.
energy_filter
bool
Optional
Default:
true
#
Enable voice activity detection with energy filtering.
translation_enabled
bool
Optional
Default:
false
#
Enable real-time translation.
translation_target_languages
list[string]
Optional
Default:
[]
#
List of target languages for translation.
Additional resources
The following resources provide more information about using Gladia with LiveKit Agents.
Python package
The
livekit-plugins-gladia
package on PyPI.
Plugin reference
Reference for the Gladia STT plugin.
GitHub repo
View the source or contribute to the LiveKit Gladia STT plugin.
Gladia documentation
Gladia's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Gladia.
On this page
Overview
Quick reference
Installation
Authentication
Initialization
Realtime translation
Updating options
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/stt/speechmatics:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Usage
Parameters
Additional resources
Overview
Speechmatics
provides enterprise-grade speech-to-text APIs. Their advanced speech models deliver highly accurate transcriptions across diverse languages, dialects, and accents. You can use the LiveKit Speechmatics plugin with the Agents framework to build voice AI agents that provide reliable, real-time transcriptions.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[speechmatics]~=1.0"
Copy
Authentication
The Speechmatics plugin requires an
API key
.
Set
SPEECHMATICS_API_KEY
in your
.env
file.
Usage
Use Speechmatics STT in an
AgentSession
or as a standalone transcription service. For example, you can use this STT in the
Voice AI quickstart
.
from
livekit
.
plugins
import
speechmatics
session
=
AgentSession
(
stt
=
speechmatics
.
STT
(
transcription_config
=
speechmatics
.
types
.
TranscriptionConfig
(
operating_point
=
"enhanced"
,
enable_partials
=
True
,
language
=
"en"
,
output_locale
=
"en-US"
,
diarization
=
"speaker"
,
enable_entities
=
True
,
additional_vocab
=
[
{
"content"
:
"financial crisis"
}
,
{
"content"
:
"gnocchi"
,
"sounds_like"
:
[
"nyohki"
,
"nokey"
,
"nochi"
]
}
,
{
"content"
:
"CEO"
,
"sounds_like"
:
[
"C.E.O."
]
}
]
,
max_delay
=
0.7
,
max_delay_mode
=
"flexible"
)
,
audio_settings
=
speechmatics
.
types
.
AudioSettings
(
encoding
=
"pcm_s16le"
,
sample_rate
=
16000
,
)
,
)
,
# ... llm, tts, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
operating_point
string
Optional
Default:
enhanced
#
Operating point to use for the transcription per required accuracy & complexity. To learn more, see
Accuracy Reference
.
enable_partials
bool
Optional
Default:
True
#
Partial transcripts allow you to receive preliminary transcriptions and update as more context is available until the higher-accuracy
final transcript
is returned. Partials are returned faster but without any post-processing such as formatting.
language
string
Optional
Default:
en
#
ISO 639-1 language code. All languages are global and can understand different dialects/accents. To see the list of all supported languages, see
Supported Languages
.
Usage
Create a Speechmatics STT that can be used in a
VoiceAgent
or as a standalone transcription service. For example,
you can use this STT in the
VoiceAgent
quickstart
.
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
operating_point
string
Optional
Default:
enhanced
#
Operating point to use for the transcription per required accuracy & complexity. To learn more, see
Accuracy Reference
.
enable_partials
bool
Optional
Default:
True
#
Partial transcripts allow you to receive preliminary transcriptions and update as more context is available until the higher-accuracy
final transcript
is returned. Partials are returned faster but without any post-processing such as formatting.
language
string
Optional
Default:
en
#
ISO 639-1 language code. All languages are global and can understand different dialects/accents. To see the list of all supported languages, see
Supported Languages
.
output_locale
string
Optional
Default:
en-US
#
RFC-5646 language code for transcription output. For supported locales, see
Output Locale
.
diarization
string
Optional
Default:
NULL
#
Setting this to
speaker
enables accurate labeling of different speakers detected with the attributed transcribed output e.g. S1, S2. For more information, visit
Speaker Diarization
.
additional_vocab
list[dict{“content”:str, ”sounds_like”:str}]
Optional
Default:
NULL
#
Add custom words for each transcription job. To learn more, see
Custom Dictionary
.
enable_entities
bool
Optional
Default:
False
#
Allows the written form of various entities such as phone numbers, emails, currency, etc to be output in the transcript. To learn more about the supported entities, see
Entities
.
max_delay
number
Optional
Default:
0.7
#
The delay in seconds between the end of a spoken word and returning the final transcript results.
max_delay_mode
string
Optional
Default:
flexible
#
If set to
flexible
, the final transcript is delayed until proper numeral formatting is complete. To learn more, see
Numeral Formatting
.
Additional resources
The following resources provide more information about using Speechmatics with LiveKit Agents.
Python package
The
livekit-plugins-speechmatics
package on PyPI.
Plugin reference
Reference for the Speechmatics STT plugin.
GitHub repo
View the source or contribute to the LiveKit Speechmatics STT plugin.
Speechmatics docs
Speechmatics STT docs.
Voice AI quickstart
Get started with LiveKit Agents and Speechmatics STT.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/tts/cartesia:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources
Try the playground
Chat with a voice assistant built with LiveKit and Cartesia TTS
Overview
Cartesia
provides customizable speech synthesis across a number of different languages and
produces natural-sounding speech with low latency. You can use the Cartesia TTS plugin for LiveKit Agents to build voice AI
applications that sound realistic.
Quick reference
This section includes a brief overview of the Cartesia TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[cartesia]~=1.0"
Copy
Authentication
The Cartesia plugin requires a
Cartesia API key
.
Set
CARTESIA_API_KEY
in your
.env
file.
Usage
Use Cartesia TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
cartesia
session
=
AgentSession
(
tts
=
cartesia
.
TTS
(
model
=
"sonic-english"
,
voice
=
"c2ac25f9-ecc4-4f56-9095-651354df60c0"
,
speed
=
0.8
,
emotion
=
[
"curiosity:highest"
,
"positivity:high"
]
,
)
# ... llm, stt, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
sonic
#
ID of the model to use for generation. See
supported models
.
voice
string | list[float]
Optional
Default:
c2ac25f9-ecc4-4f56-9095-651354df60c0
#
ID of the voice to use for generation, or an embedding array. See
official documentation
.
speed
string | float
Optional
Default:
1.0
#
Speed of generated speech. Either a float in range [-1.0, 1.0], or one of
"fastest"
,
"fast"
,
"normal"
,
"slow"
,
"slowest"
. See
speed options
.
emotion
list[string]
Optional
Default:
neutral
#
Emotion of generated speech. See
emotion options
.
language
string
Optional
Default:
en
#
Language of input text in
ISO-639-1
format. For a list of languages support by model, see
supported models
.
Customizing pronunciation
Cartesia TTS allows you to customize pronunciation using Speech Synthesis Markup Language (SSML). To learn more,
see
Specify Custom Pronunciations
.
Additional resources
The following resources provide more information about using Cartesia with LiveKit Agents.
Python package
The
livekit-plugins-cartesia
package on PyPI.
Plugin reference
Reference for the Cartesia TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Cartesia TTS plugin.
Cartesia docs
Cartesia TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and Cartesia TTS.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/tts/deepgram:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Prompting
Additional resources
Overview
Deepgram
provides responsive, human-like text-to-speech technology for voice AI. With LiveKit's Deepgram integration and the Agents framework, you can build voice AI agents that sound realistic.
Quick reference
This section provides a quick reference for the Deepgram TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[deepgram]~=1.0"
Copy
Authentication
The Deepgram plugin requires a
Deepgram API key
.
Set
DEEPGRAM_API_KEY
in your
.env
file.
Usage
Use Deepgram TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
deepgram
session
=
AgentSession
(
tts
=
deepgram
.
TTS
(
model
=
"aura-asteria-en"
,
)
# ... llm, stt, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
aura-asteria-en
#
ID of the model to use for generation. To learn more, see
supported models
.
Prompting
Deepgram supports filler words and natural pauses through prompting. To learn more, see
Text to Speech Prompting
.
Additional resources
The following resources provide more information about using Deepgram with LiveKit Agents.
Python package
The
livekit-plugins-deepgram
package on PyPI.
Plugin reference
Reference for the Deepgram TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Deepgram TTS plugin.
Deepgram docs
Deepgram's full docs site.
Voice AI quickstart
Get started with LiveKit Agents and Deepgram.
Deepgram STT
Guide to the Deepgram STT integration with LiveKit Agents.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Prompting
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/tts/elevenlabs:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources
Overview
ElevenLabs
provides an AI text-to-speech (TTS) service with thousands of human-like voices
across a number of different languages. With LiveKit's ElevenLabs integration and the Agents framework, you can build
voice AI applications that sound realistic.
Quick reference
This section provides a quick reference for the ElevenLabs TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[elevenlabs]~=1.0"
Copy
Authentication
The ElevenLabs plugin requires an
ElevenLabs API key
.
Set
ELEVEN_API_KEY
in your
.env
file.
Usage
Use ElevenLabs TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
elevenlabs
session
=
AgentSession
(
tts
=
elevenlabs
.
TTS
(
voice_id
=
"ODq5zmih8GrVes37Dizd"
,
model
=
"eleven_multilingual_v2"
)
# ... llm, stt, etc.
)
Copy
Parameters
This section describes some of the parameters you can set when you create an ElevenLabs TTS. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
eleven_flash_v2_5
#
ID of the model to use for generation. To learn more, see the
ElevenLabs documentation
.
voice_id
string
Optional
Default:
EXAVITQu4vr4xnSDxMaL
#
ID of the voice to use for generation. To learn more, see the
ElevenLabs documentation
.
voice_settings
VoiceSettings
Optional
#
Voice configuration. To learn more, see the
ElevenLabs documentation
.
stability
float
Optional
#
similarity_boost
float
Optional
#
style
float
Optional
#
use_speaker_boost
bool
Optional
#
speed
float
Optional
#
language
string
Optional
Default:
en
#
Language of output audio in
ISO-639-1
format. To learn more,
see the
ElevenLabs documentation
.
streaming_latency
int
Optional
Default:
3
#
Latency in seconds for streaming.
enable_ssml_parsing
bool
Optional
Default:
false
#
Enable Speech Synthesis Markup Language (SSML) parsing for input text. Set to
true
to
customize pronunciation
using SSML.
chunk_length_schedule
list[int]
Optional
Default:
[80, 120, 200, 260]
#
Schedule for chunk lengths. Valid values range from
50
to
500
.
Customizing pronunciation
ElevenLabs supports customizing pronunciation for specific words or phrases using SSML
phoneme
tags. This is useful to ensure correct pronunciation of certain words, even when missing from the voice's lexicon. To learn more, see
Pronunciation
.
Additional resources
The following resources provide more information about using ElevenLabs with LiveKit Agents.
Python package
The
livekit-plugins-elevenlabs
package on PyPI.
Plugin reference
Reference for the ElevenLabs TTS plugin.
GitHub repo
View the source or contribute to the LiveKit ElevenLabs TTS plugin.
ElevenLabs docs
ElevenLabs TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and ElevenLabs TTS.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/tts/neuphonic:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Overview
Neuphonic
provides hyper realistic realtime voice synthesis. You can use the Neuphonic TTS plugin for LiveKit Agents to build voice AI applications that sound realistic.
Quick reference
This section includes a brief overview of the Neuphonic TTS plugin. For more information, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[neuphonic]~=1.0"
Copy
Authentication
The Neuphonic plugin requires a
Neuphonic API key
.
Set
NEUPHONIC_API_TOKEN
in your
.env
file.
Usage
Use Neuphonic TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
neuphonic
session
=
AgentSession
(
tts
=
neuphonic
.
TTS
(
voice_id
=
"fc854436-2dac-4d21-aa69-ae17b54e98eb"
)
,
# ... llm, stt, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
voice_id
string
Required
#
ID of the voice to use for generation.
speed
float
Optional
Default:
1
#
Speed of generated speech.
model
string
Optional
Default:
neu_hq
#
ID of the model to use for generation.
lang_code
string
Optional
Default:
en
#
Language code for the generated speech.
Additional resources
The following resources provide more information about using Neuphonic with LiveKit Agents.
Python package
The
livekit-plugins-neuphonic
package on PyPI.
Plugin reference
Reference for the Neuphonic TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Neuphonic TTS plugin.
Neuphonic documentation
Neuphonic's full documentation.
Voice AI quickstart
Get started with LiveKit Agents and Neuphonic TTS.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/tts/playai:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources
Overview
PlayHT
provides realistic TTS voice generation. With LiveKit's PlayHT integration and the Agents
framework, you can build voice AI applications with fluent and conversational voices.
To learn more about TTS and generating agent speech, see
Agent speech
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[playai]~=1.0"
Copy
Authentication
The PlayHT plugin requires a
PlayHT API key
.
Set the following environment variables in your
.env
file:
PLAYHT_API_KEY
=
<
playht-api-key
>
PLAYHT_USER_ID
=
<
playht-user-id
>
Copy
Usage
Use PlayHT TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
playai
session
=
AgentSession
(
tts
=
playai
.
TTS
(
voice
=
"s3://voice-cloning-zero-shot/a59cb96d-bba8-4e24-81f2-e60b888a0275/charlottenarrativesaad/manifest.json"
,
language
=
"SPANISH"
,
model
=
"play3.0-mini"
,
)
,
# ... llm, stt, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
voice
string
Optional
Default:
s3://voice-cloning-zero-shot/d9ff78ba-d016-47f6-b0ef-dd630f59414e/female-cs/manifest.json
#
URL of the voice manifest file. For a full list, see
List of pre-built voices
.
model
TTSModel | string
Optional
Default:
Play3.0-mini
#
Name of the TTS model. For a full list, see
Models
.
language
string
Optional
Default:
ENGLISH
#
Language of the text to be spoken. For language support by model, see
Models
.
Customizing pronunciation
PlayHT TTS supports adding custom pronunciations to your speech-to-text conversions. To learn more,
see the
Add Custom Pronunciations to your Audio help article
.
Additional resources
The following resources provide more information about using PlayHT with LiveKit Agents.
Python package
The
livekit-plugins-playai
package on PyPI.
Plugin reference
Reference for the PlayHT TTS plugin.
GitHub repo
View the source or contribute to the LiveKit PlayHT TTS plugin.
PlayHT docs
PlayHT TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and PlayHT TTS.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/integrations/tts/rime:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources
Overview
Rime
provides text-to-speech synthesis (TTS) optimized for speed and quality. With LiveKit's Rime integration and the Agents framework, you can build voice AI applications that are responsive and sound realistic.
To learn more about TTS and generating agent speech, see
Agent speech
.
Quick reference
This section includes a basic usage example and some reference material. For links to more detailed documentation, see
Additional resources
.
Installation
Install the plugin from PyPI:
pip
install
"livekit-agents[rime]~=1.0"
Copy
Authentication
The Rime plugin requires a
Rime API key
.
Set
RIME_API_KEY
in your
.env
file.
Usage
Use Rime TTS within an
AgentSession
or as a standalone speech generator. For example,
you can use this TTS in the
Voice AI quickstart
.
from
livekit
.
plugins
import
rime
session
=
AgentSession
(
tts
=
rime
.
TTS
(
model
=
"mist"
,
speaker
=
"rainforest"
,
speed_alpha
=
0.9
,
reduce_latency
=
True
,
)
,
# ... llm, stt, etc.
)
Copy
Parameters
This section describes some of the available parameters. See the
plugin reference
for a complete list of all available parameters.
model
string
Optional
Default:
mist
#
ID of the model to use. To learn more, see
Models
.
speaker
string
Optional
Default:
lagoon
#
ID of the voice to use for speech generation. To learn more, see
Voices
.
audio_format
TTSEncoding
Optional
Default:
pcm
#
Audio format to use. Valid values are:
pcm
and
mp3
.
sample_rate
integer
Optional
Default:
16000
#
Sample rate of the generated audio. Set this rate to best match your application needs.
To learn more, see
Recommendations for reducing response time
.
speed_alpha
float
Optional
Default:
1.0
#
Adjusts the speed of speech. Lower than
1.0
results in faster speech; higher than
1.0
results in slower speech.
reduce_latency
boolean
Optional
Default:
false
#
When set to
true
, turns off text normalization to reduce the amount of time spent preparing input text for TTS inference. This
might result in the mispronunciation of digits and abbreviations.
To learn more, see
Recommendations for reducing response time
.
phonemize_between_brackets
boolean
Optional
Default:
false
#
When set to
true
, allows the use of custom pronunciation strings in text. To learn more, see
Custom pronunciation
.
api_key
string
Optional
Env:
RIME_API_KEY
#
Rime API Key. Required if the environment variable isn't set.
Customizing pronunciation
Rime TTS supports customizing pronunciation. To learn more, see
Custom Pronunciation guide
.
Additional resources
The following resources provide more information about using Rime with LiveKit Agents.
Python package
The
livekit-plugins-rime
package on PyPI.
Plugin reference
Reference for the Rime TTS plugin.
GitHub repo
View the source or contribute to the LiveKit Rime TTS plugin.
Rime docs
Rime TTS docs.
Voice AI quickstart
Get started with LiveKit Agents and Rime TTS.
On this page
Overview
Quick reference
Installation
Authentication
Usage
Parameters
Customizing pronunciation
Additional resources
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/tracks/publish:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Device permissions
Mute and unmute
Track permissions
Publishing from backend
Audio and video synchronization
Overview
LiveKit includes a simple and consistent method to publish the user's camera and microphone, regardless of the device or browser they are using. In all cases, LiveKit displays the correct indicators when recording is active and acquires the necessary permissions from the user.
// Enables the camera and publishes it to a new video track
room
.
localParticipant
.
setCameraEnabled
(
true
)
;
// Enables the microphone and publishes it to a new audio track
room
.
localParticipant
.
setMicrophoneEnabled
(
true
)
;
Copy
Device permissions
In native and mobile apps, you typically need to acquire consent from the user to access the microphone or camera. LiveKit integrates with the system privacy settings to record permission and display the correct indicators when audio or video capture is active.
For web browsers, the user is automatically prompted to grant camera and microphone permissions the first time your app attempts to access them and no additional configuration is required.
Swift
Android
React Native
Flutter
Add these entries to your
Info.plist
:
<
key
>
NSCameraUsageDescription
</
key
>
<
string
>
$(PRODUCT_NAME) uses your camera
</
string
>
<
key
>
NSMicrophoneUsageDescription
</
key
>
<
string
>
$(PRODUCT_NAME) uses your microphone
</
string
>
Copy
To enable background audio, you must also add the "Background Modes" capability with "Audio, AirPlay, and Picture in Picture" selected.
Your
Info.plist
should have:
<
key
>
UIBackgroundModes
</
key
>
<
array
>
<
string
>
audio
</
string
>
</
array
>
Copy
Mute and unmute
You can mute any track to stop it from sending data to the server. When a track is muted, LiveKit will trigger a
TrackMuted
event on all participants in the room. You can use this event to update your app's UI and reflect the correct state to all users in the room.
Mute/unmute a track using its corresponding
LocalTrackPublication
object.
Track permissions
By default, any published track can be subscribed to by all participants. However, publishers can restrict who can subscribe to their tracks using Track Subscription Permissions:
JavaScript
Swift
Android
Flutter
Python
localParticipant
.
setTrackSubscriptionPermissions
(
false
,
[
{
participantIdentity
:
'allowed-identity'
,
allowAll
:
true
,
}
,
]
)
;
Copy
Publishing from backend
You may also publish audio/video tracks from a backend process, which can be consumed just like any camera or microphone track. The
LiveKit Agents
framework makes it easy to add a programmable participant to any room, and publish media such as synthesized speech or video.
LiveKit also includes complete SDKs for server environments in
Go
,
Rust
,
Python
, and
Node.js
.
You can also publish media using the
LiveKit CLI
.
Audio and video synchronization
Note
AVSynchronizer
is currently only available in Python.
While WebRTC handles A/V sync natively, some scenarios require manual synchronization - for example, when synchronizing generated video with voice output.
The
AVSynchronizer
utility helps maintain sync by aligning the first audio and video frames. Subsequent frames are automatically synchronized based on configured video FPS and audio sample rate.
For implementation examples, see the
video stream examples
in our GitHub repository.
On this page
Overview
Device permissions
Mute and unmute
Track permissions
Publishing from backend
Audio and video synchronization
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/tracks/screenshare:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Sharing browser audio
Testing audio sharing
Overview
LiveKit supports screen sharing natively across all platforms. Your screen is published as a video track, just like your camera. Some platforms support local audio sharing as well.
The steps are somewhat different for each platform:
JavaScript
Swift
Android
Flutter
Unity (WebGL)
// The browser will prompt the user for access and offer a choice of screen, window, or tab
await
room
.
localParticipant
.
setScreenShareEnabled
(
true
)
;
Copy
Sharing browser audio
Note
Audio sharing is only possible in certain browsers. Check browser support on the
MDN compatibility table
.
To share audio from a browser tab, you can use the
createScreenTracks
method with the audio option enabled:
const
tracks
=
await
localParticipant
.
createScreenTracks
(
{
audio
:
true
,
}
)
;
tracks
.
forEach
(
(
track
)
=>
{
localParticipant
.
publishTrack
(
track
)
;
}
)
;
Copy
Testing audio sharing
Publisher
When sharing audio, make sure you select a
Browser Tab
(not a Window) and ☑️ Share tab audio, otherwise no audio track will be generated when calling
createScreenTracks
:
Subscriber
On the receiving side, you can use
RoomAudioRenderer
to play all audio tracks of the room automatically,
AudioTrack
or your own custom
<audio>
tag to add the track to the page. If you don't hear any sound, check you're receiving the track from the server:
JavaScript
room
.
getParticipantByIdentity
(
'<participant_id>'
)
.
getTrackPublication
(
'screen_share_audio'
)
;
Copy
On this page
Overview
Sharing browser audio
Testing audio sharing
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/tracks/subscribe:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Track subscription
Media playback
Active speaker identification
Selective subscription
From frontend
From server API
Adaptive stream
Enabling/disabling tracks
Simulcast controls
Overview
While connected to a room, a participant can receive and render any tracks published to the room. When
autoSubscribe
is enabled (default), the server automatically delivers new tracks to participants, making them ready for rendering.
Track subscription
Rendering media tracks starts with a subscription to receive the track data from the server.
As mentioned in the guide on
rooms, participants, and tracks
, LiveKit models tracks with two constructs:
TrackPublication
and
Track
. Think of a
TrackPublication
as metadata for a track registered with the server and
Track
as the raw media stream. Track publications are always available to the client, even when the track is not subscribed to.
Track subscription callbacks provide your app with both the
Track
and
TrackPublication
objects.
Subscribed callback will be fired on both
Room
and
RemoteParticipant
objects.
JavaScript
React
Swift
Android
Flutter
Python
Rust
Unity
import
{
connect
,
RoomEvent
}
from
'livekit-client'
;
room
.
on
(
RoomEvent
.
TrackSubscribed
,
handleTrackSubscribed
)
;
function
handleTrackSubscribed
(
track
:
RemoteTrack
,
publication
:
RemoteTrackPublication
,
participant
:
RemoteParticipant
,
)
{
/* Do things with track, publication or participant */
}
Copy
Note
This guide is focused on frontend applications. To consume media in your backend, use the
LiveKit Agents framework
or SDKs for
Go
,
Rust
,
Python
, or
Node.js
.
Media playback
Once subscribed to an audio or video track, it's ready to be played in your application
JavaScript
React
React Native
Swift
Android
Flutter
Unity (WebGL)
function
handleTrackSubscribed
(
track
:
RemoteTrack
,
publication
:
RemoteTrackPublication
,
participant
:
RemoteParticipant
,
)
{
// Attach track to a new HTMLVideoElement or HTMLAudioElement
const
element
=
track
.
attach
(
)
;
parentElement
.
appendChild
(
element
)
;
// Or attach to existing element
// track.attach(element)
}
Copy
Active speaker identification
LiveKit can automatically detect participants who are actively speaking and send updates when their speaking status changes. Speaker updates are sent for both local and remote participants. These events fire on both Room and Participant objects, allowing you to identify active speakers in your UI.
JavaScript
React
React Native
Swift
Android
Flutter
Unity (WebGL)
room
.
on
(
RoomEvent
.
ActiveSpeakersChanged
,
(
speakers
:
Participant
[
]
)
=>
{
// Speakers contain all of the current active speakers
}
)
;
participant
.
on
(
ParticipantEvent
.
IsSpeakingChanged
,
(
speaking
:
boolean
)
=>
{
console
.
log
(
`
${
participant
.
identity
}
is
${
speaking
?
'now'
:
'no longer'
}
speaking. audio level:
${
participant
.
audioLevel
}
`
,
)
;
}
)
;
Copy
Selective subscription
Disable
autoSubscribe
to take manual control over which tracks the participant should subscribe to. This is appropriate for spatial applications and/or applications that require precise control over what each participant receives.
Both LiveKit's SDKs and server APIs have controls for selective subscription. Once configured, only explicitly subscribed tracks are delivered to the participant.
From frontend
JavaScript
Swift
Android
Flutter
Python
Unity (WebGL)
let
room
=
await
room
.
connect
(
url
,
token
,
{
autoSubscribe
:
false
,
}
)
;
room
.
on
(
RoomEvent
.
TrackPublished
,
(
publication
,
participant
)
=>
{
publication
.
setSubscribed
(
true
)
;
}
)
;
// Also subscribe to tracks published before participant joined
room
.
remoteParticipants
.
forEach
(
(
participant
)
=>
{
participant
.
trackPublications
.
forEach
(
(
publication
)
=>
{
publication
.
setSubscribed
(
true
)
;
}
)
;
}
)
;
Copy
From server API
These controls are also available with the server APIs.
Node.js
Go
import
{
RoomServiceClient
}
from
'livekit-server-sdk'
;
const
roomServiceClient
=
new
RoomServiceClient
(
'myhost'
,
'api-key'
,
'my secret'
)
;
// Subscribe to new track
roomServiceClient
.
updateSubscriptions
(
'myroom'
,
'receiving-participant-identity'
,
[
'TR_TRACKID'
]
,
true
)
;
// Unsubscribe from existing track
roomServiceClient
.
updateSubscriptions
(
'myroom'
,
'receiving-participant-identity'
,
[
'TR_TRACKID'
]
,
false
)
;
Copy
Adaptive stream
In an application, video elements where tracks are rendered could vary in size, and sometimes hidden. It would be extremely wasteful to fetch high-resolution videos but only to render it in a 150x150 box.
Adaptive stream allows a developer to build dynamic video applications without consternation for how interface design or user interaction might impact video quality. It allows us to fetch the minimum bits necessary for high-quality rendering and helps with scaling to very large sessions.
When adaptive stream is enabled, the LiveKit SDK will monitor both size and visibility of the UI elements that the tracks are attached to. Then it'll automatically coordinate with the server to ensure the closest-matching simulcast layer that matches the UI element is sent back. If the element is hidden, the SDK will automatically pause the associated track on the server side until the element becomes visible.
Note
With JS SDK, you must use
Track.attach()
in order for adaptive stream to be effective.
Enabling/disabling tracks
Implementations seeking fine-grained control can enable or disable tracks at their discretion. This could be used to implement subscriber-side mute. (for example, muting a publisher in the room, but only for the current user).
When disabled, the participant will not receive any new data for that track. If a disabled track is subsequently enabled, new data will be received again.
The
disable
action is useful when optimizing for a participant's bandwidth consumption. For example, if a particular user's video track is offscreen, disabling this track will reduce bytes from being sent by the LiveKit server until the track's data is needed again. (this is not needed with adaptive stream)
JavaScript
Swift
Android
Flutter
Unity (WebGL)
import
{
connect
,
RoomEvent
}
from
'livekit-client'
;
room
.
on
(
RoomEvent
.
TrackSubscribed
,
handleTrackSubscribed
)
;
function
handleTrackSubscribed
(
track
:
RemoteTrack
,
publication
:
RemoteTrackPublication
,
participant
:
RemoteParticipant
,
)
{
publication
.
setEnabled
(
false
)
;
}
Copy
Note
You may be wondering how
subscribe
and
unsubscribe
differs from
enable
and
disable
. A track must be subscribed to and enabled for data to be received by the participant. If a track has not been subscribed to (or was unsubscribed) or disabled, the participant performing these actions will not receive that track's data.
The difference between these two actions is
negotiation
. Subscribing requires a negotiation handshake with the LiveKit server, while enable/disable does not. Depending on one's use case, this can make enable/disable more efficient, especially when a track may be turned on or off frequently.
Simulcast controls
If a video track has simulcast enabled, a receiving participant may want to manually specify the maximum receivable quality. This would result a quality and bandwidth reduction for the target track. This might come in handy, for instance, when an application's user interface is displaying a small thumbnail for a particular user's video track.
JavaScript
Swift
Android
Flutter
Unity (WebGL)
import
{
connect
,
RoomEvent
}
from
'livekit-client'
;
connect
(
'ws://your_host'
,
token
,
{
audio
:
true
,
video
:
true
,
}
)
.
then
(
(
room
)
=>
{
room
.
on
(
RoomEvent
.
TrackSubscribed
,
handleTrackSubscribed
)
;
}
)
;
function
handleTrackSubscribed
(
track
:
RemoteTrack
,
publication
:
RemoteTrackPublication
,
participant
:
RemoteParticipant
,
)
{
if
(
track
.
kind
===
Track
.
Kind
.
Video
)
{
publication
.
setVideoQuality
(
VideoQuality
.
LOW
)
;
}
}
Copy
On this page
Overview
Track subscription
Media playback
Active speaker identification
Selective subscription
From frontend
From server API
Adaptive stream
Enabling/disabling tracks
Simulcast controls
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/tracks/noise-cancellation:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Overview
Your user's microphone is likely to pick up undesirable audio including background noise (like traffic, music, voices, etc) and might
also pick up echoes from their own speakers. In both cases, this noise leads to a poor experience for other participants in a call.
In voice AI apps, this can also interfere with turn detection or degrade the quality of transcriptions, both of which are critical to a
good user experience.
LiveKit includes default outbound noise and echo cancellation based on the underlying open source WebRTC implementations of
echoCancellation
and
noiseSuppression
. You can adjust these settings
with the
AudioCaptureOptions
type in the LiveKit SDKs during connection.
LiveKit also offers an
enhanced noise cancellation
feature to all LiveKit Cloud customers at no additional charge for the most effective solution.
To hear the effects of the various noise removal options, play the samples below:
Original
WebRTC noiseSuppression
LiveKit Cloud enhanced noise cancellation
On this page
Overview
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/tracks/encryption:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
How E2EE works
Key distribution
Limitations
Implementation guide
Using a custom key provider
Overview
LiveKit includes built-in support for end-to-end encryption (E2EE) on realtime audio and video tracks. With E2EE enabled, media data remains fully encrypted from sender to receiver, ensuring that no intermediaries (including LiveKit servers) can access or modify the content. This feature is:
Available for both self-hosted and LiveKit Cloud customers at no additional cost.
Ideal for regulated industries and security-critical applications.
Designed to provide an additional layer of protection beyond standard transport encryption.
Note
Security is our highest priority. Learn more about
our comprehensive approach to security
.
How E2EE works
E2EE is enabled at the room level and automatically applied to all media tracks from all participants in that room. You must enable it within the LiveKit SDK for each participant. In many cases you can use a built-in key provider with a single shared key for the whole room. If you require unique keys for each participant, or key rotation during the lifetime of a single room, you can implement your own key provider.
Key distribution
It is your responsibility to securely generate, store, and distribute encryption keys to your application at runtime. LiveKit does not (and cannot) store or transport encryption keys for you.
If using a shared key, you would typically generate it on your server at the same time that you create a room and distribute it securely to participants alongside their access token for the room. When using unique keys per participant, you may need a more sophisticated method for distributing keys as new participants join the room. Remember that the key is needed for both encryption and decryption, so even when using per-participant keys, you must ensure that all participants have all keys.
Limitations
All LiveKit network traffic is encrypted using TLS, but full end-to-end encryption applies only to media tracks and is not applied to realtime data, text, API calls, or other signaling.
Implementation guide
These examples show how to use the built-in key provider with a shared key. If you need to use a custom key provider, see the section below.
JavaScript
iOS
Android
Flutter
React Native
Python
Node.js
// 1. Initialize the external key provider
const
keyProvider
=
new
ExternalE2EEKeyProvider
(
)
;
// 2. Configure room options
const
roomOptions
:
RoomOptions
=
{
e2ee
:
{
keyProvider
:
keyProvider
,
// Required for web implementations
worker
:
new
Worker
(
new
URL
(
'livekit-client/e2ee-worker'
,
import
.
meta
.
url
)
)
,
}
,
}
;
// 3. Create and configure the room
const
room
=
new
Room
(
roomOptions
)
;
// 4. Set your externally distributed encryption key
await
keyProvider
.
setKey
(
yourSecureKey
)
;
// 5. Enable E2EE for all local tracks
await
room
.
setE2EEEnabled
(
true
)
;
// 6. Connect to the room
await
room
.
connect
(
url
,
token
)
;
Copy
Example implementation
For a production-ready implementation, refer to our
Meet example app
which demonstrates E2EE in a production-grade application using the
ExternalE2EEKeyProvider
.
Using a custom key provider
If your application requires key rotation during the lifetime of a single room or unique keys per participant (such as when implementing the
MEGOLM
or
MLS
protocol), you'll need to implement your own key provider.  The full details of that are beyond the scope of this guide, but a brief outline for the JS SDK is provided below (the process is similar in the other SDKs as well):
Extend the
BaseKeyProvider
class.
Call
onSetEncryptionKey
with each key/identity pair
Set appropriate ratcheting options (
ratchetSalt
,
ratchetWindowSize
,
failureTolerance
,
keyringSize
).
Implement the
onKeyRatcheted
method to handle key updates.
Call
ratchetKey()
when key rotation is needed.
Pass your custom key provider in the room options, in place of the built-in key provider.
On this page
Overview
How E2EE works
Key distribution
Limitations
Implementation guide
Using a custom key provider
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/tracks/advanced:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Video codec support
Video quality presets
Video track configuration
Video simulcast
Dynacast
Hi-fi audio
Audio RED
Video codec support
LiveKit supports multiple video codecs to suit different application needs:
H.264
VP8
VP9 (including SVC)
AV1 (including SVC)
Scalable Video Coding (SVC) is a feature of newer codecs like VP9 and AV1 that provides the following benefits:
Improves bitrate efficiency by letting higher quality layers leverage information from lower quality layers.
Enables instant layer switching without waiting for keyframes.
Incorporates multiple spatial (resolution) and temporal (frame rate) layers in a single stream.
When using VP9 or AV1, SVC is automatically activated with L3T3_KEY
scalabilityMode
(three spatial and temporal layers).
You can specify which codec to use when connecting to a room. To learn more, see the examples in the following sections.
Video quality presets
LiveKit provides preset resolutions when creating video tracks. These presets include common resolutions and aspect ratios:
h720 (1280x720)
h540 (960x540)
h360 (640x360)
h180 (320x180)
The presets also include recommended bitrates and framerates for optimal quality. You can use these presets or define custom parameters based on your needs.
React
JavaScript
const
localParticipant
=
useLocalParticipant
(
)
;
const
audioTrack
=
await
createLocalAudioTrack
(
)
;
const
audioPublication
=
await
localParticipant
.
publishTrack
(
audioTrack
,
{
red
:
false
,
}
)
;
Copy
Video track configuration
LiveKit provides extensive control over video track settings through two categories:
Capture settings: Device selection and capabilities (resolution, framerate, facing mode).
Publish settings: Encoding parameters (bitrate, framerate, simulcast layers).
Here's how to configure these settings:
JavaScript
Swift
// Room defaults
const
room
=
new
Room
(
{
videoCaptureDefaults
:
{
deviceId
:
''
,
facingMode
:
'user'
,
resolution
:
{
width
:
1280
,
height
:
720
,
frameRate
:
30
,
}
,
}
,
publishDefaults
:
{
videoEncoding
:
{
maxBitrate
:
1_500_000
,
maxFramerate
:
30
,
}
,
videoSimulcastLayers
:
[
{
width
:
640
,
height
:
360
,
encoding
:
{
maxBitrate
:
500_000
,
maxFramerate
:
20
,
}
,
}
,
{
width
:
320
,
height
:
180
,
encoding
:
{
maxBitrate
:
150_000
,
maxFramerate
:
15
,
}
,
}
,
]
,
}
,
}
)
;
// Individual track settings
const
videoTrack
=
await
createLocalVideoTrack
(
{
facingMode
:
'user'
,
resolution
:
VideoPresets
.
h720
,
}
)
;
const
publication
=
await
room
.
localParticipant
.
publishTrack
(
videoTrack
)
;
Copy
Video simulcast
Simulcast enables publishing multiple versions of the same video track with different bitrate profiles. This allows LiveKit to dynamically forward the most suitable stream based on each recipient's bandwidth and preferred resolution.
LiveKit will automatically select appropriate layers when it detects bandwidth constraints, upgrading to higher resolutions as conditions improve.
Simulcast is enabled by default in all LiveKit SDKs and can be disabled in publish settings if needed.
Dynacast
Dynamic broadcasting (Dynacast) automatically pauses video layer publication when they aren't being consumed by subscribers. For simulcasted video, if subscribers only use medium and low-resolution layers, the high-resolution publication is paused.
To enable this bandwidth optimization:
JavaScript
Swift
Android
Flutter
const
room
=
new
Room
(
{
dynacast
:
true
}
)
;
Copy
With SVC codecs (VP9 and AV1), Dynacast can only pause entire streams, not individual layers, due to SVC encoding characteristics.
Hi-fi audio
For high-quality audio streaming, LiveKit provides several configuration options to optimize audio quality.
Recommended hi-fi settings
For high-quality audio, we provide a preset with our recommended settings:
React
JavaScript
const
localParticipant
=
useLocalParticipant
(
)
;
const
audioTrack
=
await
createLocalAudioTrack
(
{
channelCount
:
2
,
echoCancellation
:
false
,
noiseSuppression
:
false
,
}
)
;
const
audioPublication
=
await
localParticipant
.
publishTrack
(
audioTrack
,
{
audioPreset
:
AudioPresets
.
musicHighQualityStereo
,
dtx
:
false
,
red
:
false
,
}
)
;
Copy
Maximum quality settings
LiveKit supports audio tracks up to 510kbps stereo - the highest theoretical quality possible. Note that the listener's playback stack may resample the audio, so actual playback quality may be lower than published quality. For comparison, 256kbps AAC-encoded audio is considered high quality for music streaming services like Spotify.
React
JavaScript
const
localParticipant
=
useLocalParticipant
(
)
;
const
audioTrack
=
await
createLocalAudioTrack
(
{
channelCount
:
2
,
echoCancellation
:
false
,
noiseSuppression
:
false
,
}
)
;
const
audioPublication
=
await
localParticipant
.
publishTrack
(
audioTrack
,
{
audioBitrate
:
510000
,
dtx
:
false
,
red
:
false
,
}
)
;
Copy
If you configure a high bitrate, we recommend testing under real-world conditions to find what settings work best for your use case.
Audio RED
REDundant Encoding is a technique to improve audio quality by sending multiple copies of the same audio data in different packets. This is useful in lossy networks where packets may be dropped. The receiver can then use the redundant packets to reconstruct the original audio packet.
Redundant encoding increases bandwidth usage in order to achieve higher audio quality. LiveKit recommends enabling this feature because audio glitches are so distracting that the tradeoff is almost always worth it. If your use case prioritizes bandwidth and can tolerate audio glitches, you can disable RED.
Disabling Audio RED when publishing
You can disable Audio RED when publishing new audio tracks:
React
JavaScript
Swift
Android
const
localParticipant
=
useLocalParticipant
(
)
;
const
audioTrack
=
await
createLocalAudioTrack
(
)
;
const
audioPublication
=
await
localParticipant
.
publishTrack
(
audioTrack
,
{
red
:
false
,
}
)
;
Copy
On this page
Video codec support
Video quality presets
Video track configuration
Video simulcast
Dynacast
Hi-fi audio
Audio RED
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/data/text-streams:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Sending all at once
Streaming incrementally
Handling incoming streams
Stream properties
Concurrency
Joining mid-stream
No message persistence
Chat components
Overview
Text streams provide a simple way to send text between participants in realtime, supporting use cases such as chat, streamed LLM responses, and more. Each individual stream is associated with a topic, and you must register a handler to receive incoming streams for that topic. Streams can target specific participants or the entire room.
To send other kinds of data, use
byte streams
instead.
Sending all at once
Use the
sendText
method when the whole string is available up front. The input string is automatically chunked and streamed so there is no limit on string size.
JavaScript
Swift
Python
Node.js
Go
const
text
=
'Lorem ipsum dolor sit amet...'
;
const
info
=
await
room
.
localParticipant
.
sendText
(
text
,
{
topic
:
'my-topic'
,
}
)
;
console
.
log
(
`
Sent text with stream ID:
${
info
.
id
}
`
)
;
Copy
Streaming incrementally
If your text is generated incrementally, use
streamText
to open a stream writer. You must explicitly close the stream when you are done sending data.
JavaScript
Swift
Python
Node.js
Go
const
streamWriter
=
await
room
.
localParticipant
.
streamText
(
{
topic
:
'my-topic'
,
}
)
;
console
.
log
(
`
Opened text stream with ID:
${
streamWriter
.
info
.
id
}
`
)
;
// In a real app, you would generate this text asynchronously / incrementally as well
const
textChunks
=
[
"Lorem "
,
"ipsum "
,
"dolor "
,
"sit "
,
"amet..."
]
for
(
const
chunk
of
textChunks
)
{
await
streamWriter
.
write
(
chunk
)
}
// The stream must be explicitly closed when done
await
streamWriter
.
close
(
)
;
console
.
log
(
`
Closed text stream with ID:
${
streamWriter
.
info
.
id
}
`
)
;
Copy
Handling incoming streams
Whether the data was sent with
sendText
or
streamText
, it is always received as a stream. You must register a handler to receive it.
JavaScript
Swift
Python
Node.js
Go
room
.
registerTextStreamHandler
(
'my-topic'
,
(
reader
,
participantInfo
)
=>
{
const
info
=
reader
.
info
;
console
.
log
(
`
Received text stream from
${
participantInfo
.
identity
}
\n
`
+
`
Topic:
${
info
.
topic
}
\n
`
+
`
Timestamp:
${
info
.
timestamp
}
\n
`
+
`
ID:
${
info
.
id
}
\n
`
+
`
Size:
${
info
.
size
}
`
// Optional, only available if the stream was sent with `sendText`
)
;
// Option 1: Process the stream incrementally using a for-await loop.
for
await
(
const
chunk
of
reader
)
{
console
.
log
(
`
Next chunk:
${
chunk
}
`
)
;
}
// Option 2: Get the entire text after the stream completes.
const
text
=
await
reader
.
readAll
(
)
;
console
.
log
(
`
Received text:
${
text
}
`
)
;
}
)
;
Copy
Stream properties
These are all of the properties available on a text stream, and can be set from the send/stream methods or read from the handler.
Property
Description
Type
id
Unique identifier for this stream.
string
topic
Topic name used to route the stream to the appropriate handler.
string
timestamp
When the stream was created.
number
size
Total expected size in bytes (UTF-8), if known.
number
attributes
Additional attributes as needed for your application.
string dict
destinationIdentities
Identities of the participants to send the stream to. If empty, is sent to all.
array
Concurrency
Multiple streams can be written or read concurrently. If you call
sendText
or
streamText
multiple times on the same topic, the recipient's handler will be invoked multiple times, once for each stream. These invocations will occur in the same order as the streams were opened by the sender, and the stream readers will be closed in the same order in which the streams were closed by the sender.
Joining mid-stream
Participants who join a room after a stream has been initiated will not receive any of it. Only participants connected at the time the stream is opened are eligible to receive it.
No message persistence
LiveKit does not include long-term persistence for text streams. All data is transmitted in real-time between connected participants only. If you need message history, you'll need to implement storage yourself using a database or other persistence layer.
Chat components
LiveKit provides pre-built React components for common text streaming use cases like chat. For details, see the
Chat component
and
useChat hook
.
Note
Streams are a simple and powerful way to send text, but if you need precise control over individual packet behavior, the lower-level
data packets
API may be more appropriate.
On this page
Overview
Sending all at once
Streaming incrementally
Handling incoming streams
Stream properties
Concurrency
Joining mid-stream
No message persistence
Chat components
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/data/byte-streams:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Sending files
Streaming bytes
Handling incoming streams
Stream properties
Concurrency
Joining mid-stream
Chunk sizes
Overview
Byte streams provide a simple way to send files, images, or other binary data between participants in realtime. Each individual stream is associated with a topic, and you must register a handler to receive incoming streams for that topic. Streams can target specific participants or the entire room.
To send text data, use
text streams
instead.
Sending files
To send a file or an image, use the
sendFile
method. Precise support varies by SDK, as this is integrated with the platform's own file APIs.
JavaScript
Swift
Python
Node.js
Go
// Send a `File` object
const
file
=
(
$
(
'file'
)
as
HTMLInputElement
)
.
files
?.
[
0
]
!
;
const
info
=
await
room
.
localParticipant
.
sendFile
(
file
,
{
mimeType
:
file
.
type
,
topic
:
'my-topic'
,
// Optional, allows progress to be shown to the user
onProgress
:
(
progress
)
=>
console
.
log
(
'sending file, progress'
,
Math
.
ceil
(
progress
*
100
)
)
,
}
)
;
console
.
log
(
`
Sent file with stream ID:
${
info
.
id
}
`
)
;
Copy
Streaming bytes
To stream any kind of binary data, open a stream writer with the
streamBytes
method. You must explicitly close the stream when you are done sending data.
Swift
Python
Node.js
Go
let
writer
=
try
await
room
.
localParticipant
.
streamBytes
(
for
:
"my-topic"
)
print
(
"Opened byte stream with ID:
\(
writer
.
info
.
id
)
"
)
// Example sending arbitrary binary data
// For sending files, use `sendFile` instead
let
dataChunks
=
[
Data
(
[
0x00
,
0x01
]
)
,
Data
(
[
0x03
,
0x04
]
)
]
for
chunk
in
dataChunks
{
try
await
writer
.
write
(
chunk
)
}
// The stream must be explicitly closed when done
try
await
writer
.
close
(
)
print
(
"Closed byte stream with ID:
\(
writer
.
info
.
id
)
"
)
Copy
Handling incoming streams
Whether the data was sent as a file or a stream, it is always received as a stream. You must register a handler to receive it.
JavaScript
Swift
Python
Node.js
Go
room
.
registerByteStreamHandler
(
'my-topic'
,
(
reader
,
participantInfo
)
=>
{
const
info
=
reader
.
info
;
// Optional, allows you to display progress information if the stream was sent with `sendFile`
reader
.
onProgress
=
(
progress
)
=>
{
console
.
log
(
`
"progress
${
progress
?
(
progress
*
100
)
.
toFixed
(
0
)
:
'undefined'
}
%
`
)
;
}
;
// Option 1: Process the stream incrementally using a for-await loop.
for
await
(
const
chunk
of
reader
)
{
// Collect these however you want.
console
.
log
(
`
Next chunk:
${
chunk
}
`
)
;
}
// Option 2: Get the entire file after the stream completes.
const
result
=
new
Blob
(
await
reader
.
readAll
(
)
,
{
type
:
info
.
mimeType
}
)
;
console
.
log
(
`
File "
${
info
.
name
}
" received from
${
participantInfo
.
identity
}
\n
`
+
`
Topic:
${
info
.
topic
}
\n
`
+
`
Timestamp:
${
info
.
timestamp
}
\n
`
+
`
ID:
${
info
.
id
}
\n
`
+
`
Size:
${
info
.
size
}
`
// Optional, only available if the stream was sent with `sendFile`
)
;
}
)
;
Copy
Stream properties
These are all of the properties available on a text stream, and can be set from the send/stream methods or read from the handler.
Property
Description
Type
id
Unique identifier for this stream.
string
topic
Topic name used to route the stream to the appropriate handler.
string
timestamp
When the stream was created.
number
mimeType
The MIME type of the stream data. Auto-detected for files, otherwise defaults to
application/octet-stream
.
string
name
The name of the file being sent.
string
size
Total expected size in bytes, if known.
number
attributes
Additional attributes as needed for your application.
string dict
destinationIdentities
Identities of the participants to send the stream to. If empty, will be sent to all.
array
Concurrency
Multiple streams can be written or read concurrently. If you call
sendFile
or
streamBytes
multiple times on the same topic, the recipient's handler will be invoked multiple times, once for each stream. These invocations will occur in the same order as the streams were opened by the sender, and the stream readers will be closed in the same order in which the streams were closed by the sender.
Joining mid-stream
Participants who join a room after a stream has been initiated will not receive any of it. Only participants connected at the time the stream is opened are eligible to receive it.
Chunk sizes
The processes for writing and reading streams are optimized separately. This means the number and size of chunks sent may not match the number and size of those received. However, the full data received is guaranteed to be complete and in order. Chunks are generally smaller than 15kB.
Note
Streams are a simple and powerful way to send data, but if you need precise control over individual packet behavior, the lower-level
data packets
API may be more appropriate.
On this page
Overview
Sending files
Streaming bytes
Handling incoming streams
Stream properties
Concurrency
Joining mid-stream
Chunk sizes
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/data/rpc:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Method registration
Method invocation
Method names
Payload format
Response timeout
Errors
Overview
With RPC your application can define methods on one participant that can be invoked remotely by other participants within a room, and may return a response. This feature can be used to request data, coordinate application-specific state, and more. When used to
forward tool calls
from an AI Agent, your LLM can directly access data or manipulate UI in your application's frontend.
Method registration
First register the method at the destination participant with
localParticipant.registerRpcMethod
and provide the method's name and a handler function.  Any number of methods can be registered on a single participant.
JavaScript
Python
Node.js
Rust
Android
Swift
Go
localParticipant
.
registerRpcMethod
(
'greet'
,
async
(
data
:
RpcInvocationData
)
=>
{
console
.
log
(
`
Received greeting from
${
data
.
callerIdentity
}
:
${
data
.
payload
}
`
)
;
return
`
Hello,
${
data
.
callerIdentity
}
!
`
;
}
)
;
Copy
Method invocation
Use
localParticipant.performRpc
to invoke the registered RPC method on a remote participant by providing the destination participant's identity, the method name, and the payload. This is an asynchronous operation that returns a string, and may raise an error.
JavaScript
Python
Node.js
Rust
Android
Swift
Go
try
{
const
response
=
await
localParticipant
.
performRpc
(
{
destinationIdentity
:
'recipient-identity'
,
method
:
'greet'
,
payload
:
'Hello from RPC!'
,
}
)
;
console
.
log
(
'RPC response:'
,
response
)
;
}
catch
(
error
)
{
console
.
error
(
'RPC call failed:'
,
error
)
;
}
Copy
Method names
Method names can be any string, up to 64 bytes long (UTF-8).
Payload format
RPC requests and responses both support a string payload, with a maximum size of 15KiB (UTF-8). You may use any format that makes sense, such as JSON or base64-encoded data.
Response timeout
performRpc
uses a timeout to hang up automatically if the response takes too long. The default timeout is 10 seconds, but you are free to change it as needed in your
performRpc
call. In general, you should set a timeout that is as short as possible while still satisfying your use case.
The timeout you set is used for the entire duration of the request, including network latency. This means the timeout the handler is provided will be shorter than the overall timeout.
Errors
performRpc
will return certain built-in errors (detailed below), or your own custom errors generated in your remote method handler.
To return a custom error to the caller, handlers should throw an error of the type
RpcError
with the following properties:
code
: A number that indicates the type of error. Codes 1001-1999 are reserved for LiveKit internal errors.
message
: A string that provides a readable description of the error.
data
: An optional string that provides even more context about the error, with the same format and limitations as request/response payloads.
Any other error thrown in a handler will be caught and the caller will receive a generic
1500 Application Error
.
Built-in error types
Code
Name
Description
1400
UNSUPPORTED_METHOD
Method not supported at destination
1401
RECIPIENT_NOT_FOUND
Recipient not found
1402
REQUEST_PAYLOAD_TOO_LARGE
Request payload too large
1403
UNSUPPORTED_SERVER
RPC not supported by server
1404
UNSUPPORTED_VERSION
Unsupported RPC version
1500
APPLICATION_ERROR
Application error in method handler
1501
CONNECTION_TIMEOUT
Connection timeout
1502
RESPONSE_TIMEOUT
Response timeout
1503
RECIPIENT_DISCONNECTED
Recipient disconnected
1504
RESPONSE_PAYLOAD_TOO_LARGE
Response payload too large
1505
SEND_FAILED
Failed to send
On this page
Overview
Method registration
Method invocation
Method names
Payload format
Response timeout
Errors
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/data/packets:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Delivery options
Size limits
Selective delivery
Topic
Usage
Overview
Use
LocalParticipant.publishData
or
RoomService.SendData
to send individual packets of data to one or more participants in a room.
Note
This is a low-level API meant for advanced control over individual packet behavior. For most use cases, consider using the higher-level
text streams
,
byte streams
, or
RPC
features.
Delivery options
LiveKit offers two forms of packet delivery:
Reliable
: Packets are delivered in order, with automatic retransmission in the case of packet loss. This is preferable for scenarios where delivery is prioritized over latency, such as in-room chat.
Lossy
: Each packet is sent once, with no ordering guarantee. This is ideal for realtime updates where speed of delivery is a priority.
Note
Reliable delivery indicates "best-effort" delivery. It cannot fully guarantee the packet will be delivered in all cases. For instance, a receiver that is temporarily disconnected at the moment the packet is sent will not receive it. Packets are not buffered on the server and only a limited number of retransmissions are attempted.
Size limits
In the
reliable
delivery mode, each packet can be up to 15KiB in size. The protocol limit is 16KiB for the entire data packet, but LiveKit adds various headers to properly route the packets which reduces the space available for user data.
While some platforms might support larger packet sizes without returning an error, LiveKit recommends this 16KiB limit to maximize compatibility across platforms and address limitations of the Stream Control Transmission Protocol (SCTP).  To learn more, see
Understanding message size limits
.
In the
lossy
delivery mode, LiveKit recommends even smaller data packets - just 1300 bytes maximum - to stay within the network Maximum Transmit Unit (MTU) of 1400 bytes. Larger packets are fragmented into multiple packets and if any single packet is lost, the whole packet is lost with it.
Selective delivery
Packets can be sent either to the entire room or to a subset of participants with the
destinationIdentities
parameter on the
publishData
call. To send to the entire room, leave
destinationIdentities
blank.
Topic
You may have different types and purposes of data packets. To easily differentiate, set the
topic
field to any string that makes sense for your application.
For example, in a realtime multiplayer game, you might use different topics for chat messages, character position updates, and environment updates.
Usage
JavaScript
Swift
Kotlin
Flutter
Python
Go
Unity
const
strData
=
JSON
.
stringify
(
{
some
:
"data"
}
)
const
encoder
=
new
TextEncoder
(
)
const
decoder
=
new
TextDecoder
(
)
// publishData takes in a Uint8Array, so we need to convert it
const
data
=
encoder
.
encode
(
strData
)
;
// Publish lossy data to the entire room
room
.
localParticipant
.
publishData
(
data
,
{
reliable
:
false
}
)
// Publish reliable data to a set of participants
room
.
localParticipant
.
publishData
(
data
,
{
reliable
:
true
,
destinationIdentities
:
[
'my-participant-identity'
]
}
)
// Receive data from other participants
room
.
on
(
RoomEvent
.
DataReceived
,
(
payload
:
Uint8Array
,
participant
:
Participant
,
kind
:
DataPacket_Kind
)
=>
{
const
strData
=
decoder
.
decode
(
payload
)
...
}
)
Copy
On this page
Overview
Delivery options
Size limits
Selective delivery
Topic
Usage
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/state/participant-attributes:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Deleting attributes
Update frequency
Size limits
Usage from LiveKit SDKs
Usage from server APIs
Overview
Each LiveKit participant has two fields for application-specific state:
Participant.attributes
: A string key-value store
Participant.metadata
: A single string that can store any data.
These fields are stored and managed by the LiveKit server, and are automatically synchronized to new participants who join the room later.
Initial values can be set in the participant’s
access token
, ensuring the value is immediately available when the participant connects.
While the metadata field is a single string, the attributes field is a key-value store. This allows fine-grained updates to different parts of the state without affecting or transmitting the values of other keys.
Deleting attributes
To delete an attribute key, set its value to an empty string (
''
).
Update frequency
Attributes and metadata are not suitable for high-frequency updates (more than once every few seconds) due to synchronization overhead on the server. If you need to send updates more frequently, consider using
data packets
instead.
Size limits
Metadata and attributes each have a 64 KiB limit. For attributes, this limit includes the combined size of all keys and values.
Usage from LiveKit SDKs
The LiveKit SDKs receive events on attributes and metadata changes for both the local participant and any remote participants in the room. See
Handling events
for more information.
Participants must have the
canUpdateOwnMetadata
permission in their access token to update their own attributes or metadata.
JavaScript
React
Swift
Kotlin
Flutter
Python
// receiving changes
room
.
on
(
RoomEvent
.
ParticipantAttributesChanged
,
(
changed
:
Record
<
string
,
string
>
,
participant
:
Participant
)
=>
{
console
.
log
(
'participant attributes changed'
,
changed
,
'all attributes'
,
participant
.
attributes
,
)
;
}
,
)
;
room
.
on
(
RoomEvent
.
ParticipantMetadataChanged
,
(
oldMetadata
:
string
|
undefined
,
participant
:
Participant
)
=>
{
console
.
log
(
'metadata changed from'
,
oldMetadata
,
participant
.
metadata
)
;
}
,
)
;
// updating local participant
room
.
localParticipant
.
setAttributes
(
{
myKey
:
'myValue'
,
myOtherKey
:
'otherValue'
,
}
)
;
room
.
localParticipant
.
setMetadata
(
JSON
.
stringify
(
{
some
:
'values'
,
}
)
,
)
;
Copy
Usage from server APIs
From the server side, you can update attributes or metadata of any participant in the room using the
RoomService.UpdateParticipant
API.
Node.js
Go
Python
Ruby
Java/Kotlin
import
{
RoomServiceClient
}
from
'livekit-server-sdk'
;
const
roomServiceClient
=
new
RoomServiceClient
(
'myhost'
,
'api-key'
,
'my secret'
)
;
roomServiceClient
.
updateParticipant
(
'room'
,
'identity'
,
{
attributes
:
{
myKey
:
'myValue'
,
}
,
metadata
:
'updated metadata'
,
}
)
;
Copy
On this page
Overview
Deleting attributes
Update frequency
Size limits
Usage from LiveKit SDKs
Usage from server APIs
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/home/client/state/room-metadata:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Overview
Overview
Similar to
Participant metadata
, Rooms also feature a metadata field for application-specific data which is visible to all participants.
Room metadata can only be set using the server APIs, but can be accessed by all participants in the room using the LiveKit SDKs.
To set room metadata, use the
CreateRoom
and
UpdateRoomMetadata
APIs.
To subscribe to updates, you must
handle
the
RoomMetadataChanged
event.
On this page
Overview
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/openai/realtime:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
Try the playground
On this page
Quickstarts
OpenAI Realtime API and LiveKit
How it works
The Agents framework
LiveKit concepts
MultimodalAgent
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
OpenAI Realtime API integration guide
.
v1.0 for Node.js is coming soon.
Quickstarts
OpenAI Playground
Experiment with OpenAI's Realtime API in the playground with personalities like the
Snarky Teenager
or
Opera Singer
.
OpenAI
Realtime
Speech to speech
Use OpenAI's Realtime API in this quickstart guide to create a speech-to-speech agent.
OpenAI Realtime API and LiveKit
OpenAI's Realtime API is a WebSocket interface for low-latency audio streaming, best suited for server-to-server use rather than direct consumption by end-user devices.
LiveKit offers Python and Node.js integrations for the API, enabling developers to build realtime conversational AI applications using LiveKit's Agents framework. This framework integrates with LiveKit's SDKs and telephony solutions, allowing you to build applications for any platform.
How it works
WebSocket is not ideal for realtime audio and video over long distances or slower networks. LiveKit bridges this gap by converting the transport to WebRTC and routing data through our global edge network to minimize transmission latency.
With the Agents framework, user audio is first transmitted to LiveKit's edge network via WebRTC and routed to your backend agent over low-latency connections. The agent then uses Agents framework integration to relay audio to OpenAI's model via WebSocket. Similarly, speech from OpenAI is streamed back through WebSocket to the agent and relayed to the user via WebRTC.
The Agents framework
The Agents framework provides everything needed to build conversational applications using OpenAI's Realtime API, including:
Support for
Python
and
Node.js
SDKs for nearly every platform
Inbound and outbound calling (using SIP trunks)
WebRTC transport via LiveKit Cloud or self-host OSS
Worker load balancing and request distribution (see
Agent lifecycle
)
LiveKit concepts
The LiveKit Agents framework uses the following concepts:
Room
: a realtime session with participants. The room acts as bridge between your end user and your agent. Each room has a name and is identified by a unique ID.
Participant
: a user or process (i.e. agent) participating in a room.
Agent
: a programmable AI participant in a room.
Track
: audio, video, text, or data published by a user or agent, and subscribed to by other participants in the room.
MultimodalAgent
The framework includes the
MultimodalAgent
class for building speech-to-speech agents that use the OpenAI Realtime API. To learn more about the differences between speech-to-speech and voice pipeline agents, see
Voice agents comparison
.
Try the playground
On this page
Quickstarts
OpenAI Realtime API and LiveKit
How it works
The Agents framework
LiveKit concepts
MultimodalAgent
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/openai/customize/vad:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Modifying the VAD parameters
Server-side VAD
Client-side VAD
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Realtime API integration guide
.
v1.0 for Node.js is coming soon.
VAD is a technique used to determine when a user has finished speaking, or has ended their turn at speaking, and lets the assistant know to respond. Accurate turn detection is key to maintaining a natural conversational flow and avoiding interruptions or awkward pauses. To learn more about turn detection, see
Turn detection
.
Modifying the VAD parameters
By default, OpenAI's Realtime API handles turn detection using VAD on the server side.
You can disable this to manually handle turn detection.
Server-side VAD
Server-side VAD is enabled by default. This means the API determines when the user has started or stopped speaking, and responds automatically. For server-side VAD, you can fine-tune the behavior by adjusting various parameters to suit your application's needs. Here are the parameters you can adjust:
threshold
: Adjusts the sensitivity of the VAD. A lower threshold makes the VAD more sensitive to speech (detects quieter sounds), while a higher threshold makes it less sensitive. The default value is
0.5
.
prefix_padding_ms
: Minimum duration of speech (in milliseconds) required to start a new speech chunk. This helps prevent very short sounds from triggering speech detection.
silence_duration_ms
: Minimum duration of silence (in milliseconds) at the end of speech before ending the speech segment. This ensures brief pauses do not prematurely end a speech segment.
agent.py
Python
Node.js
model
=
openai
.
realtime
.
RealtimeModel
(
voice
=
"alloy"
,
temperature
=
0.8
,
instructions
=
"You are a helpful assistant"
,
turn_detection
=
openai
.
realtime
.
ServerVadOptions
(
threshold
=
0.6
,
prefix_padding_ms
=
200
,
silence_duration_ms
=
500
)
,
)
agent
=
multimodal
.
MultimodalAgent
(
model
=
model
)
agent
.
start
(
ctx
.
room
)
Copy
Client-side VAD
Note
This option is currently only available for Python.
If you want to have more control over audio input, you can turn off VAD and implement manual VAD. This is useful
for push-to-talk interfaces where there is an obvious signal a user has started and stopped speaking. When you turn off
VAD, your have to trigger audio responses explicitly.
Usage
To turn off server-side VAD, update the turn detection parameter:
model
=
openai
.
realtime
.
RealtimeModel
(
voice
=
"alloy"
,
temperature
=
0.8
,
instructions
=
"You are a helpful assistant"
,
turn_detection
=
None
,
)
agent
=
multimodal
.
MultimodalAgent
(
model
=
model
)
agent
.
start
(
ctx
.
room
)
Copy
To manually generate speech, use the
generate_reply
method:
# When it's time to generate a new response, call generate_reply
agent
.
generate_reply
(
on_duplicate
=
"cancel_existing"
)
Copy
On this page
Modifying the VAD parameters
Server-side VAD
Client-side VAD
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search


Content from https://docs.livekit.io/agents/v0/integrations/openai/customize/parameters:

Docs
Search
GitHub
Slack
Sign in with Cloud
Home
AI Agents
Telephony
Recipes
Reference
On this page
Parameters
modalities
instructions
voice
turn_detection
temperature
max_output_tokens
Example Initialization
Agents 1.0 available for Python
This documentation is for v0.x of the LiveKit Agents framework.
See updated documentation here:
Realtime API integration guide
.
v1.0 for Node.js is coming soon.
The
RealtimeModel
class is used to create a realtime conversational AI session. Below are the key parameters that can be passed when initializing the model, with a focus on the
modalities
,
instructions
,
voice
,
turn_detection
,
temperature
, and
max_output_tokens
options.
Parameters
modalities
Type
:
list[api_proto.Modality]
Default
:
["text", "audio"]
Description
: Specifies the input/output modalities supported by the model. This can be either or both of:
"text"
: The model processes text-based input and generates text responses.
"audio"
: The model processes audio input and can generate audio responses.
Example
:
modalities
=
[
"text"
,
"audio"
]
Copy
instructions
Type
:
str | None
Default
:
None
Description
: Custom instructions are the 'system prompt' for the model to follow during the conversation. This can be used to guide the behavior of the model or set specific goals.
Example
:
instructions
=
"Please provide responses that are brief and informative."
Copy
voice
Type
:
api_proto.Voice
Default
:
"alloy"
Description
: Determines the voice used for audio responses. Some examples of voices include:
"alloy"
"echo"
"shimmer"
Example
:
voice
=
"alloy"
Copy
turn_detection
Type
:
api_proto.TurnDetectionType
Default
:
{"type": "server_vad"}
Description
: Controls how the model detects when a speaker has finished talking, which is critical in realtime interactions.
"server_vad"
: OpenAI uses server side Voice Activity Detection (VAD) to detect when the user has stopped speaking. This can be fine-tuned using the following parameters:
threshold
(optional): Float value to control the sensitivity of speech detection.
prefix_padding_ms
(optional): The amount of time (in milliseconds) to pad before the detected speech.
silence_duration_ms
(optional): The amount of silence (in milliseconds) required to consider the speech finished.
Example
:
turn_detection
=
{
"type"
:
"server_vad"
,
"threshold"
:
0.6
,
"prefix_padding_ms"
:
300
,
"silence_duration_ms"
:
500
}
Copy
temperature
Type
:
float
Default
:
0.8
Description
: Controls the randomness of the model's output. Higher values (e.g.,
1.0
and above) make the model's output more diverse and creative, while lower values (e.g.,
0.6
) makes it more focused and deterministic.
Example
:
temperature
=
0.7
Copy
max_output_tokens
Type
:
int
Default
:
2048
Description
: Limits the maximum number of tokens in the generated output. This helps control the length of the responses from the model, where one token roughly corresponds to one word.
Example
:
max_output_tokens
=
1500
Copy
Example Initialization
Here is a full example of how to initialize the
RealtimeModel
with these parameters:
Python
realtime_model
=
RealtimeModel
(
modalities
=
[
"text"
,
"audio"
]
,
instructions
=
"Give brief, concise answers."
,
voice
=
"alloy"
,
turn_detection
=
openai
.
realtime
.
ServerVadOptions
(
threshold
=
0.6
,
prefix_padding_ms
=
200
,
silence_duration_ms
=
500
,
)
,
temperature
=
0.7
,
max_output_tokens
=
1500
,
)
Copy
On this page
Parameters
modalities
instructions
voice
turn_detection
temperature
max_output_tokens
Example Initialization
Home
AI Agents
Telephony
Recipes
Reference
GitHub
Slack
Sign in
Search

